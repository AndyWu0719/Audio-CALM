defaults:
  - _self_

hydra:
  run:
    dir: .
  output_subdir: null
  job:
    chdir: False

# -------------------------------------------------------------------
# MODEL CONFIGURATION
# -------------------------------------------------------------------
model:
  head_type: "flow"
  audio_loss_weight: 1.0
  ctc_loss_weight: 0.1

  # Base Model Paths
  qwen_path: "${hydra:runtime.cwd}/qwen2_7B_Instruct"
  vae_path: "${hydra:runtime.cwd}/outputs/checkpoints/audio_vae_4x_kl_annealing_l1_ssim/checkpoint-6900"
  
  # LoRA Configuration
  use_lora: True
  lora_rank: 64
  lora_alpha: 128
  lora_dropout: 0.05
  merge_lora: False       # Set True only for inference or export
  
  freeze_projector: False
  
  # Latent Settings
  use_precomputed_latents: True
  latent_dim: 64

  # Flow Matching Hyperparameters
  flow_hidden_dim: 2048   # Increased for better capacity (from 1024)
  flow_num_layers: 4      # Increased depth (from 3)

  # === Soft Restart & Component Loading ===
  # 1. Non-LoRA Components (Inherit alignment from Phase 1)
  pretrained_projector_path: "/data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/calm_moa_flow/flow-moa-asr-4-dim64-1e-4-2048-4/input_proj.bin"
  pretrained_head_path: null
  
  # 2. LoRA Adapters (MoA Strategy)
  # Set to null to retrain from scratch; otherwise provide paths
  pretrained_lora_path_tts: null
  pretrained_lora_path_asr: null

# -------------------------------------------------------------------
# DATA CONFIGURATION
# -------------------------------------------------------------------
data:
  task_mode: "tts" 
  task_prob_tts: 1.0

  datasets:
    tts:
      raw_root: "${hydra:runtime.cwd}/data/raw/LibriTTS_R/train"
      latent_dir: "${hydra:runtime.cwd}/data/latents/train/LibriTTS_R"
      eval_latent_dir: "${hydra:runtime.cwd}/data/latents/dev/LibriTTS_R"
    
    asr:
      raw_root: "${hydra:runtime.cwd}/data/raw/LibriSpeech/train"
      latent_dir: "${hydra:runtime.cwd}/data/latents/train/LibriSpeech"
      eval_latent_dir: "${hydra:runtime.cwd}/data/latents/dev/LibriSpeech"
    
    mix:
      raw_root: null
      latent_dir: "${hydra:runtime.cwd}/data/latents/train" 
      eval_latent_dir: "${hydra:runtime.cwd}/data/latents/dev"

  train_subsets: "train-clean-100,train-clean-360,train-other-500"
  eval_subsets: "dev-clean"
  
  # Sequence Constraints
  max_text_len: 512
  max_audio_len: 1024 
  latent_downsample: 4

# -------------------------------------------------------------------
# TRAINING CONFIGURATION
# -------------------------------------------------------------------
training:
  output_dir: "${hydra:runtime.cwd}/outputs/checkpoints/calm_moa_flow"
  run_name: "flow-moa-tts-4-dim64-1e-4-2048-4"
  
  # Checkpointing (Fresh start for MoA)
  resume_from_checkpoint: null 
  ignore_data_skip: False
  
  # Batching & Accumulation
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 32   # Effective Batch = 4 * 16 * GPUs
  
  # Optimization
  learning_rate: 1e-4     # Higher LR allowed for fresh LoRA training
  num_train_epochs: 3
  
  # Hardware Acceleration
  bf16: True
  gradient_checkpointing: True      # Set False for 30% speedup if VRAM permits (A100/4090)
  
  # Optimizer & Scheduler
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.05
  max_grad_norm: 1.0
  
  # Logging & Saving
  logging_steps: 10
  save_strategy: "steps"
  save_steps: 1000
  save_total_limit: 2
  
  # Evaluation Strategy
  eval_strategy: "steps"
  eval_steps: 1000
  load_best_model_at_end: True
  metric_for_best_model: "loss"
  
  # Miscellaneous
  group_by_length: False
  label_smoothing_factor: 0.0 # ASR 0.1
  ddp_find_unused_parameters: True
  dataloader_num_workers: 8
  dataloader_pin_memory: True
  remove_unused_columns: False
  report_to: "wandb"
  seed: 42
  deepspeed: "${hydra:runtime.cwd}/train/ds_config.json"

# -------------------------------------------------------------------
# EVALUATION CONFIGURATION
# -------------------------------------------------------------------
evaluation:
  task: "asr"
  test_file: "${hydra:runtime.cwd}/data/latents_jsonl/dev_clean_latent.jsonl"
  
  # Inference Checkpoint
  checkpoint_path: "${hydra:runtime.cwd}/outputs/checkpoints/calm_moa_flow/flow-moa-asr-4-dim64-1e-4-2048-4/checkpoint-6594"
  output_dir: "${hydra:runtime.cwd}/outputs/eval_results_moa"
  
  # Generation Parameters
  max_samples: 50
  max_latents: 300
  use_vocoder: True
  flow_steps: 50
  
  # Metrics
  eval_asr_model: "openai/whisper-tiny.en"
  
  # Tracking
  wandb_project: "Audio-CALM-MoA"
  wandb_entity: null
  web_demo: False