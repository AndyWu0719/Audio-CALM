defaults:
  - _self_

hydra:
  run:
    dir: .
  output_subdir: null
  job:
    chdir: False

# -------------------------------------------------------------------
# MODEL (tunable block at top)
# -------------------------------------------------------------------
model:
  # Tunables (change frequently)
  qwen_path: "${hydra:runtime.cwd}/qwen2_7B_Instruct"
  vae_path: "${hydra:runtime.cwd}/outputs/checkpoints/audio_vae_4x_kl_annealing_l1_ssim/checkpoint-6900"
  use_lora: True
  lora_rank: 64
  lora_alpha: 128
  lora_dropout: 0.05
  use_precomputed_latents: True
  num_mixtures: 16
  latent_dim: 64
  audio_loss_weight: 1.0
  freeze_projector: True
  merge_lora: True

  # Paths / other (less frequently changed)
  pretrained_projector_path: "${hydra:runtime.cwd}/outputs/checkpoints/calm_latent_gmm/4-mix8-dim64-1e-4-1/checkpoint-10990/input_proj.bin"
  pretrained_lora_path: "${hydra:runtime.cwd}/outputs/checkpoints/calm_latent_gmm/4-mix8-dim64-1e-4-1/checkpoint-10990"

# -------------------------------------------------------------------
# DATA (tunable block at top)
# -------------------------------------------------------------------
data:
  # Tunables (change frequently)
  train_subsets: "train-clean-100,train-clean-360,train-other-500"
  eval_subsets: "dev-clean"
  max_text_len: 512
  max_audio_len: 512
  latent_downsample: 4
  task_mode: "mix"
  task_prob_tts: 0.5

  # Paths (less frequently changed)
  librispeech_root: "/data0/determined/users/andywu/speechcalm/data/full_librispeech/LibriSpeech"
  mel_dir: "${hydra:runtime.cwd}/data/latents/train"
  eval_mel_dir: "${hydra:runtime.cwd}/data/latents/dev"

# -------------------------------------------------------------------
# TRAINING (tunable block at top)
# -------------------------------------------------------------------
training:
  # Tunables (change frequently)
  output_dir: "${hydra:runtime.cwd}/outputs/checkpoints/calm_latent_gmm/gmm-mix-4-mix16-dim64-1e-4"
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 1e-4
  num_train_epochs: 5
  bf16: True
  gradient_checkpointing: True
  save_steps: 1000
  eval_steps: 1000
  run_name: "gmm-mix-4-mix8-dim64-1e-4"

  # Other settings (less frequently changed)
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  max_grad_norm: 1.0
  optim: "adamw_torch"
  ddp_find_unused_parameters: False
  deepspeed: "${hydra:runtime.cwd}/train/ds_config.json"
  dataloader_num_workers: 8
  remove_unused_columns: False
  logging_steps: 10
  save_strategy: "steps"
  save_total_limit: 2
  eval_strategy: "steps"
  load_best_model_at_end: True
  metric_for_best_model: "loss"
  report_to: "wandb"
  seed: 42

# -------------------------------------------------------------------
# EVALUATION (tunable block at top)
# -------------------------------------------------------------------
evaluation:
  # Tunables (change frequently)
  task: "tts"
  test_file: "${hydra:runtime.cwd}/data/latents_jsonl/dev_clean_latent.jsonl"
  checkpoint_path: "${hydra:runtime.cwd}/outputs/checkpoints/gmm-mix-4-mix16-dim64-1e-4/checkpoint-7000"
  output_dir: "${hydra:runtime.cwd}/outputs/eval_results_gmm_tts/gmm-mix-4-mix16-dim64-1e-4"
  max_samples: 50
  max_latents: 300
  eval_asr_model: "openai/whisper-tiny.en"
  use_vocoder: True
  seed: 42

  # Optional / UI
  web_demo: False
  wandb_project: "Audio-CALM-VAE"
  wandb_entity: null