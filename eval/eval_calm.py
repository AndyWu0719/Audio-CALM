"""
Unified Evaluation Script for Audio-CALM (v2 Refactored).
Supports: ASR (Speech-to-Text) and TTS (Text-to-Speech)
"""

import os
import sys
import json
import csv
import logging
import random
import torch
import torchaudio
import soundfile as sf
import hydra
import wandb
import numpy as np
from tqdm import tqdm
from typing import List, Dict
from omegaconf import DictConfig, OmegaConf, open_dict
from peft import PeftModel
import evaluate
from rich.logging import RichHandler
from rich.console import Console
import matplotlib.pyplot as plt
from transformers import pipeline

# --- Environment Patches ---
# ä¿®å¤éƒ¨åˆ†ç¯å¢ƒä¸­ torchaudio åç«¯æ£€æµ‹çš„é—®é¢˜
if not hasattr(torchaudio, "list_audio_backends"):
    try:
        import torchaudio.backend
        torchaudio.list_audio_backends = getattr(torchaudio.backend, "list_audio_backends", lambda: ["soundfile"])
    except ImportError:
        torchaudio.list_audio_backends = lambda: []

sys.path.append(os.getcwd())
# ã€å¯¹åº”å…³ç³»ã€‘ï¼šå¯¼å…¥ modeling_calm.py ä¸­çš„æ¨¡å‹å®šä¹‰
from models.modeling_calm import QwenCALM, QwenCALMConfig

logging.basicConfig(level="INFO", format="%(message)s", datefmt="[%X]", handlers=[RichHandler(rich_tracebacks=True, show_path=False)])
logger = logging.getLogger("eval")
console = Console()

# Metric setup
wer_metric = evaluate.load("wer")

def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def load_dataset_jsonl(path: str, max_samples: int = -1) -> List[Dict]:
    """
    åŠŸèƒ½ï¼šåŠ è½½æµ‹è¯•é›†æ•°æ® (.jsonl æ ¼å¼)ã€‚
    """
    if not os.path.exists(path):
        logger.error(f"Dataset not found: {path}")
        return []
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip(): data.append(json.loads(line))
    if max_samples > 0 and max_samples < len(data):
        logger.info(f"Subsampling {max_samples} from {len(data)} total samples.")
        random.shuffle(data)
        data = data[:max_samples]
    return data

# ==============================================================================
# 1. Vocoder (With Interpolation for TTS)
# ==============================================================================
class Vocoder:
    """
    åŠŸèƒ½ï¼šå£°ç å™¨ï¼Œè´Ÿè´£å°† Mel é¢‘è°±è½¬æ¢ä¸ºæ³¢å½¢ã€‚
    
    ã€å¯¹åº”å…³ç³»ã€‘ï¼š
    - è¾“å…¥ï¼šæ¥è‡ª QwenCALM.vae.decode() è¾“å‡ºçš„ Mel é¢‘è°±ã€‚
    - å…³é”®é€»è¾‘ï¼šå¤„ç† Log-Mel (VAEè¾“å‡º) åˆ° Log10-Mel (HiFi-GANè¾“å…¥) çš„è½¬æ¢ã€‚
    """
    def __init__(self, device="cuda"):
        self.device = device
        logger.info("ğŸ”§ Initializing Vocoder...")
        self.hifi = None
        try:
            from speechbrain.inference.vocoders import HIFIGAN
            self.hifi = HIFIGAN.from_hparams(
                source="speechbrain/tts-hifigan-libritts-16kHz",
                savedir="tmp_hifigan",
                run_opts={"device": device}
            )
            logger.info("âœ… SpeechBrain HiFi-GAN loaded.")
        except Exception as e:
            logger.warning(f"âš ï¸ HiFi-GAN not found ({e}). Will use Griffin-Lim.")

        # Griffin-Lim ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
        self.n_fft = 1024
        self.n_mels = 80
        self.sample_rate = 16000
        self.hop_length = 256
        self.win_length = 1024
        
        mel_fb = torchaudio.transforms.MelScale(
            n_mels=self.n_mels, sample_rate=self.sample_rate,
            f_min=0, f_max=8000, n_stft=self.n_fft // 2 + 1,
            norm="slaney", mel_scale="slaney"
        ).to(device).fb 
        self.inverse_mel_basis = torch.linalg.pinv(mel_fb).to(device)
        self.griffin_lim = torchaudio.transforms.GriffinLim(
            n_fft=self.n_fft, n_iter=60, win_length=self.win_length,
            hop_length=self.hop_length, power=1.0
        ).to(device)

    def decode(self, mel):
        """
        åŠŸèƒ½ï¼šæ‰§è¡Œè§£ç ã€‚
        """
        mel = mel.to(self.device).to(torch.float32)
        
        # 1. ç»´åº¦ç»Ÿä¸€: [B, 80, T]
        if mel.dim() == 2: mel = mel.unsqueeze(0)
        if mel.shape[-1] == 80: mel = mel.transpose(1, 2)

        # 2. HiFi-GAN è§£ç 
        if self.hifi is not None:
            # [CRITICAL FIX] æ¢å¤ç¼©æ”¾é€»è¾‘
            # VAE è¾“å‡ºæ˜¯ Log (ln) Melï¼ŒHiFiGAN éœ€è¦ Log10 Mel
            # å…³ç³»: ln(x) = ln(10) * log10(x) => log10(x) â‰ˆ ln(x) * 0.43429
            # å¦‚æœä¸ç¼©æ”¾ï¼Œèƒ½é‡ä¼šè¿‡å¤§å¯¼è‡´ç ´éŸ³
            mel_log10 = mel * 0.43429
            
            try: return self.hifi.decode_batch(mel_log10.transpose(1, 2)).squeeze(1)
            except: 
                try: return self.hifi.decode_batch(mel_log10).squeeze(1)
                except: pass

        # 3. Fallback: Griffin-Lim (éœ€è¦ Linear Mel)
        energy_mel = torch.exp(mel) 
        linear_energy = torch.matmul(energy_mel.transpose(1, 2), self.inverse_mel_basis).transpose(1, 2)
        linear_mag = torch.sqrt(torch.clamp(linear_energy, min=1e-8))
        wav = self.griffin_lim(linear_mag)
        
        # Peak Normalization
        peak = torch.max(torch.abs(wav))
        if peak > 1.0: wav = wav / peak
        return wav.squeeze(1)

# ==============================================================================
# 2. Model Loading (Fixed for Native SOA Support)
# ==============================================================================
def load_model(cfg, device):
    """
    åŠŸèƒ½ï¼šåŠ è½½å®Œæ•´çš„ QwenCALM æ¨¡å‹ç”¨äºæ¨ç†ã€‚
    
    ã€å¯¹åº”å…³ç³»ã€‘ï¼š
    - åŠ è½½ Config: å¯¹åº” config/calm_config.yaml
    - åŠ è½½ Base Model: Qwen2
    - åŠ è½½ Adapter: å¯¹åº” train_calm.py ä¿å­˜çš„ LoRA
    - åŠ è½½ Projector/Head/SOA: å¯¹åº” train_calm.py æ‰‹åŠ¨ä¿å­˜çš„ .bin æ–‡ä»¶
    """
    logger.info(f"ğŸ¤– Loading Model Base: {cfg.model.qwen_path}")
    
    config = QwenCALMConfig(
        qwen_path=cfg.model.qwen_path, 
        vae_path=cfg.model.vae_path, 
        latent_dim=cfg.model.latent_dim,
        flow_hidden_dim=cfg.model.get("flow_hidden_dim", 2048), 
        flow_num_layers=cfg.model.get("flow_num_layers", 4),
        use_precomputed_latents=False 
    )
    
    # 1. åˆå§‹åŒ–æ¨¡å‹ç»“æ„
    model = QwenCALM(config)
    
    ckpt_dir = cfg.evaluation.checkpoint_path
    logger.info(f"ğŸ“‚ Loading Checkpoints from: {ckpt_dir}")

    # 2. åŠ è½½ LLM Adapters (LoRA)
    # å°è¯•åŠ è½½ ASR æˆ– TTS Adapter
    if os.path.exists(os.path.join(ckpt_dir, "asr")) or os.path.exists(os.path.join(ckpt_dir, "tts")):
        if os.path.exists(os.path.join(ckpt_dir, "asr")):
            logger.info("  - Loading ASR LoRA...")
            model.llm = PeftModel.from_pretrained(model.llm, os.path.join(ckpt_dir, "asr"), adapter_name="asr")
        if os.path.exists(os.path.join(ckpt_dir, "tts")):
            logger.info("  - Loading TTS LoRA...")
            if isinstance(model.llm, PeftModel):
                try: model.llm.load_adapter(os.path.join(ckpt_dir, "tts"), adapter_name="tts")
                except: pass
            else:
                model.llm = PeftModel.from_pretrained(model.llm, os.path.join(ckpt_dir, "tts"), adapter_name="tts")
    else:
        # Fallback: æ ¹ç›®å½•ä¸‹å•ä¸ª Adapter
        if os.path.exists(os.path.join(ckpt_dir, "adapter_config.json")):
            logger.info("  - Loading Single LoRA...")
            model.llm = PeftModel.from_pretrained(model.llm, ckpt_dir)

    # 3. åŠ è½½ Projectors (Input/Output)
    for component in ["input_proj", "output_head"]:
        bin_path = os.path.join(ckpt_dir, f"{component}.bin")
        if os.path.exists(bin_path):
            logger.info(f"  - Loading {component}...")
            state_dict = torch.load(bin_path, map_location="cpu")
            # ä¿®å¤ DDP ä¿å­˜æ—¶å¯èƒ½å¸¦æœ‰çš„ module. å‰ç¼€
            state_dict = {k.replace("module.", ""): v for k, v in state_dict.items()}
            getattr(model, component).load_state_dict(state_dict)
        else:
            logger.warning(f"  âš ï¸  {component}.bin not found! Model may not work.")

    # 4. åŠ è½½ SOA Embed
    soa_path = os.path.join(ckpt_dir, "soa_embed.bin")
    if os.path.exists(soa_path):
        logger.info(f"  - Loading soa_embed...")
        soa_data = torch.load(soa_path, map_location="cpu")
        
        # å…¼å®¹å¤„ç†ï¼šæ”¯æŒ dict æˆ–ç›´æ¥ tensor
        tensor_data = soa_data
        if isinstance(soa_data, dict):
            key = next((k for k in ["weight", "soa_embed"] if k in soa_data), None)
            if key:
                tensor_data = soa_data[key]
            else:
                tensor_data = list(soa_data.values())[0]
        
        # ç²¾åº¦å¯¹é½
        if cfg.training.get("bf16", False) and tensor_data.dtype != torch.bfloat16:
             tensor_data = tensor_data.to(torch.bfloat16)
             
        model.soa_embed.data = tensor_data
    else:
        logger.warning(f"  âš ï¸  soa_embed.bin not found! TTS will produce noise.")

    model.to(device).eval()
    
    # 5. æ··åˆç²¾åº¦è®¾ç½®
    if cfg.training.get("bf16", False): 
        logger.info("  - Converting to bfloat16 (VAE remains fp32)")
        model.to(torch.bfloat16)
        model.vae.to(torch.float32) # VAE ä¿æŒ FP32 ä»¥ä¿è¯éŸ³è´¨
        
    return model

# ==============================================================================
# 3. ASR Inference Logic
# ==============================================================================
@torch.no_grad()
def run_asr_inference(model, tokenizer, latent_path, device):
    """
    åŠŸèƒ½ï¼šASR æ¨ç†ã€‚
    """
    # åˆ‡æ¢ Adapter
    if hasattr(model.llm, "set_adapter") and hasattr(model.llm, "peft_config"):
        if "asr" in model.llm.peft_config:
            model.llm.set_adapter("asr")

    # 1. åŠ è½½éŸ³é¢‘ Latent
    if not os.path.exists(latent_path): return ""
    payload = torch.load(latent_path, map_location="cpu")
    audio = payload.get("latent", payload) if isinstance(payload, dict) else payload
    
    # [T, D] -> [1, T, D]
    if audio.dim() == 2:
        if audio.shape[0] == 64: audio = audio.transpose(0, 1) 
        audio = audio.unsqueeze(0) 
    
    audio = audio.to(device).to(model.llm.dtype)
    
    # 2. æŠ•å½±éŸ³é¢‘ç‰¹å¾ (Projector)
    # ã€å¯¹åº”å…³ç³»ã€‘ï¼šè°ƒç”¨ modeling_calm.py ä¸­ AudioInputProjector
    # offset=0 è¡¨ç¤ºä»å¤´å¼€å§‹ç¼–ç 
    audio_embeds = model.input_proj(audio, offset=0) 

    # 3. æ„å»º Prompt
    prompt = "Transcribe the audio content into text."
    prefix_text = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    prefix_ids = tokenizer(prefix_text, return_tensors="pt", add_special_tokens=False).input_ids.to(device)
    prefix_embeds = model.get_input_embeddings()(prefix_ids)

    # 4. æ‹¼æ¥å¹¶ç”Ÿæˆ
    inputs_embeds = torch.cat([audio_embeds, prefix_embeds], dim=1)

    outputs = model.llm.generate(
        inputs_embeds=inputs_embeds,
        max_new_tokens=256,
        num_beams=5,
        do_sample=False,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        repetition_penalty=1.0 
    )
    
    transcription = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return transcription

def eval_task_asr(cfg, model, tokenizer, data):
    """
    åŠŸèƒ½ï¼šASR ä»»åŠ¡è¯„ä¼°å¾ªç¯ï¼Œè®¡ç®— WERã€‚
    """
    console.print("[bold green]>>> Running ASR Evaluation (Beam=5)[/bold green]")
    
    # Normalizer Setup
    try:
        from transformers.models.whisper.english_normalizer import BasicTextNormalizer
        normalizer = BasicTextNormalizer() 
    except ImportError:
        import re
        def normalizer(text): return re.sub(r"[^a-z0-9\s]", "", text.lower()).strip()

    out_path = os.path.join(cfg.evaluation.output_dir, "asr_results.csv")
    csv_file = open(out_path, "w", newline="", encoding="utf-8")
    writer = csv.writer(csv_file)
    writer.writerow(["id", "wer", "text_ref", "text_pred", "norm_ref", "norm_pred"])

    wers = []
    
    for i, item in enumerate(tqdm(data, desc="ASR Decoding")):
        text_ref = item.get("text", "")
        latent_path = item.get("audio") or item.get("latent_path") or item.get("file_path")
        
        if not latent_path: continue
        
        try:
            text_pred = run_asr_inference(model, tokenizer, latent_path, cfg.device)
            norm_ref = normalizer(text_ref)
            norm_pred = normalizer(text_pred)
            
            if len(norm_ref) == 0:
                wer = 0.0 if len(norm_pred) == 0 else 1.0
            else:
                wer = wer_metric.compute(predictions=[norm_pred], references=[norm_ref])
            
            wers.append(wer)
            writer.writerow([i, f"{wer:.4f}", text_ref, text_pred, norm_ref, norm_pred])
            
            if (i+1) % 10 == 0:
                avg_wer = sum(wers) / len(wers)
                console.print(f"[Sample {i+1}] Current Avg WER: {avg_wer:.2%}")
                
        except Exception as e:
            logger.error(f"Error sample {i}: {e}")

    csv_file.close()
    if len(wers) > 0:
        final_wer = sum(wers) / len(wers)
        console.print(f"[bold blue]âœ… Final WER: {final_wer:.2%}[/bold blue]")

# ==============================================================================
# 4. TTS Logic (Flow Matching)
# ==============================================================================
def generate_one_step_flow(model, condition, steps, cfg_scale, device):
    """
    åŠŸèƒ½ï¼šæ‰§è¡Œä¸€æ­¥æµåŒ¹é… (Flow Matching) ç”Ÿæˆã€‚
    ä½œç”¨ï¼šä»é«˜æ–¯å™ªå£°å‡ºå‘ï¼Œæ ¹æ® condition é¢„æµ‹é€Ÿåº¦åœºï¼Œé€šè¿‡ Euler ç§¯åˆ†æ¨è¿›ä¸€æ­¥ï¼Œç”Ÿæˆä¸€å¸§éŸ³é¢‘ Latentã€‚
    
    ã€å¯¹åº”å…³ç³»ã€‘ï¼š
    - è°ƒç”¨ `model.output_head` (modeling_calm.py)ã€‚
    """
    # 1. åˆå§‹åŒ–å™ªå£° x0 ~ N(0, 1)
    noise = torch.randn(1, 1, model.config.latent_dim, device=device, dtype=model.llm.dtype)
    dt = 1.0 / steps
    x = noise
    
    # 2. ODE ç§¯åˆ†å¾ªç¯
    for i in range(steps):
        t = torch.full((1,), i/steps, device=device, dtype=x.dtype)
        
        # [FIXED] è°ƒç”¨ Flow Head
        # v_cond: æœ‰æ¡ä»¶ç”Ÿæˆ
        v_cond = model.output_head(condition, x, t)
        # v_uncond: æ— æ¡ä»¶ç”Ÿæˆ (è¾“å…¥å…¨é›¶ Condition)
        v_uncond = model.output_head(torch.zeros_like(condition), x, t)
        
        # 3. CFG (Classifier-Free Guidance) å¼•å¯¼
        # å…¬å¼: v = v_uncond + scale * (v_cond - v_uncond)
        v = v_uncond + cfg_scale * (v_cond - v_uncond)
        
        # 4. æ›´æ–° x
        x = x + v * dt
        
    return x

@torch.no_grad()
def run_tts_inference(model, tokenizer, vocoder, text, steps=10, cfg_scale=1.0, device="cuda", save_plot_path=None):
    """
    åŠŸèƒ½ï¼šTTS æ¨ç†ä¸»å‡½æ•° (è‡ªå›å½’ç”Ÿæˆ + æ™ºèƒ½åœæ­¢)ã€‚
    """
    # åˆ‡æ¢ Adapter
    if hasattr(model.llm, "set_adapter") and hasattr(model.llm, "peft_config"):
        if "tts" in model.llm.peft_config:
            model.llm.set_adapter("tts")

    # 1. å‡†å¤‡ Text Prompt
    prompt = f"Read this text:\n{text}"
    formatted_text = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    text_ids = tokenizer(formatted_text, return_tensors="pt", add_special_tokens=False).input_ids.to(device)
    text_embeds = model.get_input_embeddings()(text_ids)
    
    # 2. æ·»åŠ  SOA Token
    soa_token = model.soa_embed.expand(1, -1, -1) 
    inputs_embeds = torch.cat([text_embeds, soa_token], dim=1)
    
    # 3. è¿è¡Œ LLM é¢„å¡«å……
    out = model.llm(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=None, output_hidden_states=True)
    past_kv = out.past_key_values
    
    # è·å–ç¬¬ä¸€ä¸ª Condition
    condition = out.hidden_states[-1][:, -1:, :] 
    
    # 4. ç”Ÿæˆç¬¬ä¸€å¸§
    curr_latent = generate_one_step_flow(model, condition, steps, cfg_scale, device)
    history_latents = [curr_latent]

    # 5. è‡ªå›å½’å¾ªç¯ (å¸¦åˆ¹è½¦æœºåˆ¶)
    # ç†è®ºæœ€å¤§é•¿åº¦: å‡è®¾ 10 ç§’ = 156 å¸§, 250 å¸§è¶³å¤Ÿäº†
    max_frames = 250 
    pbar = tqdm(range(max_frames), desc="Gen Audio", leave=False)
    
    # è·å– EOS Token ID
    eos_token_id = tokenizer.eos_token_id
    if eos_token_id is None: 
        eos_token_id = 151645 # Qwen é»˜è®¤ EOS

    stop_reason = "max_length" # è®°å½•åœæ­¢åŸå› 

    for i in pbar:
        # è¾“å…¥: å½“å‰ç”Ÿæˆçš„ Latent
        input_latent = curr_latent
        curr_embeds = model.input_proj(input_latent, offset=i)
        
        # LLM Step
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åŒæ—¶éœ€è¦ hidden_states (ç»™ Flow ç”¨) å’Œ logits (ç»™åœæ­¢æ£€æµ‹ç”¨)
        out = model.llm(inputs_embeds=curr_embeds, use_cache=True, past_key_values=past_kv, output_hidden_states=True)
        past_kv = out.past_key_values
        
        # A. è·å– Condition ç»™ Flow Head
        condition = out.hidden_states[-1][:, -1:, :]
        
        # B. [æ–°å¢] åœæ­¢æ£€æµ‹é€»è¾‘ (Stop Token Detection)
        # è·å– LM Head çš„é¢„æµ‹ç»“æœ
        logits = out.logits[:, -1, :] # [1, Vocab]
        pred_token_id = torch.argmax(logits, dim=-1).item()
        
        # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šå¦‚æœ LLM é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ˜¯ EOSï¼Œè¯´æ˜å®ƒè§‰å¾—éŸ³é¢‘è¯¥ç»“æŸäº†
        if pred_token_id == eos_token_id:
            stop_reason = "eos_token"
            # console.print(f"[yellow]ğŸ›‘ Stop Token Detected at step {i}[/yellow]")
            break
            
        # C. [å¯é€‰] é™éŸ³æ£€æµ‹ (Silence Detection) ä½œä¸ºåŒä¿é™©
        # å¦‚æœ Latent çš„èƒ½é‡æä½ä¸”å·²ç»ç”Ÿæˆäº†ä¸€å®šé•¿åº¦ï¼Œä¹Ÿå¯ä»¥åœ
        # (éœ€è¦æ ¹æ®ä½ çš„ Latent ç»Ÿè®¡ç‰¹æ€§è°ƒæ•´é˜ˆå€¼ï¼Œæ¯”å¦‚ 0.05)
        latent_energy = torch.mean(torch.abs(input_latent)).item()
        if i > 50 and latent_energy < 0.05:
            stop_reason = "silence"
            # console.print(f"[yellow]ğŸ›‘ Silence Detected at step {i}[/yellow]")
            break

        # Flow ç”Ÿæˆä¸‹ä¸€å¸§
        curr_latent = generate_one_step_flow(model, condition, steps, cfg_scale, device)
        history_latents.append(curr_latent)

    # console.print(f"Generated {len(history_latents)} frames. Reason: {stop_reason}")

    # 6. è§£ç ä¸ºæ³¢å½¢
    latents = torch.cat(history_latents, dim=1).transpose(1, 2).to(torch.float32) 
    mel = model.vae.decode(latents)
    
    if save_plot_path:
        mel_cpu = mel.squeeze().float().cpu().numpy()
        plt.figure(figsize=(10, 4))
        plt.imshow(mel_cpu, aspect='auto', origin='lower', interpolation='none')
        plt.colorbar()
        plt.title(f"Generated Mel (Text: {text[:20]}...) [Stop: {stop_reason}]")
        plt.tight_layout()
        plt.savefig(save_plot_path)
        plt.close()
    
    wav = vocoder.decode(mel)
    return wav.cpu()

def eval_task_tts(cfg, model, tokenizer, vocoder, data):
    """
    åŠŸèƒ½ï¼šTTS ä»»åŠ¡è¯„ä¼°å¾ªç¯ï¼Œç”ŸæˆéŸ³é¢‘ -> ASR è½¬å½• -> è®¡ç®— WERã€‚
    """
    wav_dir = os.path.join(cfg.evaluation.output_dir, "generated_wavs")
    os.makedirs(wav_dir, exist_ok=True)
    
    # [ä¿®æ”¹] CSV Header å¢åŠ  metrics
    csv_file = open(os.path.join(cfg.evaluation.output_dir, "tts_results.csv"), "w", newline="", encoding="utf-8")
    writer = csv.writer(csv_file)
    writer.writerow(["id", "text_ref", "text_pred", "wer", "wav_path"])
    
    console.print(f"[bold green]>>> Running TTS Evaluation (Steps={cfg.evaluation.flow_steps})[/bold green]")
    img_dir = os.path.join(cfg.evaluation.output_dir, "mel_plots")
    os.makedirs(img_dir, exist_ok=True)

    # [æ–°å¢] åŠ è½½è¯„ä¼°ç”¨çš„ ASR æ¨¡å‹ (Whisper)
    # å»ºè®®ä½¿ç”¨ whisper-small.en æˆ– whisper-base.enï¼Œé€Ÿåº¦å¿«ä¸”ç²¾åº¦å¤Ÿç”¨
    asr_model_id = cfg.evaluation.get("eval_asr_model", "openai/whisper-tiny.en")
    console.print(f"[bold yellow]Loading ASR Evaluator: {asr_model_id}...[/bold yellow]")
    asr_pipe = pipeline(
        "automatic-speech-recognition", 
        model=asr_model_id, 
        device=cfg.device
    )

    # æ–‡æœ¬æ ‡å‡†åŒ–å™¨ (ç§»é™¤æ ‡ç‚¹ï¼Œç»Ÿä¸€å°å†™)
    import re
    def normalize(text): return re.sub(r"[^a-z0-9\s]", "", text.lower()).strip()

    wers = []

    for i, item in enumerate(data):
        text_ref = item.get("text", "")
        if not text_ref: continue
        
        try:
            scale = cfg.evaluation.get("cfg_scale", 1.0)
            steps = cfg.evaluation.get("flow_steps", 10)
            
            # 1. ç”ŸæˆéŸ³é¢‘
            wav = run_tts_inference(
                model, tokenizer, vocoder, text_ref, 
                steps=steps, 
                cfg_scale=scale, 
                device=cfg.device,
                save_plot_path=os.path.join(img_dir, f"mel_{i}.png")
            )
            
            wav_np = wav.squeeze().numpy()
            save_path = os.path.join(wav_dir, f"sample_{i}.wav")
            sf.write(save_path, wav_np, 16000)
            
            # 2. [æ–°å¢] ASR è½¬å½• (æŠŠç”Ÿæˆçš„éŸ³é¢‘è½¬å›æ–‡å­—)
            # Whisper éœ€è¦ numpy array
            transcription = asr_pipe(wav_np)["text"]
            
            # 3. [æ–°å¢] è®¡ç®— WER
            norm_ref = normalize(text_ref)
            norm_pred = normalize(transcription)
            
            if len(norm_ref) > 0:
                wer = wer_metric.compute(predictions=[norm_pred], references=[norm_ref])
            else:
                wer = 1.0
                
            wers.append(wer)
            
            # 4. å†™å…¥ CSV
            writer.writerow([i, text_ref, transcription, f"{wer:.4f}", save_path])
            
            if (i+1) % 5 == 0: 
                avg_wer = sum(wers) / len(wers)
                console.print(f"[Sample {i+1}] Avg WER: {avg_wer:.2%} | Ref: {text_ref[:20]}... | Pred: {transcription[:20]}...")
                
        except Exception as e:
            logger.error(f"Error sample {i}: {e}")

    csv_file.close()
    
    if len(wers) > 0:
        final_wer = sum(wers) / len(wers)
        console.print(f"[bold blue]âœ… Final TTS WER: {final_wer:.2%}[/bold blue]")

# ==============================================================================
# Main Entry
# ==============================================================================
@hydra.main(version_base=None, config_path="../config", config_name="calm_config")
def main(cfg: DictConfig):
    set_seed(cfg.evaluation.get("seed", 42))
    os.makedirs(cfg.evaluation.output_dir, exist_ok=True)
    
    with open_dict(cfg): 
        cfg.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    if not cfg.evaluation.get("web_demo", False):
        wandb.init(project=cfg.evaluation.get("wandb_project", "Audio-CALM-Eval"), config=OmegaConf.to_container(cfg))

    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(cfg.model.qwen_path, trust_remote_code=True)
    
    # 1. åŠ è½½æ¨¡å‹
    model = load_model(cfg, cfg.device)
    
    # 2. åŠ è½½æ•°æ®
    data = load_dataset_jsonl(cfg.evaluation.test_file, cfg.evaluation.max_samples)
    
    # 3. ä»»åŠ¡åˆ†å‘
    task = cfg.evaluation.task.lower()
    if task == "tts":
        vocoder = Vocoder(cfg.device)
        eval_task_tts(cfg, model, tokenizer, vocoder, data)
    elif task == "asr":
        eval_task_asr(cfg, model, tokenizer, data)
    elif task == "mix":
        vocoder = Vocoder(cfg.device)
        console.rule("[bold]Starting TTS Evaluation[/bold]")
        eval_task_tts(cfg, model, tokenizer, vocoder, data)
        console.rule("[bold]Starting ASR Evaluation[/bold]")
        eval_task_asr(cfg, model, tokenizer, data)
    else:
        logger.error(f"Unknown task: {task}")

if __name__ == "__main__":
    main()