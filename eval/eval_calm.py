"""
Unified Evaluation Script for Audio-CALM (v2 Refactored).
Supports: ASR (Speech-to-Text) and TTS (Text-to-Speech)
"""

import os
import sys
import json
import csv
import logging
import random
import torch
import torchaudio
import soundfile as sf
import hydra
import wandb
import numpy as np
from tqdm import tqdm
from typing import List, Dict
from omegaconf import DictConfig, OmegaConf, open_dict
from peft import PeftModel
import evaluate
from rich.logging import RichHandler
from rich.console import Console
import matplotlib.pyplot as plt

# --- Environment Patches ---
# ‰øÆÂ§çÈÉ®ÂàÜÁéØÂ¢É‰∏≠ torchaudio ÂêéÁ´ØÊ£ÄÊµãÁöÑÈóÆÈ¢ò
if not hasattr(torchaudio, "list_audio_backends"):
    try:
        import torchaudio.backend
        torchaudio.list_audio_backends = getattr(torchaudio.backend, "list_audio_backends", lambda: ["soundfile"])
    except ImportError:
        torchaudio.list_audio_backends = lambda: []

sys.path.append(os.getcwd())
# „ÄêÂØπÂ∫îÂÖ≥Á≥ª„ÄëÔºöÂØºÂÖ• modeling_calm.py ‰∏≠ÁöÑÊ®°ÂûãÂÆö‰πâ
from models.modeling_calm import QwenCALM, QwenCALMConfig

logging.basicConfig(level="INFO", format="%(message)s", datefmt="[%X]", handlers=[RichHandler(rich_tracebacks=True, show_path=False)])
logger = logging.getLogger("eval")
console = Console()

# Metric setup
wer_metric = evaluate.load("wer")

def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def load_dataset_jsonl(path: str, max_samples: int = -1) -> List[Dict]:
    """
    ÂäüËÉΩÔºöÂä†ËΩΩÊµãËØïÈõÜÊï∞ÊçÆ (.jsonl Ê†ºÂºè)„ÄÇ
    """
    if not os.path.exists(path):
        logger.error(f"Dataset not found: {path}")
        return []
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip(): data.append(json.loads(line))
    if max_samples > 0 and max_samples < len(data):
        logger.info(f"Subsampling {max_samples} from {len(data)} total samples.")
        random.shuffle(data)
        data = data[:max_samples]
    return data

# ==============================================================================
# 1. Vocoder (With Interpolation for TTS)
# ==============================================================================
class Vocoder:
    """
    ÂäüËÉΩÔºöÂ£∞Á†ÅÂô®ÔºåË¥üË¥£Â∞Ü Mel È¢ëË∞±ËΩ¨Êç¢‰∏∫Ê≥¢ÂΩ¢„ÄÇ
    
    „ÄêÂØπÂ∫îÂÖ≥Á≥ª„ÄëÔºö
    - ËæìÂÖ•ÔºöÊù•Ëá™ QwenCALM.vae.decode() ËæìÂá∫ÁöÑ Mel È¢ëË∞±„ÄÇ
    - ÂÖ≥ÈîÆÈÄªËæëÔºöÂ§ÑÁêÜ Log-Mel (VAEËæìÂá∫) Âà∞ Log10-Mel (HiFi-GANËæìÂÖ•) ÁöÑËΩ¨Êç¢„ÄÇ
    """
    def __init__(self, device="cuda"):
        self.device = device
        logger.info("üîß Initializing Vocoder...")
        self.hifi = None
        try:
            from speechbrain.inference.vocoders import HIFIGAN
            self.hifi = HIFIGAN.from_hparams(
                source="speechbrain/tts-hifigan-libritts-16kHz",
                savedir="tmp_hifigan",
                run_opts={"device": device}
            )
            logger.info("‚úÖ SpeechBrain HiFi-GAN loaded.")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è HiFi-GAN not found ({e}). Will use Griffin-Lim.")

        # Griffin-Lim ‰Ωú‰∏∫Â§áÈÄâÊñπÊ°à
        self.n_fft = 1024
        self.n_mels = 80
        self.sample_rate = 16000
        self.hop_length = 256
        self.win_length = 1024
        
        mel_fb = torchaudio.transforms.MelScale(
            n_mels=self.n_mels, sample_rate=self.sample_rate,
            f_min=0, f_max=8000, n_stft=self.n_fft // 2 + 1,
            norm="slaney", mel_scale="slaney"
        ).to(device).fb 
        self.inverse_mel_basis = torch.linalg.pinv(mel_fb).to(device)
        self.griffin_lim = torchaudio.transforms.GriffinLim(
            n_fft=self.n_fft, n_iter=60, win_length=self.win_length,
            hop_length=self.hop_length, power=1.0
        ).to(device)

    def decode(self, mel):
        """
        ÂäüËÉΩÔºöÊâßË°åËß£Á†Å„ÄÇ
        """
        mel = mel.to(self.device).to(torch.float32)
        
        # 1. Áª¥Â∫¶Áªü‰∏Ä: [B, 80, T]
        if mel.dim() == 2: mel = mel.unsqueeze(0)
        if mel.shape[-1] == 80: mel = mel.transpose(1, 2)

        # 2. HiFi-GAN Ëß£Á†Å
        if self.hifi is not None:
            # [CRITICAL FIX] ÊÅ¢Â§çÁº©ÊîæÈÄªËæë
            # VAE ËæìÂá∫ÊòØ Log (ln) MelÔºåHiFiGAN ÈúÄË¶Å Log10 Mel
            # ÂÖ≥Á≥ª: ln(x) = ln(10) * log10(x) => log10(x) ‚âà ln(x) * 0.43429
            # Â¶ÇÊûú‰∏çÁº©ÊîæÔºåËÉΩÈáè‰ºöËøáÂ§ßÂØºËá¥Á†¥Èü≥
            mel_log10 = mel * 0.43429
            
            try: return self.hifi.decode_batch(mel_log10.transpose(1, 2)).squeeze(1)
            except: 
                try: return self.hifi.decode_batch(mel_log10).squeeze(1)
                except: pass

        # 3. Fallback: Griffin-Lim (ÈúÄË¶Å Linear Mel)
        energy_mel = torch.exp(mel) 
        linear_energy = torch.matmul(energy_mel.transpose(1, 2), self.inverse_mel_basis).transpose(1, 2)
        linear_mag = torch.sqrt(torch.clamp(linear_energy, min=1e-8))
        wav = self.griffin_lim(linear_mag)
        
        # Peak Normalization
        peak = torch.max(torch.abs(wav))
        if peak > 1.0: wav = wav / peak
        return wav.squeeze(1)

# ==============================================================================
# 2. Model Loading (Fixed for Native SOA Support)
# ==============================================================================
def load_model(cfg, device):
    """
    ÂäüËÉΩÔºöÂä†ËΩΩÂÆåÊï¥ÁöÑ QwenCALM Ê®°ÂûãÁî®‰∫éÊé®ÁêÜ„ÄÇ
    
    „ÄêÂØπÂ∫îÂÖ≥Á≥ª„ÄëÔºö
    - Âä†ËΩΩ Config: ÂØπÂ∫î config/calm_config.yaml
    - Âä†ËΩΩ Base Model: Qwen2
    - Âä†ËΩΩ Adapter: ÂØπÂ∫î train_calm.py ‰øùÂ≠òÁöÑ LoRA
    - Âä†ËΩΩ Projector/Head/SOA: ÂØπÂ∫î train_calm.py ÊâãÂä®‰øùÂ≠òÁöÑ .bin Êñá‰ª∂
    """
    logger.info(f"ü§ñ Loading Model Base: {cfg.model.qwen_path}")
    
    config = QwenCALMConfig(
        qwen_path=cfg.model.qwen_path, 
        vae_path=cfg.model.vae_path, 
        latent_dim=cfg.model.latent_dim,
        flow_hidden_dim=cfg.model.get("flow_hidden_dim", 2048), 
        flow_num_layers=cfg.model.get("flow_num_layers", 4),
        use_precomputed_latents=False 
    )
    
    # 1. ÂàùÂßãÂåñÊ®°ÂûãÁªìÊûÑ
    model = QwenCALM(config)
    
    ckpt_dir = cfg.evaluation.checkpoint_path
    logger.info(f"üìÇ Loading Checkpoints from: {ckpt_dir}")

    # 2. Âä†ËΩΩ LLM Adapters (LoRA)
    # Â∞ùËØïÂä†ËΩΩ ASR Êàñ TTS Adapter
    if os.path.exists(os.path.join(ckpt_dir, "asr")) or os.path.exists(os.path.join(ckpt_dir, "tts")):
        if os.path.exists(os.path.join(ckpt_dir, "asr")):
            logger.info("  - Loading ASR LoRA...")
            model.llm = PeftModel.from_pretrained(model.llm, os.path.join(ckpt_dir, "asr"), adapter_name="asr")
        if os.path.exists(os.path.join(ckpt_dir, "tts")):
            logger.info("  - Loading TTS LoRA...")
            if isinstance(model.llm, PeftModel):
                try: model.llm.load_adapter(os.path.join(ckpt_dir, "tts"), adapter_name="tts")
                except: pass
            else:
                model.llm = PeftModel.from_pretrained(model.llm, os.path.join(ckpt_dir, "tts"), adapter_name="tts")
    else:
        # Fallback: Ê†πÁõÆÂΩï‰∏ãÂçï‰∏™ Adapter
        if os.path.exists(os.path.join(ckpt_dir, "adapter_config.json")):
            logger.info("  - Loading Single LoRA...")
            model.llm = PeftModel.from_pretrained(model.llm, ckpt_dir)

    # 3. Âä†ËΩΩ Projectors (Input/Output)
    for component in ["input_proj", "output_head"]:
        bin_path = os.path.join(ckpt_dir, f"{component}.bin")
        if os.path.exists(bin_path):
            logger.info(f"  - Loading {component}...")
            state_dict = torch.load(bin_path, map_location="cpu")
            # ‰øÆÂ§ç DDP ‰øùÂ≠òÊó∂ÂèØËÉΩÂ∏¶ÊúâÁöÑ module. ÂâçÁºÄ
            state_dict = {k.replace("module.", ""): v for k, v in state_dict.items()}
            getattr(model, component).load_state_dict(state_dict)
        else:
            logger.warning(f"  ‚ö†Ô∏è  {component}.bin not found! Model may not work.")

    # 4. Âä†ËΩΩ SOA Embed
    soa_path = os.path.join(ckpt_dir, "soa_embed.bin")
    if os.path.exists(soa_path):
        logger.info(f"  - Loading soa_embed...")
        soa_data = torch.load(soa_path, map_location="cpu")
        
        # ÂÖºÂÆπÂ§ÑÁêÜÔºöÊîØÊåÅ dict ÊàñÁõ¥Êé• tensor
        tensor_data = soa_data
        if isinstance(soa_data, dict):
            key = next((k for k in ["weight", "soa_embed"] if k in soa_data), None)
            if key:
                tensor_data = soa_data[key]
            else:
                tensor_data = list(soa_data.values())[0]
        
        # Á≤æÂ∫¶ÂØπÈΩê
        if cfg.training.get("bf16", False) and tensor_data.dtype != torch.bfloat16:
             tensor_data = tensor_data.to(torch.bfloat16)
             
        model.soa_embed.data = tensor_data
    else:
        logger.warning(f"  ‚ö†Ô∏è  soa_embed.bin not found! TTS will produce noise.")

    model.to(device).eval()
    
    # 5. Ê∑∑ÂêàÁ≤æÂ∫¶ËÆæÁΩÆ
    if cfg.training.get("bf16", False): 
        logger.info("  - Converting to bfloat16 (VAE remains fp32)")
        model.to(torch.bfloat16)
        model.vae.to(torch.float32) # VAE ‰øùÊåÅ FP32 ‰ª•‰øùËØÅÈü≥Ë¥®
        
    return model

# ==============================================================================
# 3. ASR Inference Logic
# ==============================================================================
@torch.no_grad()
def run_asr_inference(model, tokenizer, latent_path, device):
    """
    ÂäüËÉΩÔºöASR Êé®ÁêÜ„ÄÇ
    """
    # ÂàáÊç¢ Adapter
    if hasattr(model.llm, "set_adapter") and hasattr(model.llm, "peft_config"):
        if "asr" in model.llm.peft_config:
            model.llm.set_adapter("asr")

    # 1. Âä†ËΩΩÈü≥È¢ë Latent
    if not os.path.exists(latent_path): return ""
    payload = torch.load(latent_path, map_location="cpu")
    audio = payload.get("latent", payload) if isinstance(payload, dict) else payload
    
    # [T, D] -> [1, T, D]
    if audio.dim() == 2:
        if audio.shape[0] == 64: audio = audio.transpose(0, 1) 
        audio = audio.unsqueeze(0) 
    
    audio = audio.to(device).to(model.llm.dtype)
    
    # 2. ÊäïÂΩ±Èü≥È¢ëÁâπÂæÅ (Projector)
    # „ÄêÂØπÂ∫îÂÖ≥Á≥ª„ÄëÔºöË∞ÉÁî® modeling_calm.py ‰∏≠ AudioInputProjector
    # offset=0 Ë°®Á§∫‰ªéÂ§¥ÂºÄÂßãÁºñÁ†Å
    audio_embeds = model.input_proj(audio, offset=0) 

    # 3. ÊûÑÂª∫ Prompt
    prompt = "Transcribe the audio content into text."
    prefix_text = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    prefix_ids = tokenizer(prefix_text, return_tensors="pt", add_special_tokens=False).input_ids.to(device)
    prefix_embeds = model.get_input_embeddings()(prefix_ids)

    # 4. ÊãºÊé•Âπ∂ÁîüÊàê
    inputs_embeds = torch.cat([audio_embeds, prefix_embeds], dim=1)

    outputs = model.llm.generate(
        inputs_embeds=inputs_embeds,
        max_new_tokens=256,
        num_beams=5,
        do_sample=False,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        repetition_penalty=1.0 
    )
    
    transcription = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return transcription

def eval_task_asr(cfg, model, tokenizer, data):
    """
    ÂäüËÉΩÔºöASR ‰ªªÂä°ËØÑ‰º∞Âæ™ÁéØÔºåËÆ°ÁÆó WER„ÄÇ
    """
    console.print("[bold green]>>> Running ASR Evaluation (Beam=5)[/bold green]")
    
    # Normalizer Setup
    try:
        from transformers.models.whisper.english_normalizer import BasicTextNormalizer
        normalizer = BasicTextNormalizer() 
    except ImportError:
        import re
        def normalizer(text): return re.sub(r"[^a-z0-9\s]", "", text.lower()).strip()

    out_path = os.path.join(cfg.evaluation.output_dir, "asr_results.csv")
    csv_file = open(out_path, "w", newline="", encoding="utf-8")
    writer = csv.writer(csv_file)
    writer.writerow(["id", "wer", "text_ref", "text_pred", "norm_ref", "norm_pred"])

    wers = []
    
    for i, item in enumerate(tqdm(data, desc="ASR Decoding")):
        text_ref = item.get("text", "")
        latent_path = item.get("audio") or item.get("latent_path") or item.get("file_path")
        
        if not latent_path: continue
        
        try:
            text_pred = run_asr_inference(model, tokenizer, latent_path, cfg.device)
            norm_ref = normalizer(text_ref)
            norm_pred = normalizer(text_pred)
            
            if len(norm_ref) == 0:
                wer = 0.0 if len(norm_pred) == 0 else 1.0
            else:
                wer = wer_metric.compute(predictions=[norm_pred], references=[norm_ref])
            
            wers.append(wer)
            writer.writerow([i, f"{wer:.4f}", text_ref, text_pred, norm_ref, norm_pred])
            
            if (i+1) % 10 == 0:
                avg_wer = sum(wers) / len(wers)
                console.print(f"[Sample {i+1}] Current Avg WER: {avg_wer:.2%}")
                
        except Exception as e:
            logger.error(f"Error sample {i}: {e}")

    csv_file.close()
    if len(wers) > 0:
        final_wer = sum(wers) / len(wers)
        console.print(f"[bold blue]‚úÖ Final WER: {final_wer:.2%}[/bold blue]")

# ==============================================================================
# 4. TTS Logic (Flow Matching)
# ==============================================================================
def generate_one_step_flow(model, condition, steps, cfg_scale, device):
    """
    ÂäüËÉΩÔºöÊâßË°å‰∏ÄÊ≠•ÊµÅÂåπÈÖç (Flow Matching) ÁîüÊàê„ÄÇ
    ‰ΩúÁî®Ôºö‰ªéÈ´òÊñØÂô™Â£∞Âá∫ÂèëÔºåÊ†πÊçÆ condition È¢ÑÊµãÈÄüÂ∫¶Âú∫ÔºåÈÄöËøá Euler ÁßØÂàÜÊé®Ëøõ‰∏ÄÊ≠•ÔºåÁîüÊàê‰∏ÄÂ∏ßÈü≥È¢ë Latent„ÄÇ
    
    „ÄêÂØπÂ∫îÂÖ≥Á≥ª„ÄëÔºö
    - Ë∞ÉÁî® `model.output_head` (modeling_calm.py)„ÄÇ
    """
    # 1. ÂàùÂßãÂåñÂô™Â£∞ x0 ~ N(0, 1)
    noise = torch.randn(1, 1, model.config.latent_dim, device=device, dtype=model.llm.dtype)
    dt = 1.0 / steps
    x = noise
    
    # 2. ODE ÁßØÂàÜÂæ™ÁéØ
    for i in range(steps):
        t = torch.full((1,), i/steps, device=device, dtype=x.dtype)
        
        # [FIXED] Ë∞ÉÁî® Flow Head
        # v_cond: ÊúâÊù°‰ª∂ÁîüÊàê
        v_cond = model.output_head(condition, x, t)
        # v_uncond: Êó†Êù°‰ª∂ÁîüÊàê (ËæìÂÖ•ÂÖ®Èõ∂ Condition)
        v_uncond = model.output_head(torch.zeros_like(condition), x, t)
        
        # 3. CFG (Classifier-Free Guidance) ÂºïÂØº
        # ÂÖ¨Âºè: v = v_uncond + scale * (v_cond - v_uncond)
        v = v_uncond + cfg_scale * (v_cond - v_uncond)
        
        # 4. Êõ¥Êñ∞ x
        x = x + v * dt
        
    return x

@torch.no_grad()
def run_tts_inference(model, tokenizer, vocoder, text, steps=10, cfg_scale=1.0, device="cuda", save_plot_path=None):
    """
    ÂäüËÉΩÔºöTTS Êé®ÁêÜ‰∏ªÂáΩÊï∞ (Ëá™ÂõûÂΩíÁîüÊàê)„ÄÇ
    Ê≠•È™§Ôºö
    1. È¢ÑÂ°´ÂÖÖ (Prefill): Â§ÑÁêÜ PromptÔºåËé∑Âèñ SOA Token ÁöÑËæìÂá∫‰Ωú‰∏∫ÂàùÂßã Condition„ÄÇ
    2. Ëá™ÂõûÂΩíÂæ™ÁéØ (Autoregressive Loop): ÈÄêÂ∏ßÁîüÊàêÈü≥È¢ë Latent„ÄÇ
    3. Ëß£Á†Å (Decode): Latent -> VAE Decode -> Mel -> Vocoder -> Waveform„ÄÇ
    """
    # ÂàáÊç¢ Adapter
    if hasattr(model.llm, "set_adapter") and hasattr(model.llm, "peft_config"):
        if "tts" in model.llm.peft_config:
            model.llm.set_adapter("tts")

    # 1. ÂáÜÂ§á Text Prompt
    prompt = f"Read this text:\n{text}"
    formatted_text = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    text_ids = tokenizer(formatted_text, return_tensors="pt", add_special_tokens=False).input_ids.to(device)
    text_embeds = model.get_input_embeddings()(text_ids)
    
    # 2. Ê∑ªÂä† SOA (Start of Audio) Token
    # ËøôÊòØÁîüÊàêÁöÑËß¶ÂèëÂô®
    soa_token = model.soa_embed.expand(1, -1, -1) 
    inputs_embeds = torch.cat([text_embeds, soa_token], dim=1)
    
    # 3. ËøêË°å LLM È¢ÑÂ°´ÂÖÖ
    out = model.llm(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=None, output_hidden_states=True)
    past_kv = out.past_key_values
    
    # Ëé∑ÂèñÁ¨¨‰∏Ä‰∏™ Condition (Êù•Ëá™ SOA ÁöÑËæìÂá∫ÈöêÁä∂ÊÄÅ)
    condition = out.hidden_states[-1][:, -1:, :] 
    
    # 4. ÁîüÊàêÁ¨¨‰∏ÄÂ∏ßÈü≥È¢ë Latent
    curr_latent = generate_one_step_flow(model, condition, steps, cfg_scale, device)
    history_latents = [curr_latent]

    # 5. Ëá™ÂõûÂΩíÂæ™ÁéØ
    # ÈôêÂà∂ÊúÄÂ§ßÈïøÂ∫¶Èò≤Ê≠¢Ê≠ªÂæ™ÁéØ
    max_frames = 500 
    pbar = tqdm(range(max_frames), desc="Gen Audio", leave=False)
    
    for i in pbar:
        # ËæìÂÖ•: ÂΩìÂâçÁîüÊàêÁöÑ Latent
        input_latent = curr_latent
        # [Offset] ÂëäËØâ Projector ÂΩìÂâçÊòØÁ¨¨ i Â∏ß (i=0 ÂØπÂ∫î Audio ÁöÑÁ¨¨0Â∏ß)
        # „ÄêÂØπÂ∫îÂÖ≥Á≥ª„ÄëÔºöË∞ÉÁî® modeling_calm.py ‰∏≠ AudioInputProjector.forward(x, offset)
        curr_embeds = model.input_proj(input_latent, offset=i)
        
        # LLM Step: È¢ÑÊµã‰∏ã‰∏Ä‰∏™ Condition
        out = model.llm(inputs_embeds=curr_embeds, use_cache=True, past_key_values=past_kv, output_hidden_states=True)
        past_kv = out.past_key_values
        
        # Ëé∑Âèñ Condition
        condition = out.hidden_states[-1][:, -1:, :]
        
        # TODO: ÂèØ‰ª•Âú®ËøôÈáåÊ∑ªÂä† Stop Token Ê£ÄÊµãÈÄªËæë
        
        # Flow ÁîüÊàê‰∏ã‰∏ÄÂ∏ß
        curr_latent = generate_one_step_flow(model, condition, steps, cfg_scale, device)
        history_latents.append(curr_latent)

    # 6. Ëß£Á†Å‰∏∫Ê≥¢ÂΩ¢
    # [1, Latent, T]
    latents = torch.cat(history_latents, dim=1).transpose(1, 2).to(torch.float32) 
    
    # VAE Decode -> Mel Spectrogram
    # „ÄêÂØπÂ∫îÂÖ≥Á≥ª„ÄëÔºöË∞ÉÁî® modeling_vae.py ‰∏≠ÁöÑ AcousticVAE.decode
    mel = model.vae.decode(latents) # [1, 80, T]
    
    # Ë∞ÉËØïÂèØËßÜÂåñ: ‰øùÂ≠ò Mel È¢ëË∞±Âõæ
    if save_plot_path:
        mel_cpu = mel.squeeze().float().cpu().numpy()
        plt.figure(figsize=(10, 4))
        plt.imshow(mel_cpu, aspect='auto', origin='lower', interpolation='none')
        plt.colorbar()
        plt.title(f"Generated Mel (Text: {text[:20]}...)")
        plt.tight_layout()
        plt.savefig(save_plot_path)
        plt.close()
    
    # Vocoder Decode -> Waveform
    wav = vocoder.decode(mel)
    
    return wav.cpu()

def eval_task_tts(cfg, model, tokenizer, vocoder, data):
    """
    ÂäüËÉΩÔºöTTS ‰ªªÂä°ËØÑ‰º∞Âæ™ÁéØÔºåÁîüÊàêÈü≥È¢ëÂπ∂‰øùÂ≠ò„ÄÇ
    """
    wav_dir = os.path.join(cfg.evaluation.output_dir, "generated_wavs")
    os.makedirs(wav_dir, exist_ok=True)
    csv_file = open(os.path.join(cfg.evaluation.output_dir, "tts_results.csv"), "w", newline="", encoding="utf-8")
    writer = csv.writer(csv_file)
    writer.writerow(["id", "text", "wav_path"])
    
    console.print(f"[bold green]>>> Running TTS Evaluation (Steps={cfg.evaluation.flow_steps})[/bold green]")
    img_dir = os.path.join(cfg.evaluation.output_dir, "mel_plots")
    os.makedirs(img_dir, exist_ok=True)
    
    for i, item in enumerate(data):
        text = item.get("text", "")
        if not text: continue
        try:
            scale = cfg.evaluation.get("cfg_scale", 1.0)
            steps = cfg.evaluation.get("flow_steps", 10)
            
            wav = run_tts_inference(
                model, tokenizer, vocoder, text, 
                steps=steps, 
                cfg_scale=scale, 
                device=cfg.device,
                save_plot_path=os.path.join(img_dir, f"mel_{i}.png")
            )
            
            wav_np = wav.squeeze().numpy()
            save_path = os.path.join(wav_dir, f"sample_{i}.wav")
            sf.write(save_path, wav_np, 16000)
            
            writer.writerow([i, text, save_path])
            if (i+1) % 5 == 0: 
                console.print(f"[Sample {i+1}] Generated: {save_path}")
                
        except Exception as e:
            logger.error(f"Error sample {i}: {e}")

    csv_file.close()

# ==============================================================================
# Main Entry
# ==============================================================================
@hydra.main(version_base=None, config_path="../config", config_name="calm_config")
def main(cfg: DictConfig):
    set_seed(cfg.evaluation.get("seed", 42))
    os.makedirs(cfg.evaluation.output_dir, exist_ok=True)
    
    with open_dict(cfg): 
        cfg.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    if not cfg.evaluation.get("web_demo", False):
        wandb.init(project=cfg.evaluation.get("wandb_project", "Audio-CALM-Eval"), config=OmegaConf.to_container(cfg))

    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(cfg.model.qwen_path, trust_remote_code=True)
    
    # 1. Âä†ËΩΩÊ®°Âûã
    model = load_model(cfg, cfg.device)
    
    # 2. Âä†ËΩΩÊï∞ÊçÆ
    data = load_dataset_jsonl(cfg.evaluation.test_file, cfg.evaluation.max_samples)
    
    # 3. ‰ªªÂä°ÂàÜÂèë
    task = cfg.evaluation.task.lower()
    if task == "tts":
        vocoder = Vocoder(cfg.device)
        eval_task_tts(cfg, model, tokenizer, vocoder, data)
    elif task == "asr":
        eval_task_asr(cfg, model, tokenizer, data)
    elif task == "mix":
        vocoder = Vocoder(cfg.device)
        console.rule("[bold]Starting TTS Evaluation[/bold]")
        eval_task_tts(cfg, model, tokenizer, vocoder, data)
        console.rule("[bold]Starting ASR Evaluation[/bold]")
        eval_task_asr(cfg, model, tokenizer, data)
    else:
        logger.error(f"Unknown task: {task}")

if __name__ == "__main__":
    main()