[2025-12-17 19:32:41,443][eval][INFO] - Mode: tts (TTS=True, ASR=False)
[2025-12-17 19:34:54,844][eval][INFO] - Mode: tts (TTS=True, ASR=False)
[2025-12-17 19:36:46,875][eval][INFO] - Mode: tts (TTS=True, ASR=False)
[2025-12-17 19:36:48,942][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-17 19:36:50,080][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/calm_latent_flow/outputs/checkpoints/flow-mix-4-dim64-1e-4-2048-4/checkpoint-6000...
[2025-12-17 19:36:50,081][eval][WARNING] - âš ï¸ input_proj.bin not found in checkpoint!
[2025-12-17 19:36:50,082][eval][WARNING] - âš ï¸ output_head.bin not found in checkpoint!
[2025-12-17 19:36:53,451][eval][INFO] - Loading vocoder from speechbrain/tts-hifigan-libritts-16kHz...
[2025-12-17 19:36:54,240][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:36:54,241][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:36:54,243][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:36:54,244][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:36:54,245][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:36:54,247][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:36:54,247][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:36:54,247][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:36:54,247][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:36:54,247][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:36:54,248][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:36:54,248][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:36:54,248][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:36:54,248][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:36:54,274][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:38:05,909][eval][INFO] - Mode: tts (TTS=True, ASR=False)
[2025-12-17 19:38:07,799][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-17 19:38:08,918][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/outputs/checkpoints/flow-mix-4-dim64-1e-4-2048-4/checkpoint-6000...
[2025-12-17 19:38:08,919][eval][WARNING] - âš ï¸ input_proj.bin not found in checkpoint!
[2025-12-17 19:38:08,919][eval][WARNING] - âš ï¸ output_head.bin not found in checkpoint!
[2025-12-17 19:38:12,073][eval][INFO] - Loading vocoder from speechbrain/tts-hifigan-libritts-16kHz...
[2025-12-17 19:40:34,968][eval][INFO] - Mode: tts (TTS=True, ASR=False)
[2025-12-17 19:40:36,813][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-17 19:40:37,929][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/outputs/checkpoints/flow-mix-4-dim64-1e-4-2048-4/checkpoint-6000...
[2025-12-17 19:40:37,930][eval][WARNING] - âš ï¸ input_proj.bin not found in checkpoint!
[2025-12-17 19:40:37,930][eval][WARNING] - âš ï¸ output_head.bin not found in checkpoint!
[2025-12-17 19:40:41,120][eval][INFO] - Loading vocoder from speechbrain/tts-hifigan-libritts-16kHz...
[2025-12-17 19:40:41,140][speechbrain.utils.torch_audio_backend][WARNING] - SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html
[2025-12-17 19:40:41,148][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-17 19:40:41,149][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-17 19:40:41,149][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-17 19:40:41,149][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-17 19:40:41,267][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-17 19:40:41,267][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-17 19:40:41,268][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-17 19:40:41,269][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-17 19:40:41,278][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/hyperparams.yaml'
[2025-12-17 19:40:41,279][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-17 19:40:41,662][speechbrain.utils.torch_audio_backend][WARNING] - SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html
[2025-12-17 19:40:42,000][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan_checkpoints.
[2025-12-17 19:40:42,000][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt'
[2025-12-17 19:40:42,001][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-17 19:40:42,001][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-17 19:40:42,001][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-17 19:40:42,764][eval][INFO] - >>> Starting TTS Evaluation
[2025-12-17 19:41:22,015][eval][INFO] - Mode: tts (TTS=True, ASR=False)
[2025-12-17 19:41:23,907][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-17 19:41:25,063][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/flow-mix-4-dim64-1e-4-2048-4/checkpoint-6000...
[2025-12-17 19:41:29,082][eval][INFO] - Loaded input_proj.
[2025-12-17 19:41:29,168][eval][INFO] - Loaded output_head.
[2025-12-17 19:41:32,259][eval][INFO] - Loading vocoder from speechbrain/tts-hifigan-libritts-16kHz...
[2025-12-17 19:41:32,286][speechbrain.utils.torch_audio_backend][WARNING] - SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html
[2025-12-17 19:41:32,294][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-17 19:41:32,295][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-17 19:41:32,295][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-17 19:41:32,295][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-17 19:41:32,352][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-17 19:41:32,354][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-17 19:41:32,355][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-17 19:41:32,355][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-17 19:41:32,366][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/hyperparams.yaml'
[2025-12-17 19:41:32,366][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-17 19:41:32,658][speechbrain.utils.torch_audio_backend][WARNING] - SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html
[2025-12-17 19:41:32,773][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan_checkpoints.
[2025-12-17 19:41:32,774][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt'
[2025-12-17 19:41:32,774][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-17 19:41:32,774][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-17 19:41:32,775][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-17 19:41:33,548][eval][INFO] - >>> Starting TTS Evaluation
[2025-12-17 19:41:56,356][eval][ERROR] - TTS Failed on sample 0: Expected 1D or 2D tensor, got 3D tensor
[2025-12-17 19:42:09,943][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,943][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,945][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,946][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,946][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,947][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,950][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,950][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,950][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,950][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,951][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,951][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,951][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,951][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,952][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:42:09,957][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:44:05,084][eval][INFO] - Mode: tts (TTS=True, ASR=False)
[2025-12-17 19:44:06,836][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-17 19:44:07,960][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/flow-mix-4-dim64-1e-4-2048-4/checkpoint-6000...
[2025-12-17 19:44:11,922][eval][INFO] - Loaded input_proj.
[2025-12-17 19:44:11,996][eval][INFO] - Loaded output_head.
[2025-12-17 19:44:15,202][eval][INFO] - Loading vocoder from speechbrain/tts-hifigan-libritts-16kHz...
[2025-12-17 19:44:15,218][speechbrain.utils.torch_audio_backend][WARNING] - SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html
[2025-12-17 19:44:15,225][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-17 19:44:15,226][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-17 19:44:15,226][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-17 19:44:15,226][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-17 19:44:15,269][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-17 19:44:15,270][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-17 19:44:15,271][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-17 19:44:15,271][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-17 19:44:15,280][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/hyperparams.yaml'
[2025-12-17 19:44:15,281][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-17 19:44:15,974][speechbrain.utils.torch_audio_backend][WARNING] - SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html
[2025-12-17 19:44:16,084][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan_checkpoints.
[2025-12-17 19:44:16,084][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt'
[2025-12-17 19:44:16,085][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-17 19:44:16,085][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-17 19:44:16,086][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-17 19:44:16,837][eval][INFO] - >>> Starting TTS Evaluation
[2025-12-17 19:47:24,151][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,152][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,154][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,155][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,155][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,156][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,159][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,159][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,159][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,159][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,160][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,160][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,160][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,160][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,161][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,162][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:47:24,181][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 19:54:21,397][eval][INFO] - Mode: tts (TTS=True, ASR=False)
[2025-12-17 19:54:23,256][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-17 19:54:24,372][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/flow-mix-4-dim64-1e-4-2048-4/checkpoint-6000...
[2025-12-17 19:54:28,396][eval][INFO] - Loaded input_proj.
[2025-12-17 19:54:28,472][eval][INFO] - Loaded output_head.
[2025-12-17 19:54:31,561][eval][INFO] - Loading vocoder from speechbrain/tts-hifigan-libritts-16kHz...
[2025-12-17 19:54:31,577][speechbrain.utils.torch_audio_backend][WARNING] - SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html
[2025-12-17 19:54:31,583][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-17 19:54:31,584][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-17 19:54:31,584][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-17 19:54:31,584][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-17 19:54:31,623][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-17 19:54:31,624][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-17 19:54:31,625][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-17 19:54:31,625][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-17 19:54:31,634][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/hyperparams.yaml'
[2025-12-17 19:54:31,635][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-17 19:54:32,439][speechbrain.utils.torch_audio_backend][WARNING] - SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html
[2025-12-17 19:54:32,543][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan_checkpoints.
[2025-12-17 19:54:32,544][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt'
[2025-12-17 19:54:32,544][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-17 19:54:32,544][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-17 19:54:32,544][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-17 19:54:33,318][eval][INFO] - >>> Starting TTS Evaluation
[2025-12-17 19:54:33,319][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-17 19:54:33,594][eval][INFO] - âœ… Oracle test saved to /data0/determined/users/andywu/Audio-CALM-v2/outputs/eval_results_flow/eval-flow-mix-4-dim64-1e-4--2048-4/oracle_test. Please listen to 'oracle_denorm.wav' first!
[2025-12-17 19:59:21,723][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-17 19:59:22,848][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/flow-mix-4-dim64-1e-4-2048-4/checkpoint-6000...
[2025-12-17 19:59:30,162][eval][INFO] - Loading vocoder from speechbrain/tts-hifigan-libritts-16kHz...
[2025-12-17 19:59:30,178][speechbrain.utils.torch_audio_backend][WARNING] - SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html
[2025-12-17 19:59:30,184][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-17 19:59:30,185][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-17 19:59:30,185][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-17 19:59:30,185][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-17 19:59:30,224][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-17 19:59:30,224][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-17 19:59:30,225][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-17 19:59:30,225][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-17 19:59:30,235][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/hyperparams.yaml'
[2025-12-17 19:59:30,235][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-17 19:59:30,940][speechbrain.utils.torch_audio_backend][WARNING] - SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html
[2025-12-17 19:59:31,055][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan_checkpoints.
[2025-12-17 19:59:31,055][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt'
[2025-12-17 19:59:31,056][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-17 19:59:31,056][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-17 19:59:31,056][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-17 19:59:31,813][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-17 19:59:32,086][eval][INFO] - âœ… Oracle test saved. If this sounds good, VAE/Vocoder are fine.
[2025-12-17 20:02:26,072][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:02:26,072][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:02:26,075][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:02:26,075][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:02:26,076][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:02:26,076][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:02:26,080][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:02:26,080][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:02:26,081][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:02:26,081][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:02:26,081][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:02:26,081][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:02:26,082][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:02:26,087][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:02:26,102][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:08:38,136][eval][INFO] - Mode: tts (TTS=True, ASR=False)
[2025-12-17 20:08:40,026][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-17 20:08:41,159][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/flow-mix-4-dim64-1e-4-2048-4/checkpoint-6000...
[2025-12-17 20:08:45,266][eval][INFO] - Loaded input_proj.
[2025-12-17 20:08:45,340][eval][INFO] - Loaded output_head.
[2025-12-17 20:08:48,517][eval][INFO] - Loading vocoder from speechbrain/tts-hifigan-libritts-16kHz...
[2025-12-17 20:08:48,532][speechbrain.utils.torch_audio_backend][WARNING] - SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html
[2025-12-17 20:08:48,539][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-17 20:08:48,539][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-17 20:08:48,539][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-17 20:08:48,540][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-17 20:08:48,578][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-17 20:08:48,579][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-17 20:08:48,580][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-17 20:08:48,580][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-17 20:08:48,592][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/hyperparams.yaml'
[2025-12-17 20:08:48,592][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-17 20:08:49,376][speechbrain.utils.torch_audio_backend][WARNING] - SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html
[2025-12-17 20:08:49,491][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan_checkpoints.
[2025-12-17 20:08:49,492][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt'
[2025-12-17 20:08:49,492][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-17 20:08:49,492][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-17 20:08:49,492][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-17 20:08:50,282][eval][INFO] - >>> Starting TTS Evaluation
[2025-12-17 20:08:50,283][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-17 20:08:50,551][eval][INFO] - âœ… Oracle test saved to /data0/determined/users/andywu/Audio-CALM-v2/outputs/eval_results_flow/eval-flow-mix-4-dim64-1e-4--2048-4/oracle_test. Please listen to 'oracle_raw_norm.wav'!
[2025-12-17 20:09:43,286][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,287][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,289][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,289][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,290][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,290][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,291][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,294][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,295][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,295][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,295][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,295][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,295][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,295][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,296][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,296][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,301][asyncio][WARNING] - socket.send() raised exception.
[2025-12-17 20:09:43,316][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:15:26,042][eval][INFO] - Mode: tts (TTS=True, ASR=False)
[2025-12-18 13:15:28,022][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-18 13:15:29,410][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/flow/flow-mix-4-dim64-1e-4-2048-4/checkpoint-6594...
[2025-12-18 13:15:34,414][eval][INFO] - Loaded input_proj.
[2025-12-18 13:15:34,619][eval][INFO] - Loaded output_head.
[2025-12-18 13:15:56,322][eval][INFO] - Loading vocoder from speechbrain/tts-hifigan-libritts-16kHz...
[2025-12-18 13:15:56,338][speechbrain.utils.torch_audio_backend][WARNING] - SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html
[2025-12-18 13:15:56,345][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-18 13:15:56,345][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-18 13:15:56,346][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-18 13:15:56,346][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-18 13:15:56,384][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-18 13:15:56,385][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-18 13:15:56,386][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-18 13:15:56,386][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-18 13:15:56,396][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/hyperparams.yaml'
[2025-12-18 13:15:56,397][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-18 13:15:56,726][speechbrain.utils.torch_audio_backend][WARNING] - SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html
[2025-12-18 13:15:56,829][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan_checkpoints.
[2025-12-18 13:15:56,830][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt'
[2025-12-18 13:15:56,830][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-18 13:15:56,830][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-18 13:15:56,830][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-18 13:15:57,626][eval][INFO] - >>> Starting TTS Evaluation
[2025-12-18 13:15:57,627][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-18 13:15:57,934][eval][INFO] - âœ… Oracle test saved to /data0/determined/users/andywu/Audio-CALM-v2/outputs/eval_results_flow/eval-flow-mix-4-dim64-1e-4--2048-4/oracle_test. Please listen to 'oracle_raw_norm.wav'!
[2025-12-18 13:21:14,491][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:21:14,492][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:21:14,494][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:21:14,495][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:21:14,496][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:21:14,499][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:21:14,499][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:21:14,499][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:21:14,499][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:21:14,500][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:21:14,500][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:21:14,500][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:21:14,500][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:21:14,500][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:21:14,506][asyncio][WARNING] - socket.send() raised exception.
[2025-12-18 13:22:56,576][eval][INFO] - Mode: asr (TTS=False, ASR=True)
[2025-12-18 13:22:58,407][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-18 13:22:59,699][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/flow/flow-mix-4-dim64-1e-4-2048-4/checkpoint-6594...
[2025-12-18 13:23:03,719][eval][INFO] - Loaded input_proj.
[2025-12-18 13:23:03,789][eval][INFO] - Loaded output_head.
[2025-12-18 13:23:07,837][eval][INFO] - >>> Starting ASR Evaluation
[2025-12-18 13:24:50,780][eval][INFO] - Mode: asr (TTS=False, ASR=True)
[2025-12-18 13:24:52,601][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-18 13:24:53,895][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/flow/flow-mix-4-dim64-1e-4-2048-4/checkpoint-6594...
[2025-12-18 13:24:57,941][eval][INFO] - Loaded input_proj.
[2025-12-18 13:24:58,038][eval][INFO] - Loaded output_head.
[2025-12-18 13:25:01,880][eval][INFO] - >>> Starting ASR Evaluation
[2025-12-18 13:25:56,110][eval][INFO] - ðŸ”¥ ASR Evaluation Complete. Global WER: 0.3083
[2025-12-20 02:28:11,848][eval][INFO] - Mode: asr (TTS=False, ASR=True)
[2025-12-20 02:28:13,734][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-20 02:28:14,849][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/calm_moa_flow/flow-moa-asr-4-dim64-1e-4-2048-4/checkpoint-4000...
[2025-12-20 02:28:14,850][eval][INFO] - Found multiple adapters (MoA).
[2025-12-20 02:28:14,850][eval][INFO] - Loading ASR adapter from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/calm_moa_flow/flow-moa-asr-4-dim64-1e-4-2048-4/checkpoint-4000/asr
[2025-12-20 02:28:18,939][eval][INFO] - Loaded input_proj.
[2025-12-20 02:28:19,006][eval][INFO] - Loaded output_head.
[2025-12-20 02:28:23,677][eval][INFO] - >>> Starting ASR Evaluation
[2025-12-20 02:29:20,019][eval][INFO] - ðŸ”¥ ASR Evaluation Complete. Global WER: 0.1536
[2025-12-21 21:14:19,258][eval][INFO] - Mode: asr (TTS=False, ASR=True)
[2025-12-21 21:14:21,109][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-21 21:14:22,268][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/calm_moa_flow/flow-moa-asr-4-dim64-1e-4-2048-4/checkpoint-6594...
[2025-12-21 21:14:22,269][eval][INFO] - Found multiple adapters (MoA).
[2025-12-21 21:14:22,269][eval][INFO] - Loading ASR adapter from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/calm_moa_flow/flow-moa-asr-4-dim64-1e-4-2048-4/checkpoint-6594/asr
[2025-12-21 21:14:27,184][eval][INFO] - Loaded input_proj.
[2025-12-21 21:14:27,380][eval][INFO] - Loaded output_head.
[2025-12-21 21:14:32,579][eval][INFO] - >>> Starting ASR Evaluation
[2025-12-21 21:15:26,543][eval][INFO] - ðŸ”¥ ASR Evaluation Complete. Global WER: 0.1317
[2025-12-24 04:53:18,751][eval][INFO] - Mode: tts (TTS=True, ASR=False)
[2025-12-24 04:53:20,637][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-24 04:53:22,070][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/calm_moa_flow/flow-moa-tts-4-dim64-1e-4-2048-4/checkpoint-8316...
[2025-12-24 04:53:22,071][eval][INFO] - Found multiple adapters (MoA).
[2025-12-24 04:53:22,071][eval][INFO] - Loading TTS adapter from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/calm_moa_flow/flow-moa-tts-4-dim64-1e-4-2048-4/checkpoint-8316/tts
[2025-12-24 04:53:26,503][eval][INFO] - Loaded input_proj.
[2025-12-24 04:53:26,709][eval][INFO] - Loaded output_head.
[2025-12-24 04:53:48,585][eval][INFO] - Loading vocoder from speechbrain/tts-hifigan-libritts-16kHz...
[2025-12-24 04:53:48,616][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 04:53:48,617][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 04:53:48,617][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 04:53:48,617][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 04:53:48,638][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 04:53:48,639][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 04:53:48,640][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 04:53:48,640][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 04:53:48,652][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/hyperparams.yaml'
[2025-12-24 04:53:48,652][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 04:53:49,095][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan_checkpoints.
[2025-12-24 04:53:49,095][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt'
[2025-12-24 04:53:49,096][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-24 04:53:49,096][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 04:53:49,096][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-24 04:53:49,877][eval][INFO] - >>> Starting TTS Evaluation
[2025-12-24 04:53:49,878][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 04:53:49,878][eval][WARNING] - Could not find latent for oracle test.
[2025-12-24 04:54:17,935][eval][ERROR] - TTS Failed on sample 0: Could not load libtorchcodec. Likely causes:
          1. FFmpeg is not properly installed in your environment. We support
             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed
             the "full-shared" version which ships DLLs.
          2. The PyTorch version (2.5.1+cu124) is not compatible with
             this version of TorchCodec. Refer to the version compatibility
             table:
             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.
          3. Another runtime dependency; see exceptions below.
        The following exceptions were raised as we tried to load libtorchcodec:
        
[start of libtorchcodec loading traceback]
FFmpeg version 8: libavutil.so.60: cannot open shared object file: No such file or directory
FFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory
FFmpeg version 6: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core6.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
FFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory
FFmpeg version 4: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core4.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
[end of libtorchcodec loading traceback].
[2025-12-24 04:54:41,255][eval][ERROR] - TTS Failed on sample 1: Could not load libtorchcodec. Likely causes:
          1. FFmpeg is not properly installed in your environment. We support
             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed
             the "full-shared" version which ships DLLs.
          2. The PyTorch version (2.5.1+cu124) is not compatible with
             this version of TorchCodec. Refer to the version compatibility
             table:
             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.
          3. Another runtime dependency; see exceptions below.
        The following exceptions were raised as we tried to load libtorchcodec:
        
[start of libtorchcodec loading traceback]
FFmpeg version 8: libavutil.so.60: cannot open shared object file: No such file or directory
FFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory
FFmpeg version 6: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core6.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
FFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory
FFmpeg version 4: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core4.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
[end of libtorchcodec loading traceback].
[2025-12-24 04:55:04,577][eval][ERROR] - TTS Failed on sample 2: Could not load libtorchcodec. Likely causes:
          1. FFmpeg is not properly installed in your environment. We support
             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed
             the "full-shared" version which ships DLLs.
          2. The PyTorch version (2.5.1+cu124) is not compatible with
             this version of TorchCodec. Refer to the version compatibility
             table:
             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.
          3. Another runtime dependency; see exceptions below.
        The following exceptions were raised as we tried to load libtorchcodec:
        
[start of libtorchcodec loading traceback]
FFmpeg version 8: libavutil.so.60: cannot open shared object file: No such file or directory
FFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory
FFmpeg version 6: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core6.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
FFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory
FFmpeg version 4: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core4.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
[end of libtorchcodec loading traceback].
[2025-12-24 04:55:27,819][eval][ERROR] - TTS Failed on sample 3: Could not load libtorchcodec. Likely causes:
          1. FFmpeg is not properly installed in your environment. We support
             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed
             the "full-shared" version which ships DLLs.
          2. The PyTorch version (2.5.1+cu124) is not compatible with
             this version of TorchCodec. Refer to the version compatibility
             table:
             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.
          3. Another runtime dependency; see exceptions below.
        The following exceptions were raised as we tried to load libtorchcodec:
        
[start of libtorchcodec loading traceback]
FFmpeg version 8: libavutil.so.60: cannot open shared object file: No such file or directory
FFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory
FFmpeg version 6: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core6.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
FFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory
FFmpeg version 4: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core4.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
[end of libtorchcodec loading traceback].
[2025-12-24 04:55:45,115][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,115][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,118][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,118][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,119][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,120][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,120][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,121][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,121][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,122][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,125][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,125][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,125][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,125][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,126][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,126][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,126][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:55:45,126][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 04:57:45,810][eval][INFO] - Mode: tts (TTS=True, ASR=False)
[2025-12-24 04:57:47,759][eval][INFO] - Loading base model from /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct...
[2025-12-24 04:57:49,043][eval][INFO] - Loading weights from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/calm_moa_flow/flow-moa-tts-4-dim64-1e-4-2048-4/checkpoint-8316...
[2025-12-24 04:57:49,043][eval][INFO] - Found multiple adapters (MoA).
[2025-12-24 04:57:49,044][eval][INFO] - Loading TTS adapter from /data0/determined/users/andywu/Audio-CALM-v2/outputs/checkpoints/calm_moa_flow/flow-moa-tts-4-dim64-1e-4-2048-4/checkpoint-8316/tts
[2025-12-24 04:57:52,533][eval][INFO] - Loaded input_proj.
[2025-12-24 04:57:52,602][eval][INFO] - Loaded output_head.
[2025-12-24 04:57:56,056][eval][INFO] - Loading vocoder from speechbrain/tts-hifigan-libritts-16kHz...
[2025-12-24 04:57:56,077][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 04:57:56,077][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 04:57:56,077][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 04:57:56,078][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 04:57:56,098][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-24 04:57:56,098][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 04:57:56,099][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 04:57:56,099][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 04:57:56,108][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/hyperparams.yaml'
[2025-12-24 04:57:56,109][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 04:57:56,516][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan_checkpoints.
[2025-12-24 04:57:56,517][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt'
[2025-12-24 04:57:56,517][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-24 04:57:56,517][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 04:57:56,517][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan_checkpoints/generator.ckpt
[2025-12-24 04:57:57,257][eval][INFO] - >>> Starting TTS Evaluation
[2025-12-24 04:57:57,257][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 04:57:57,258][eval][WARNING] - Could not find latent for oracle test.
[2025-12-24 04:58:24,779][eval][ERROR] - TTS Failed on sample 0: Could not load libtorchcodec. Likely causes:
          1. FFmpeg is not properly installed in your environment. We support
             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed
             the "full-shared" version which ships DLLs.
          2. The PyTorch version (2.5.1+cu124) is not compatible with
             this version of TorchCodec. Refer to the version compatibility
             table:
             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.
          3. Another runtime dependency; see exceptions below.
        The following exceptions were raised as we tried to load libtorchcodec:
        
[start of libtorchcodec loading traceback]
FFmpeg version 8: libavutil.so.60: cannot open shared object file: No such file or directory
FFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory
FFmpeg version 6: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core6.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
FFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory
FFmpeg version 4: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core4.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
[end of libtorchcodec loading traceback].
[2025-12-24 04:58:48,055][eval][ERROR] - TTS Failed on sample 1: Could not load libtorchcodec. Likely causes:
          1. FFmpeg is not properly installed in your environment. We support
             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed
             the "full-shared" version which ships DLLs.
          2. The PyTorch version (2.5.1+cu124) is not compatible with
             this version of TorchCodec. Refer to the version compatibility
             table:
             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.
          3. Another runtime dependency; see exceptions below.
        The following exceptions were raised as we tried to load libtorchcodec:
        
[start of libtorchcodec loading traceback]
FFmpeg version 8: libavutil.so.60: cannot open shared object file: No such file or directory
FFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory
FFmpeg version 6: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core6.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
FFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory
FFmpeg version 4: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core4.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
[end of libtorchcodec loading traceback].
[2025-12-24 04:59:52,610][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 04:59:54,079][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 04:59:57,134][eval][INFO] - Loaded input_proj.
[2025-12-24 04:59:57,203][eval][INFO] - Loaded output_head.
[2025-12-24 05:00:00,363][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:00:00,590][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 05:00:00,590][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 05:00:00,591][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 05:00:00,591][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 05:00:00,610][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 05:00:00,610][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 05:00:00,611][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 05:00:00,611][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 05:00:00,620][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 05:00:00,913][speechbrain.utils.fetching][DEBUG] - Fetch: Local file found, creating symlink '/home/andywu/.cache/huggingface/hub/models--speechbrain--tts-hifigan-libritts-16kHz/snapshots/4d32c6f19e7ff9316c5c56d7903d3d345ddcace4/hyperparams.yaml' -> '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 05:00:00,914][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 05:00:01,264][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 05:00:01,265][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 05:00:01,551][speechbrain.utils.fetching][DEBUG] - Fetch: Local file found, creating symlink '/home/andywu/.cache/huggingface/hub/models--speechbrain--tts-hifigan-libritts-16kHz/snapshots/4d32c6f19e7ff9316c5c56d7903d3d345ddcace4/generator.ckpt' -> '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 05:00:01,551][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:00:01,551][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 05:00:01,552][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:00:01,615][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 05:00:03,441][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 05:00:03,570][eval][ERROR] - Oracle test failed: Input type (c10::BFloat16) and bias type (float) should be the same
[2025-12-24 05:00:27,627][eval][ERROR] - Error on sample 0: Input type (float) and bias type (c10::BFloat16) should be the same
[2025-12-24 05:00:36,447][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,447][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,449][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,450][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,451][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,454][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,454][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,455][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,455][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,455][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,455][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,455][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,458][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,459][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,459][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,459][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,459][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,459][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,459][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,459][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,459][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,459][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,459][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:00:36,460][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:02:53,246][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 05:02:54,714][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 05:02:57,845][eval][INFO] - Loaded input_proj.
[2025-12-24 05:02:57,912][eval][INFO] - Loaded output_head.
[2025-12-24 05:03:01,087][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 05:03:01,112][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:03:01,385][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 05:03:01,386][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 05:03:01,386][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 05:03:01,386][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 05:03:01,406][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-24 05:03:01,406][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 05:03:01,407][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 05:03:01,407][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 05:03:01,417][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 05:03:01,417][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 05:03:01,803][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 05:03:01,804][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 05:03:01,804][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:03:01,804][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 05:03:01,804][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:03:01,856][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 05:03:03,658][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 05:03:03,864][eval][INFO] - âœ… Oracle test saved to /data0/determined/users/andywu/Audio-CALM-v2/outputs/eval_results_moa/eval-flow-tts-checkpoint-5000/oracle_test.
[2025-12-24 05:03:27,936][eval][ERROR] - Error on sample 0: Could not load libtorchcodec. Likely causes:
          1. FFmpeg is not properly installed in your environment. We support
             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed
             the "full-shared" version which ships DLLs.
          2. The PyTorch version (2.5.1+cu124) is not compatible with
             this version of TorchCodec. Refer to the version compatibility
             table:
             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.
          3. Another runtime dependency; see exceptions below.
        The following exceptions were raised as we tried to load libtorchcodec:
        
[start of libtorchcodec loading traceback]
FFmpeg version 8: libavutil.so.60: cannot open shared object file: No such file or directory
FFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory
FFmpeg version 6: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core6.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
FFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory
FFmpeg version 4: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core4.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
[end of libtorchcodec loading traceback].
[2025-12-24 05:03:39,882][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,883][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,885][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,885][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,890][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,890][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,890][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,890][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,890][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,890][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,892][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,894][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,894][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,894][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,894][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,894][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,894][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:39,895][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:03:54,295][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 05:03:55,607][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 05:03:58,899][eval][INFO] - Loaded input_proj.
[2025-12-24 05:03:58,985][eval][INFO] - Loaded output_head.
[2025-12-24 05:04:02,217][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 05:04:02,243][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:04:02,270][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 05:04:02,270][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 05:04:02,270][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 05:04:02,271][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 05:04:02,301][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-24 05:04:02,302][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 05:04:02,303][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 05:04:02,303][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 05:04:02,313][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 05:04:02,313][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 05:04:02,710][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 05:04:02,711][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 05:04:02,711][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:04:02,711][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 05:04:02,711][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:04:02,759][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 05:04:04,705][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 05:04:04,913][eval][INFO] - âœ… Oracle test saved to /data0/determined/users/andywu/Audio-CALM-v2/outputs/eval_results_moa/eval-flow-tts-checkpoint-5000/oracle_test.
[2025-12-24 05:04:28,972][eval][ERROR] - Error on sample 0: Could not load libtorchcodec. Likely causes:
          1. FFmpeg is not properly installed in your environment. We support
             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed
             the "full-shared" version which ships DLLs.
          2. The PyTorch version (2.5.1+cu124) is not compatible with
             this version of TorchCodec. Refer to the version compatibility
             table:
             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.
          3. Another runtime dependency; see exceptions below.
        The following exceptions were raised as we tried to load libtorchcodec:
        
[start of libtorchcodec loading traceback]
FFmpeg version 8: libavutil.so.60: cannot open shared object file: No such file or directory
FFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory
FFmpeg version 6: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core6.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
FFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory
FFmpeg version 4: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core4.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
[end of libtorchcodec loading traceback].
[2025-12-24 05:04:51,862][eval][ERROR] - Error on sample 1: Could not load libtorchcodec. Likely causes:
          1. FFmpeg is not properly installed in your environment. We support
             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed
             the "full-shared" version which ships DLLs.
          2. The PyTorch version (2.5.1+cu124) is not compatible with
             this version of TorchCodec. Refer to the version compatibility
             table:
             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.
          3. Another runtime dependency; see exceptions below.
        The following exceptions were raised as we tried to load libtorchcodec:
        
[start of libtorchcodec loading traceback]
FFmpeg version 8: libavutil.so.60: cannot open shared object file: No such file or directory
FFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory
FFmpeg version 6: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core6.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
FFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory
FFmpeg version 4: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core4.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
[end of libtorchcodec loading traceback].
[2025-12-24 05:05:14,774][eval][ERROR] - Error on sample 2: Could not load libtorchcodec. Likely causes:
          1. FFmpeg is not properly installed in your environment. We support
             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed
             the "full-shared" version which ships DLLs.
          2. The PyTorch version (2.5.1+cu124) is not compatible with
             this version of TorchCodec. Refer to the version compatibility
             table:
             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.
          3. Another runtime dependency; see exceptions below.
        The following exceptions were raised as we tried to load libtorchcodec:
        
[start of libtorchcodec loading traceback]
FFmpeg version 8: libavutil.so.60: cannot open shared object file: No such file or directory
FFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory
FFmpeg version 6: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core6.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
FFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory
FFmpeg version 4: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core4.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
[end of libtorchcodec loading traceback].
[2025-12-24 05:05:37,699][eval][ERROR] - Error on sample 3: Could not load libtorchcodec. Likely causes:
          1. FFmpeg is not properly installed in your environment. We support
             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed
             the "full-shared" version which ships DLLs.
          2. The PyTorch version (2.5.1+cu124) is not compatible with
             this version of TorchCodec. Refer to the version compatibility
             table:
             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.
          3. Another runtime dependency; see exceptions below.
        The following exceptions were raised as we tried to load libtorchcodec:
        
[start of libtorchcodec loading traceback]
FFmpeg version 8: libavutil.so.60: cannot open shared object file: No such file or directory
FFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory
FFmpeg version 6: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core6.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
FFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory
FFmpeg version 4: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core4.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
[end of libtorchcodec loading traceback].
[2025-12-24 05:06:00,610][eval][ERROR] - Error on sample 4: Could not load libtorchcodec. Likely causes:
          1. FFmpeg is not properly installed in your environment. We support
             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed
             the "full-shared" version which ships DLLs.
          2. The PyTorch version (2.5.1+cu124) is not compatible with
             this version of TorchCodec. Refer to the version compatibility
             table:
             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.
          3. Another runtime dependency; see exceptions below.
        The following exceptions were raised as we tried to load libtorchcodec:
        
[start of libtorchcodec loading traceback]
FFmpeg version 8: libavutil.so.60: cannot open shared object file: No such file or directory
FFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory
FFmpeg version 6: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core6.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
FFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory
FFmpeg version 4: /data0/determined/users/andywu/config/.conda/envs/qwen2_CALM/lib/python3.10/site-packages/torchcodec/libtorchcodec_core4.so: undefined symbol: _ZNK3c106Device3strB5cxx11Ev
[end of libtorchcodec loading traceback].
[2025-12-24 05:06:23,432][eval][ERROR] - Error on sample 5: No module named 'torchcodec'
[2025-12-24 05:06:46,287][eval][ERROR] - Error on sample 6: No module named 'torchcodec'
[2025-12-24 05:07:09,172][eval][ERROR] - Error on sample 7: No module named 'torchcodec'
[2025-12-24 05:07:12,897][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,897][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,904][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,904][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,904][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,904][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,904][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,906][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,908][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,908][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,908][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,908][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,908][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,908][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,908][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:12,908][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:07:25,040][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 05:07:26,312][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 05:07:29,515][eval][INFO] - Loaded input_proj.
[2025-12-24 05:07:29,605][eval][INFO] - Loaded output_head.
[2025-12-24 05:07:33,253][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 05:07:33,281][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:07:33,311][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 05:07:33,311][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 05:07:33,311][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 05:07:33,312][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 05:07:33,344][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 05:07:33,344][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 05:07:33,345][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 05:07:33,345][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 05:07:33,360][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 05:07:33,360][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 05:07:33,850][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 05:07:33,852][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 05:07:33,852][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:07:33,852][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 05:07:33,852][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:07:33,946][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 05:07:35,774][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 05:07:36,048][eval][INFO] - âœ… Oracle test saved to /data0/determined/users/andywu/Audio-CALM-v2/outputs/eval_results_moa/eval-flow-tts-checkpoint-5000/oracle_test.
[2025-12-24 05:14:05,026][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,026][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,030][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,031][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,035][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,035][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,035][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,035][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,035][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,037][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,039][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,039][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,039][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,039][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,039][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,039][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,039][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,039][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,039][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,039][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,039][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:14:05,039][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:10,018][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 05:16:11,300][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 05:16:14,604][eval][INFO] - Loaded input_proj.
[2025-12-24 05:16:14,677][eval][INFO] - Loaded output_head.
[2025-12-24 05:16:17,998][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 05:16:18,023][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:16:18,048][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 05:16:18,048][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 05:16:18,049][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 05:16:18,049][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 05:16:18,069][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 05:16:18,069][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 05:16:18,070][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 05:16:18,070][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 05:16:18,079][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 05:16:18,079][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 05:16:18,472][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 05:16:18,473][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 05:16:18,473][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:16:18,473][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 05:16:18,474][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:16:18,533][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 05:16:20,369][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 05:16:20,515][eval][ERROR] - Oracle test failed: istft(CUDAComplexFloatType[1, 80, 256], n_fft=1024, hop_length=256, win_length=1024, window=torch.cuda.FloatTensor{[1024]}, center=1, normalized=0, onesided=None, length=None, return_complex=0) : expected the frequency dimension (3rd to the last) of the input tensor to match n_fft / 2 + 1 when onesided=True, but got 80
[2025-12-24 05:16:44,239][eval][ERROR] - Error on sample 0: istft(CUDAComplexFloatType[1, 80, 1200], n_fft=1024, hop_length=256, win_length=1024, window=torch.cuda.FloatTensor{[1024]}, center=1, normalized=0, onesided=None, length=None, return_complex=0) : expected the frequency dimension (3rd to the last) of the input tensor to match n_fft / 2 + 1 when onesided=True, but got 80
[2025-12-24 05:16:55,194][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,194][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,201][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,201][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,201][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,201][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,202][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,204][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,206][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,206][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,206][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,206][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,206][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,206][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:16:55,206][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:17:53,306][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 05:17:54,591][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 05:17:57,788][eval][INFO] - Loaded input_proj.
[2025-12-24 05:17:57,856][eval][INFO] - Loaded output_head.
[2025-12-24 05:18:00,958][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 05:18:00,983][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:18:01,008][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 05:18:01,008][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 05:18:01,009][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 05:18:01,009][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 05:18:01,032][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 05:18:01,033][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 05:18:01,033][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 05:18:01,034][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 05:18:01,042][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 05:18:01,043][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 05:18:01,413][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 05:18:01,414][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 05:18:01,414][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:18:01,414][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 05:18:01,415][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:18:01,627][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 05:18:03,417][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 05:18:03,544][eval][ERROR] - Oracle test failed: Expected size for first two dimensions of batch2 tensor to be: [1, 513] but got: [1, 80].
[2025-12-24 05:18:27,571][eval][ERROR] - Error on sample 0: Expected size for first two dimensions of batch2 tensor to be: [1, 513] but got: [1, 80].
[2025-12-24 05:18:34,848][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,848][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,850][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,851][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,851][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,851][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,851][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,851][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,852][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,852][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,852][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,852][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,852][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,852][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,853][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,853][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,853][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,853][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,853][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,853][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,853][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,854][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,854][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,854][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,854][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,854][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,858][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,858][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,859][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,859][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,859][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:18:34,859][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:19:59,051][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 05:20:00,353][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 05:20:03,627][eval][INFO] - Loaded input_proj.
[2025-12-24 05:20:03,695][eval][INFO] - Loaded output_head.
[2025-12-24 05:20:06,826][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 05:20:06,852][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:23:09,049][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 05:23:10,323][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 05:23:13,633][eval][INFO] - Loaded input_proj.
[2025-12-24 05:23:13,708][eval][INFO] - Loaded output_head.
[2025-12-24 05:23:16,849][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 05:23:16,874][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:23:16,971][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 05:23:19,279][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 05:23:19,757][eval][INFO] - âœ… Oracle test saved to /data0/determined/users/andywu/Audio-CALM-v2/outputs/eval_results_moa/eval-flow-tts-checkpoint-5000/oracle_test.
[2025-12-24 05:25:49,679][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,680][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,682][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,682][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,683][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,684][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,684][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,688][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,688][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,688][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,688][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,689][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,689][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,692][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,692][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,692][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,692][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,692][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,692][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,692][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,692][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,693][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,693][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,693][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:25:49,696][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:24,232][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 05:35:25,529][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 05:35:28,816][eval][INFO] - Loaded input_proj.
[2025-12-24 05:35:28,895][eval][INFO] - Loaded output_head.
[2025-12-24 05:35:33,389][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 05:35:33,421][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:35:33,544][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 05:35:35,574][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 05:35:35,747][eval][INFO] - âœ… Oracle test saved to /data0/determined/users/andywu/Audio-CALM-v2/outputs/eval_results_moa/eval-flow-tts-checkpoint-5000/oracle_test.
[2025-12-24 05:35:54,470][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,471][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,473][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,474][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,474][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,475][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,481][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,481][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,481][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,481][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,482][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,482][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,482][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,483][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,486][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,486][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,487][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,487][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,487][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,487][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,487][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,487][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,487][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,487][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:35:54,491][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:07,271][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 05:36:08,584][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 05:36:11,955][eval][INFO] - Loaded input_proj.
[2025-12-24 05:36:12,028][eval][INFO] - Loaded output_head.
[2025-12-24 05:36:15,513][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 05:36:15,540][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:36:15,632][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 05:36:17,463][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 05:36:17,622][eval][INFO] - âœ… Oracle test saved to /data0/determined/users/andywu/Audio-CALM-v2/outputs/eval_results_moa/eval-flow-tts-checkpoint-5000/oracle_test.
[2025-12-24 05:36:49,264][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,264][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,267][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,268][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,272][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,272][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,272][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,272][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,273][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,273][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,273][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,275][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,277][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,277][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,277][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,277][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,278][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:36:49,278][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:39:38,484][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 05:39:39,795][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 05:39:43,128][eval][INFO] - Loaded input_proj.
[2025-12-24 05:39:43,196][eval][INFO] - Loaded output_head.
[2025-12-24 05:39:46,279][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 05:39:46,305][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:39:46,330][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 05:39:46,330][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 05:39:46,331][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 05:39:46,331][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 05:39:46,351][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 05:39:46,352][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 05:39:46,352][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 05:39:46,353][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 05:39:46,361][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 05:39:46,362][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 05:39:46,751][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 05:39:46,752][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 05:39:46,752][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:39:46,753][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 05:39:46,753][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:39:46,802][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 05:39:48,606][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 05:39:48,814][eval][INFO] - âœ… Oracle test saved to /data0/determined/users/andywu/Audio-CALM-v2/outputs/eval_results_moa/eval-flow-tts-checkpoint-5000/oracle_test.
[2025-12-24 05:41:16,108][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,108][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,111][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,111][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,112][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,117][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,117][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,117][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,118][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,118][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,118][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,119][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,122][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,122][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,122][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,122][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,122][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,122][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,122][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,122][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,122][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,123][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,123][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,123][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:41:16,123][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:45:55,035][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 05:45:56,320][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 05:45:59,389][eval][INFO] - Loaded input_proj.
[2025-12-24 05:45:59,455][eval][INFO] - Loaded output_head.
[2025-12-24 05:46:02,532][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 05:46:02,557][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:46:02,582][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 05:46:02,583][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 05:46:02,583][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 05:46:02,583][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 05:46:02,603][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-24 05:46:02,604][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 05:46:02,605][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 05:46:02,605][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 05:46:02,614][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 05:46:02,614][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 05:46:02,979][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 05:46:02,980][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 05:46:02,980][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:46:02,980][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 05:46:02,980][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:46:03,027][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 05:46:04,940][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 05:46:05,146][eval][INFO] - âœ… Oracle test saved to /data0/determined/users/andywu/Audio-CALM-v2/outputs/eval_results_moa/eval-flow-tts-checkpoint-5000/oracle_test.
[2025-12-24 05:48:05,503][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 05:48:06,771][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 05:48:09,968][eval][INFO] - Loaded input_proj.
[2025-12-24 05:48:10,036][eval][INFO] - Loaded output_head.
[2025-12-24 05:48:13,132][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 05:48:13,158][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:48:13,182][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 05:48:13,183][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 05:48:13,183][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 05:48:13,183][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 05:48:13,203][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 05:48:13,204][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 05:48:13,204][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 05:48:13,205][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 05:48:13,213][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 05:48:13,214][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 05:48:13,583][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 05:48:13,585][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 05:48:13,585][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:48:13,585][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 05:48:13,585][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:48:13,632][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 05:48:15,448][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Tests...
[2025-12-24 05:48:15,703][eval][ERROR] - Oracle test failed: istft(CUDAComplexFloatType[1, 80, 256], n_fft=1024, hop_length=256, win_length=1024, window=torch.cuda.FloatTensor{[1024]}, center=1, normalized=0, onesided=None, length=None, return_complex=0) : expected the frequency dimension (3rd to the last) of the input tensor to match n_fft / 2 + 1 when onesided=True, but got 80
[2025-12-24 05:48:33,959][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:48:33,959][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:48:33,966][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:48:33,967][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:48:33,967][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:48:33,967][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:48:33,971][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:48:33,971][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:48:33,971][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:48:33,971][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:48:33,971][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:48:33,972][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:48:33,972][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:49:31,239][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 05:49:32,539][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 05:49:35,842][eval][INFO] - Loaded input_proj.
[2025-12-24 05:49:35,910][eval][INFO] - Loaded output_head.
[2025-12-24 05:49:39,101][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 05:49:39,127][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:49:39,152][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 05:49:39,153][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 05:49:39,153][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 05:49:39,153][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 05:49:39,175][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 05:49:39,175][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 05:49:39,176][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 05:49:39,176][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 05:49:39,185][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 05:49:39,185][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 05:49:39,552][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 05:49:39,554][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 05:49:39,555][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:49:39,555][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 05:49:39,555][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:49:39,603][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 05:49:41,430][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Tests...
[2025-12-24 05:51:49,393][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,394][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,396][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,397][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,401][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,401][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,401][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,401][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,402][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,402][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,404][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,406][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,406][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,406][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,406][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,406][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,407][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,407][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:51:49,407][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:03,926][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 05:52:05,267][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 05:52:08,677][eval][INFO] - Loaded input_proj.
[2025-12-24 05:52:08,758][eval][INFO] - Loaded output_head.
[2025-12-24 05:52:11,960][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 05:52:11,987][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 05:52:12,013][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 05:52:12,014][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 05:52:12,014][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 05:52:12,014][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 05:52:12,039][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-24 05:52:12,039][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 05:52:12,040][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 05:52:12,040][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 05:52:12,049][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 05:52:12,050][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 05:52:12,418][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 05:52:12,419][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 05:52:12,419][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:52:12,419][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 05:52:12,420][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 05:52:12,468][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 05:52:14,284][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Tests...
[2025-12-24 05:52:28,666][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,667][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,669][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,669][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,669][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,670][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,671][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,675][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,675][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,676][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,676][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,676][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,676][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,680][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,680][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,680][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,681][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,681][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 05:52:28,681][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:00,903][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 06:02:02,187][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 06:02:05,419][eval][INFO] - Loaded input_proj.
[2025-12-24 06:02:05,487][eval][INFO] - Loaded output_head.
[2025-12-24 06:02:08,636][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 06:02:08,661][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 06:02:08,686][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 06:02:08,687][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 06:02:08,687][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 06:02:08,687][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 06:02:08,709][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 06:02:08,710][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 06:02:08,711][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 06:02:08,711][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 06:02:08,720][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 06:02:08,720][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 06:02:09,173][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 06:02:09,174][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 06:02:09,174][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 06:02:09,174][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 06:02:09,175][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 06:02:09,223][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 06:02:11,099][eval][INFO] - ðŸ›¡ï¸ æ­£åœ¨è¿è¡Œã€åœ¨çº¿ä¸€è‡´æ€§æµ‹è¯•ã€‘(Sanity Check)...
[2025-12-24 06:02:11,099][eval][WARNING] - æ— æ³•æ‰¾åˆ° WAV æ–‡ä»¶è¿›è¡Œ Sanity Checkï¼Œè·³è¿‡ã€‚è·¯å¾„: /data0/determined/users/andywu/Audio-CALM-v2/data/latents/dev/LibriTTS_R/dev-clean/6345/93302/6345_93302_000062_000002.pt
[2025-12-24 06:02:58,123][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,123][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,125][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,126][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,127][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,131][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,132][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,132][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,132][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,132][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,132][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,132][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,135][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,136][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,136][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,136][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,136][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,136][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,136][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,136][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,136][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,136][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,137][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,137][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,137][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:02:58,137][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:03:48,930][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 06:03:50,241][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 06:03:53,571][eval][INFO] - Loaded input_proj.
[2025-12-24 06:03:53,653][eval][INFO] - Loaded output_head.
[2025-12-24 06:03:56,851][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 06:03:56,877][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 06:03:56,903][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 06:03:56,903][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 06:03:56,904][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 06:03:56,904][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 06:03:56,930][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 06:03:56,930][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 06:03:56,931][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 06:03:56,931][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 06:03:56,940][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 06:03:56,941][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 06:03:57,302][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 06:03:57,302][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 06:03:57,303][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 06:03:57,303][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 06:03:57,303][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 06:03:57,351][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 06:03:59,116][eval][INFO] - ðŸ›¡ï¸ æ­£åœ¨è¿è¡Œã€åœ¨çº¿ä¸€è‡´æ€§æµ‹è¯•ã€‘(Sanity Check)...
[2025-12-24 06:05:22,970][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 06:05:24,266][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 06:05:27,625][eval][INFO] - Loaded input_proj.
[2025-12-24 06:05:27,699][eval][INFO] - Loaded output_head.
[2025-12-24 06:05:30,880][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 06:05:30,905][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 06:05:30,931][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 06:05:30,931][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 06:05:30,932][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 06:05:30,932][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 06:05:30,953][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 06:05:30,954][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 06:05:30,954][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 06:05:30,955][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 06:05:30,964][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 06:05:30,964][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 06:05:31,321][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 06:05:31,322][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 06:05:31,322][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 06:05:31,323][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 06:05:31,323][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 06:05:31,370][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 06:05:33,205][eval][INFO] - ðŸ›¡ï¸ æ­£åœ¨è¿è¡Œã€åœ¨çº¿ä¸€è‡´æ€§æµ‹è¯•ã€‘(Sanity Check)...
[2025-12-24 06:05:33,205][eval][WARNING] - æ— æ³•æ‰¾åˆ° WAV æ–‡ä»¶è¿›è¡Œ Sanity Checkï¼Œè·³è¿‡ã€‚è·¯å¾„: /data0/determined/users/andywu/Audio-CALM-v2/data/latents/dev/LibriTTS_R/dev-clean/6345/93302/6345_93302_000062_000002.pt
[2025-12-24 06:05:42,363][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,363][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,366][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,366][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,366][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,367][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,368][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,372][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,372][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,373][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,373][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,373][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,373][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,373][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,374][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,378][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,378][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:05:42,381][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:10,882][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 06:06:12,174][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 06:06:15,478][eval][INFO] - Loaded input_proj.
[2025-12-24 06:06:15,562][eval][INFO] - Loaded output_head.
[2025-12-24 06:06:18,686][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 06:06:18,712][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 06:06:18,737][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 06:06:18,738][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 06:06:18,738][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 06:06:18,738][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 06:06:18,763][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-24 06:06:18,763][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 06:06:18,764][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 06:06:18,764][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 06:06:18,773][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 06:06:18,774][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 06:06:19,137][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 06:06:19,138][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 06:06:19,138][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 06:06:19,138][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 06:06:19,138][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 06:06:19,186][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 06:06:20,982][eval][INFO] - ðŸ›¡ï¸ æ­£åœ¨è¿è¡Œã€åœ¨çº¿ä¸€è‡´æ€§æµ‹è¯•ã€‘(Sanity Check)...
[2025-12-24 06:06:53,091][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,092][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,099][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,099][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,099][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,100][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,100][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,104][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,104][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,104][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,104][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,104][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,104][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,104][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,104][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,104][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,105][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:06:53,105][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:21,473][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 06:07:22,776][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 06:07:26,102][eval][INFO] - Loaded input_proj.
[2025-12-24 06:07:26,170][eval][INFO] - Loaded output_head.
[2025-12-24 06:07:29,341][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 06:07:29,367][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 06:07:29,393][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 06:07:29,393][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 06:07:29,393][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 06:07:29,394][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 06:07:29,414][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 06:07:29,415][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 06:07:29,415][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 06:07:29,416][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 06:07:29,425][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 06:07:29,425][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 06:07:29,792][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 06:07:29,793][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 06:07:29,793][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 06:07:29,793][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 06:07:29,794][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 06:07:29,841][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 06:07:31,658][eval][INFO] - ðŸ›¡ï¸ æ­£åœ¨è¿è¡Œã€åœ¨çº¿ä¸€è‡´æ€§æµ‹è¯•ã€‘(Sanity Check)...
[2025-12-24 06:07:44,224][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,224][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,233][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,233][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,233][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,233][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,233][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,234][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,234][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,234][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,238][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,238][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,238][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,238][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,238][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,238][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,238][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,239][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,239][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,239][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,239][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 06:07:44,239][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:14:20,754][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 16:14:22,081][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 16:14:25,321][eval][INFO] - Loaded input_proj.
[2025-12-24 16:14:25,401][eval][INFO] - Loaded output_head.
[2025-12-24 16:14:29,524][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 16:14:29,550][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 16:14:29,576][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 16:14:29,576][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 16:14:29,576][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 16:14:29,576][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 16:14:29,600][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 16:14:29,600][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 16:14:29,601][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 16:14:29,601][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 16:14:29,610][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 16:14:29,611][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 16:14:29,973][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 16:14:29,974][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 16:14:29,974][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 16:14:29,974][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 16:14:29,974][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 16:14:30,028][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 16:14:31,842][eval][INFO] - ðŸ§ª Running Standard Oracle Reconstruction Test...
[2025-12-24 16:15:11,365][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,366][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,368][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,368][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,369][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,370][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,373][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,373][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,373][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,374][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,374][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,375][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 16:15:11,377][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:06:12,698][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 22:06:14,027][eval][INFO] - Detected Mixture of Adapters (MoA).
[2025-12-24 22:06:17,422][eval][INFO] - Loaded input_proj.
[2025-12-24 22:06:17,497][eval][INFO] - Loaded output_head.
[2025-12-24 22:06:21,496][eval][INFO] - Model set to BF16 (VAE kept in FP32).
[2025-12-24 22:06:21,522][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 22:06:21,548][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 22:06:21,548][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 22:06:21,549][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 22:06:21,549][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 22:06:21,569][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 22:06:21,570][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 22:06:21,571][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 22:06:21,571][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 22:06:21,580][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 22:06:21,581][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 22:06:21,995][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 22:06:21,997][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 22:06:21,997][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:06:21,998][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 22:06:21,998][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:06:22,061][eval][INFO] - Loading ASR Metric Model: openai/whisper-tiny.en
[2025-12-24 22:06:23,885][eval][INFO] - ðŸ§ª Running Standard Oracle Reconstruction Test...
[2025-12-24 22:07:19,635][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,635][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,642][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,642][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,642][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,642][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,643][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,643][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,643][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,646][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,647][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,647][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,647][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,647][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,647][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,647][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,648][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,648][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,648][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:07:19,651][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:11:52,354][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 22:12:01,136][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 22:12:01,162][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 22:12:01,162][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 22:12:01,163][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 22:12:01,163][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 22:12:01,184][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 22:12:01,184][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 22:12:01,185][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 22:12:01,186][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 22:12:01,195][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 22:12:01,195][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 22:12:01,559][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 22:12:01,560][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 22:12:01,561][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:12:01,561][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 22:12:01,562][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:12:03,503][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 22:12:03,612][eval][ERROR] - Oracle test failed: Expected size for first two dimensions of batch2 tensor to be: [1, 80] but got: [1, 513].
[2025-12-24 22:12:26,629][eval][ERROR] - Error sample 0: Expected size for first two dimensions of batch2 tensor to be: [1, 80] but got: [1, 513].
[2025-12-24 22:12:34,125][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,125][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,127][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,128][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,128][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,129][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,129][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,133][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,133][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,134][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,134][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,134][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,134][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,134][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,135][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,137][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,138][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,138][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,138][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,138][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,138][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,138][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,138][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,138][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,139][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,139][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,139][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:12:34,139][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:14:56,688][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 22:15:05,413][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 22:15:05,438][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 22:15:05,438][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 22:15:05,439][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 22:15:05,439][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 22:15:05,459][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 22:15:05,460][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 22:15:05,461][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 22:15:05,461][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 22:15:05,470][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 22:15:05,470][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 22:15:05,839][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 22:15:05,840][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 22:15:05,840][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:15:05,840][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 22:15:05,840][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:15:07,793][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 22:15:40,918][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,918][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,925][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,925][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,925][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,925][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,926][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,929][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,929][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,929][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,929][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,929][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,929][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,930][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,930][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:15:40,930][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:21:52,538][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 22:22:01,430][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 22:22:01,465][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 22:22:01,465][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 22:22:01,465][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 22:22:01,466][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 22:22:01,501][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-24 22:22:01,501][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 22:22:01,502][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 22:22:01,502][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 22:22:01,511][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 22:22:01,512][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 22:22:02,006][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 22:22:02,007][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 22:22:02,007][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:22:02,008][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 22:22:02,008][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:22:04,190][eval][INFO] - ðŸ§ª Running Oracle Reconstruction Test...
[2025-12-24 22:23:47,727][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,728][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,731][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,731][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,732][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,735][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,736][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,736][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,736][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,736][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,736][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,736][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,736][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,739][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,740][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,740][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,740][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,741][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,742][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:23:47,742][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:24:38,066][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 22:24:46,942][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 22:24:47,033][eval][INFO] - Vocoder Inverse Basis Shape: torch.Size([80, 513])
[2025-12-24 22:24:48,873][eval][INFO] - ðŸ§ª Running Oracle Test (Diagnostic Mode)...
[2025-12-24 22:24:48,874][eval][INFO] - ðŸ“Š GT Latent Stats: Shape=torch.Size([1, 64, 64])
[2025-12-24 22:24:48,878][eval][INFO] -    Mean=0.0436 | Std=0.8274
[2025-12-24 22:24:48,885][eval][INFO] -    Min =-2.9659  | Max=3.1478
[2025-12-24 22:24:48,987][eval][INFO] - ðŸ“Š VAE Mel Output Stats:
[2025-12-24 22:24:48,987][eval][INFO] -    Mean=-6.4137 | Std=2.7501
[2025-12-24 22:24:48,988][eval][INFO] -    Min =-14.3937  | Max=4.3885
[2025-12-24 22:29:24,910][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 22:29:33,465][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 22:29:33,490][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 22:29:33,491][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 22:29:33,491][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 22:29:33,491][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 22:29:33,511][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 22:29:33,512][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 22:29:33,512][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 22:29:33,513][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 22:29:33,521][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 22:29:33,522][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 22:29:33,893][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 22:29:33,894][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 22:29:33,894][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:29:33,894][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 22:29:33,894][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:29:33,941][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded successfully.
[2025-12-24 22:30:43,093][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 22:30:51,759][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 22:30:51,784][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 22:30:51,785][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 22:30:51,785][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 22:30:51,785][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 22:30:51,806][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 22:30:51,806][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 22:30:51,807][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 22:30:51,808][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 22:30:51,816][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 22:30:51,817][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 22:30:52,190][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 22:30:52,190][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 22:30:52,191][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:30:52,191][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 22:30:52,191][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:30:52,239][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded successfully.
[2025-12-24 22:31:54,622][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 22:32:03,241][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 22:32:03,265][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 22:32:03,266][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 22:32:03,266][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 22:32:03,266][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 22:32:03,286][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 22:32:03,287][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 22:32:03,288][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 22:32:03,288][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 22:32:03,297][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 22:32:03,297][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 22:32:03,663][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 22:32:03,664][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 22:32:03,664][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:32:03,665][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 22:32:03,665][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:32:03,713][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded successfully.
[2025-12-24 22:32:05,547][eval][INFO] - ðŸ§ª Running Oracle Test (Diagnostic Mode)...
[2025-12-24 22:32:05,548][eval][INFO] - ðŸ“Š GT Latent Stats: Shape=torch.Size([1, 64, 64])
[2025-12-24 22:32:05,553][eval][INFO] -    Mean=0.0436 | Std=0.8274
[2025-12-24 22:32:05,560][eval][INFO] -    Min =-2.9659  | Max=3.1478
[2025-12-24 22:32:05,676][eval][INFO] - ðŸ“Š VAE Mel Output Stats:
[2025-12-24 22:32:05,677][eval][INFO] -    Mean=-6.4137 | Std=2.7501
[2025-12-24 22:32:05,677][eval][INFO] -    Min =-14.3937  | Max=4.3885
[2025-12-24 22:32:05,689][eval][WARNING] - HiFi-GAN inference failed: Given groups=1, weight of size [512, 80, 7], expected input[1, 256, 96] to have 80 channels, but got 256 channels instead. Switching to GL.
[2025-12-24 22:32:21,150][eval][INFO] - Gen Latent Stats: Mean=-0.00, Std=0.15, Min=-3.80, Max=3.12
[2025-12-24 22:32:21,160][eval][WARNING] - HiFi-GAN inference failed: Given groups=1, weight of size [512, 80, 7], expected input[1, 804, 96] to have 80 channels, but got 804 channels instead. Switching to GL.
[2025-12-24 22:32:36,451][eval][INFO] - Gen Latent Stats: Mean=0.00, Std=0.21, Min=-6.50, Max=6.97
[2025-12-24 22:32:36,453][eval][WARNING] - HiFi-GAN inference failed: Given groups=1, weight of size [512, 80, 7], expected input[1, 804, 96] to have 80 channels, but got 804 channels instead. Switching to GL.
[2025-12-24 22:32:53,658][eval][INFO] - Gen Latent Stats: Mean=-0.01, Std=0.18, Min=-5.16, Max=5.56
[2025-12-24 22:32:53,660][eval][WARNING] - HiFi-GAN inference failed: Given groups=1, weight of size [512, 80, 7], expected input[1, 804, 96] to have 80 channels, but got 804 channels instead. Switching to GL.
[2025-12-24 22:33:10,915][eval][INFO] - Gen Latent Stats: Mean=0.00, Std=0.41, Min=-13.88, Max=11.38
[2025-12-24 22:33:10,917][eval][WARNING] - HiFi-GAN inference failed: Given groups=1, weight of size [512, 80, 7], expected input[1, 804, 96] to have 80 channels, but got 804 channels instead. Switching to GL.
[2025-12-24 22:33:28,369][eval][INFO] - Gen Latent Stats: Mean=0.00, Std=0.23, Min=-5.53, Max=7.44
[2025-12-24 22:33:28,371][eval][WARNING] - HiFi-GAN inference failed: Given groups=1, weight of size [512, 80, 7], expected input[1, 804, 96] to have 80 channels, but got 804 channels instead. Switching to GL.
[2025-12-24 22:33:43,081][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,082][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,084][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,084][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,085][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,085][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,086][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,090][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,090][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,091][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,091][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,091][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,091][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,094][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,096][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,096][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,096][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,096][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,096][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,096][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,096][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,096][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,096][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,096][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,096][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,096][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,096][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,097][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:33:43,097][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:11,778][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 22:35:20,364][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 22:35:20,388][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 22:35:20,389][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 22:35:20,389][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 22:35:20,389][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 22:35:20,409][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 22:35:20,410][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 22:35:20,411][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 22:35:20,411][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 22:35:20,420][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 22:35:20,420][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 22:35:20,786][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 22:35:20,787][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 22:35:20,787][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:35:20,788][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 22:35:20,788][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:35:20,844][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded successfully.
[2025-12-24 22:35:22,670][eval][INFO] - ðŸ§ª Running Oracle Test (Diagnostic Mode)...
[2025-12-24 22:35:22,672][eval][INFO] - ðŸ“Š GT Latent Stats: Shape=torch.Size([1, 64, 64])
[2025-12-24 22:35:22,676][eval][INFO] -    Mean=0.0436 | Std=0.8274
[2025-12-24 22:35:22,684][eval][INFO] -    Min =-2.9659  | Max=3.1478
[2025-12-24 22:35:22,800][eval][INFO] - ðŸ“Š VAE Mel Output Stats:
[2025-12-24 22:35:22,801][eval][INFO] -    Mean=-6.4137 | Std=2.7501
[2025-12-24 22:35:22,801][eval][INFO] -    Min =-14.3937  | Max=4.3885
[2025-12-24 22:35:38,191][eval][INFO] - Gen Latent Stats: Mean=-0.00, Std=0.15, Min=-3.61, Max=3.39
[2025-12-24 22:35:53,445][eval][INFO] - Gen Latent Stats: Mean=0.00, Std=0.20, Min=-5.66, Max=7.03
[2025-12-24 22:35:58,821][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,822][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,824][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,824][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,825][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,826][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,830][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,830][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,830][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,830][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,830][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,830][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,832][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,834][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,834][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,834][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,834][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,834][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,834][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,835][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,835][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:35:58,835][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:36:37,269][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 22:36:46,045][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 22:36:46,070][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 22:36:46,070][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 22:36:46,071][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 22:36:46,071][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 22:36:46,091][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 22:36:46,092][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 22:36:46,092][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 22:36:46,093][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 22:36:46,101][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 22:36:46,102][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 22:36:46,721][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 22:36:46,722][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 22:36:46,723][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:36:46,723][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 22:36:46,723][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 22:36:46,771][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded successfully.
[2025-12-24 22:36:48,584][eval][INFO] - ðŸ§ª Running Oracle Test (Diagnostic Mode)...
[2025-12-24 22:36:48,585][eval][INFO] - ðŸ“Š GT Latent Stats: Shape=torch.Size([1, 64, 64])
[2025-12-24 22:36:48,590][eval][INFO] -    Mean=0.0436 | Std=0.8274
[2025-12-24 22:36:48,597][eval][INFO] -    Min =-2.9659  | Max=3.1478
[2025-12-24 22:36:48,715][eval][INFO] - ðŸ“Š VAE Mel Output Stats:
[2025-12-24 22:36:48,715][eval][INFO] -    Mean=-6.4137 | Std=2.7501
[2025-12-24 22:36:48,716][eval][INFO] -    Min =-14.3937  | Max=4.3885
[2025-12-24 22:37:07,965][eval][INFO] - Gen Latent Stats: Mean=-0.00, Std=0.15, Min=-3.56, Max=3.39
[2025-12-24 22:37:28,958][eval][INFO] - Gen Latent Stats: Mean=0.00, Std=0.20, Min=-5.62, Max=7.03
[2025-12-24 22:37:47,806][eval][INFO] - Gen Latent Stats: Mean=-0.01, Std=0.18, Min=-4.66, Max=5.72
[2025-12-24 22:37:49,495][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,496][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,502][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,502][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,502][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,502][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,503][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,505][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,507][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,507][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,507][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,507][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,507][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,507][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,507][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,507][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,508][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,508][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,508][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,508][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,508][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,508][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,508][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:37:49,508][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:39:52,301][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 22:40:00,997][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 22:40:01,004][eval][INFO] - ðŸ”§ Initializing Manual Griffin-Lim Vocoder...
[2025-12-24 22:40:03,100][eval][INFO] - ðŸ§ª Running Oracle Test (Diagnostic Mode)...
[2025-12-24 22:40:03,101][eval][INFO] - ðŸ“Š GT Latent Stats: Shape=torch.Size([1, 64, 64])
[2025-12-24 22:40:03,105][eval][INFO] -    Mean=0.0436 | Std=0.8274
[2025-12-24 22:40:03,113][eval][INFO] -    Min =-2.9659  | Max=3.1478
[2025-12-24 22:40:03,213][eval][INFO] - ðŸ“Š VAE Mel Output Stats:
[2025-12-24 22:40:03,214][eval][INFO] -    Mean=-6.4137 | Std=2.7501
[2025-12-24 22:40:03,214][eval][INFO] -    Min =-14.3937  | Max=4.3885
[2025-12-24 22:40:22,282][eval][INFO] - Gen Latent Stats: Mean=-0.00, Std=0.15, Min=-3.78, Max=3.09
[2025-12-24 22:40:35,197][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,197][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,199][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,200][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,204][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,204][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,204][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,204][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,207][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,208][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,208][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,208][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,208][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,209][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:40:35,209][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 22:42:23,406][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 22:42:32,109][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 23:29:24,129][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 23:29:32,264][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 23:29:54,688][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 23:30:03,898][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 23:30:03,943][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 23:30:03,943][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 23:30:03,944][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 23:30:03,944][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 23:30:04,012][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-24 23:30:04,013][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 23:30:04,015][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 23:30:04,015][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 23:30:04,027][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 23:30:04,028][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 23:30:04,423][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 23:30:04,424][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 23:30:04,424][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:30:04,425][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 23:30:04,425][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:30:04,480][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-24 23:30:06,512][eval][INFO] - ðŸ§ª Running Oracle Test (Diagnostic Mode)...
[2025-12-24 23:30:06,514][eval][INFO] - ðŸ“Š GT Latent Stats: Shape=torch.Size([1, 64, 64])
[2025-12-24 23:30:06,518][eval][INFO] -    Mean=0.0436 | Std=0.8274
[2025-12-24 23:30:06,526][eval][INFO] -    Min =-2.9659  | Max=3.1478
[2025-12-24 23:30:06,649][eval][INFO] - ðŸ“Š VAE Mel Output Stats:
[2025-12-24 23:30:06,649][eval][INFO] -    Mean=-6.4137 | Std=2.7501
[2025-12-24 23:30:06,650][eval][INFO] -    Min =-14.3937  | Max=4.3885
[2025-12-24 23:30:26,153][eval][INFO] - Gen Latent Stats: Mean=-0.00, Std=0.15, Min=-3.56, Max=3.39
[2025-12-24 23:30:47,281][eval][INFO] - Gen Latent Stats: Mean=0.00, Std=0.20, Min=-5.62, Max=7.03
[2025-12-24 23:30:52,580][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,581][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,584][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,589][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,589][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,589][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,590][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,590][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,590][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,590][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,591][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,594][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,595][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,595][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,595][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,595][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,595][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,595][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,595][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:30:52,595][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:33:33,248][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 23:33:41,516][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 23:33:41,524][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-24 23:33:41,543][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 23:33:41,544][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 23:33:41,544][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 23:33:41,544][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 23:33:41,566][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 23:33:41,567][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 23:33:41,567][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 23:33:41,568][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 23:33:41,577][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 23:33:41,577][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 23:33:41,945][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 23:33:41,945][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 23:33:41,946][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:33:41,946][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 23:33:41,946][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:33:41,992][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-24 23:33:43,913][eval][INFO] - ðŸ§ª Running Oracle Test (Diagnostic Mode)...
[2025-12-24 23:33:43,914][eval][INFO] - ðŸ“Š GT Latent Input Shape: torch.Size([1, 64, 64])
[2025-12-24 23:34:03,749][eval][INFO] - Gen Latent Stats: Mean=-0.00, Std=0.15
[2025-12-24 23:34:25,131][eval][INFO] - Gen Latent Stats: Mean=0.00, Std=0.20
[2025-12-24 23:34:44,490][eval][INFO] - Gen Latent Stats: Mean=-0.01, Std=0.18
[2025-12-24 23:35:03,832][eval][INFO] - Gen Latent Stats: Mean=0.00, Std=0.42
[2025-12-24 23:35:24,949][eval][INFO] - Gen Latent Stats: Mean=-0.00, Std=0.22
[2025-12-24 23:35:26,012][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,013][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,015][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,015][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,016][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,017][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,021][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,021][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,021][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,022][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,022][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,022][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,022][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,022][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,023][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,023][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,023][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,023][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,023][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,024][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,024][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,024][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,024][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,027][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,028][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,028][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,028][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,028][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,028][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,028][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,028][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,028][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,029][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:35:26,029][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:38:39,178][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 23:38:47,087][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 23:38:47,095][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-24 23:38:47,114][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 23:38:47,114][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 23:38:47,114][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 23:38:47,115][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 23:38:47,136][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 23:38:47,136][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 23:38:47,137][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 23:38:47,137][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 23:38:47,146][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 23:38:47,146][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 23:38:47,521][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 23:38:47,523][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 23:38:47,524][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:38:47,524][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 23:38:47,524][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:38:47,569][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-24 23:38:49,469][eval][INFO] - ðŸ§ª Running Oracle Test (Diagnostic Mode)...
[2025-12-24 23:38:49,470][eval][INFO] - ðŸ“Š GT Latent Input Shape: torch.Size([1, 64, 64])
[2025-12-24 23:39:09,136][eval][INFO] - Gen Latent Stats: Mean=-0.00, Std=0.12
[2025-12-24 23:39:30,411][eval][INFO] - Gen Latent Stats: Mean=0.00, Std=0.11
[2025-12-24 23:39:49,542][eval][INFO] - Gen Latent Stats: Mean=-0.01, Std=0.10
[2025-12-24 23:40:08,634][eval][INFO] - Gen Latent Stats: Mean=-0.00, Std=0.16
[2025-12-24 23:40:27,688][eval][INFO] - Gen Latent Stats: Mean=-0.00, Std=0.13
[2025-12-24 23:40:46,771][eval][INFO] - Gen Latent Stats: Mean=0.00, Std=0.13
[2025-12-24 23:40:49,193][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,193][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,200][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,200][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,200][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,201][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:40:49,205][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:43:53,815][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 23:44:01,421][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 23:44:01,427][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-24 23:44:01,446][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 23:44:01,447][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 23:44:01,447][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 23:44:01,447][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 23:44:01,468][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 23:44:01,468][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 23:44:01,469][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 23:44:01,469][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 23:44:01,478][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 23:44:01,478][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 23:44:01,847][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 23:44:01,848][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 23:44:01,848][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:44:01,848][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 23:44:01,849][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:44:01,892][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-24 23:44:03,823][eval][INFO] - ðŸ§ª Running Oracle Test (Diagnostic Mode)...
[2025-12-24 23:44:03,825][eval][INFO] - ðŸ“Š GT Latent Input Shape: torch.Size([1, 64, 64])
[2025-12-24 23:44:04,016][eval][ERROR] - Error sample 0: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,017][eval][ERROR] - Error sample 1: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,017][eval][ERROR] - Error sample 2: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,018][eval][ERROR] - Error sample 3: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,018][eval][ERROR] - Error sample 4: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,018][eval][ERROR] - Error sample 5: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,018][eval][ERROR] - Error sample 6: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,018][eval][ERROR] - Error sample 7: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,019][eval][ERROR] - Error sample 8: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,019][eval][ERROR] - Error sample 9: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,019][eval][ERROR] - Error sample 10: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,019][eval][ERROR] - Error sample 11: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,019][eval][ERROR] - Error sample 12: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,019][eval][ERROR] - Error sample 13: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,019][eval][ERROR] - Error sample 14: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,020][eval][ERROR] - Error sample 15: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,020][eval][ERROR] - Error sample 16: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,020][eval][ERROR] - Error sample 17: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,020][eval][ERROR] - Error sample 18: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,020][eval][ERROR] - Error sample 19: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,020][eval][ERROR] - Error sample 20: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,021][eval][ERROR] - Error sample 21: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,021][eval][ERROR] - Error sample 22: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,023][eval][ERROR] - Error sample 23: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,024][eval][ERROR] - Error sample 24: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,024][eval][ERROR] - Error sample 25: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,024][eval][ERROR] - Error sample 26: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,025][eval][ERROR] - Error sample 27: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,025][eval][ERROR] - Error sample 28: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,025][eval][ERROR] - Error sample 29: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,025][eval][ERROR] - Error sample 30: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,025][eval][ERROR] - Error sample 31: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,025][eval][ERROR] - Error sample 32: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,026][eval][ERROR] - Error sample 33: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,026][eval][ERROR] - Error sample 34: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,026][eval][ERROR] - Error sample 35: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,026][eval][ERROR] - Error sample 36: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,026][eval][ERROR] - Error sample 37: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,026][eval][ERROR] - Error sample 38: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,027][eval][ERROR] - Error sample 39: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,027][eval][ERROR] - Error sample 40: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,027][eval][ERROR] - Error sample 41: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,027][eval][ERROR] - Error sample 42: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,027][eval][ERROR] - Error sample 43: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,028][eval][ERROR] - Error sample 44: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,028][eval][ERROR] - Error sample 45: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,028][eval][ERROR] - Error sample 46: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,028][eval][ERROR] - Error sample 47: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,028][eval][ERROR] - Error sample 48: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:04,028][eval][ERROR] - Error sample 49: run_tts_inference() got an unexpected keyword argument 'gt_latent_path'
[2025-12-24 23:44:55,195][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 23:45:04,609][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 23:45:04,618][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-24 23:45:04,659][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 23:45:04,659][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 23:45:04,660][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 23:45:04,661][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 23:45:04,734][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-24 23:45:04,735][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 23:45:04,735][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 23:45:04,736][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 23:45:04,747][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 23:45:04,747][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 23:45:05,124][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 23:45:05,125][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 23:45:05,125][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:45:05,125][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 23:45:05,125][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:45:05,210][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-24 23:45:07,236][eval][INFO] - ðŸ§ª Running Oracle Test (Diagnostic Mode)...
[2025-12-24 23:45:07,238][eval][INFO] - ðŸ“Š GT Latent Input Shape: torch.Size([1, 64, 64])
[2025-12-24 23:45:13,981][eval][INFO] - Gen Latent Stats: Mean=0.01, Std=0.20
[2025-12-24 23:45:35,386][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,387][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,397][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,397][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,397][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,397][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,397][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,397][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,397][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,398][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,398][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,400][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,402][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,402][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,402][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,402][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,402][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,402][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,402][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,402][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,402][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,402][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,402][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,403][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,403][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:45:35,406][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:48:05,580][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 23:48:13,899][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 23:48:13,907][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-24 23:48:13,926][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 23:48:13,926][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 23:48:13,926][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 23:48:13,927][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 23:48:13,947][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-24 23:48:13,948][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 23:48:13,949][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 23:48:13,949][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 23:48:13,958][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 23:48:13,958][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 23:48:14,315][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 23:48:14,316][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 23:48:14,316][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:48:14,316][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 23:48:14,316][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:48:14,362][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-24 23:48:16,272][eval][INFO] - ðŸ§ª Running Oracle Test (Diagnostic Mode)...
[2025-12-24 23:48:16,274][eval][INFO] - ðŸ“Š GT Latent Input Shape: torch.Size([1, 64, 64])
[2025-12-24 23:48:22,967][eval][INFO] - Gen Latent Stats: Mean=0.00, Std=0.14
[2025-12-24 23:48:44,955][eval][INFO] - Gen Latent Stats: Mean=0.00, Std=0.09
[2025-12-24 23:48:49,726][eval][INFO] - Gen Latent Stats: Mean=0.00, Std=0.17
[2025-12-24 23:49:00,034][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,035][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,037][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,037][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,038][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,039][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,043][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,043][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,044][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,044][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,044][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,047][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,048][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,048][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,048][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,048][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,049][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,049][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,049][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:49:00,049][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:55:59,826][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 23:56:07,730][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 23:56:07,737][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-24 23:56:07,757][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 23:56:07,757][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 23:56:07,758][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 23:56:07,758][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 23:56:07,783][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-24 23:56:07,783][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 23:56:07,784][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 23:56:07,784][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 23:56:07,793][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 23:56:07,794][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 23:56:08,176][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 23:56:08,177][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 23:56:08,177][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:56:08,177][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 23:56:08,178][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:56:08,223][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-24 23:56:10,178][eval][INFO] - ðŸ§ª Running Oracle Test (Diagnostic Mode)...
[2025-12-24 23:56:10,179][eval][INFO] - ðŸ“Š GT Latent Input Shape: torch.Size([1, 64, 64])
[2025-12-24 23:56:10,379][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:10,383][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:10,391][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: "<|im_start|>user\nRead this text:\nEven if we were married I could never be sure you weren't kissing some horrid girl or other.<|im_end|>\n<|im_start|>assistant\n"
[2025-12-24 23:56:10,419][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 13159, 421, 582, 1033, 12224, 358, 1410, 2581, 387, 2704, 498, 14716, 944, 51046, 1045, 4812, 1869, 3743, 476, 1008, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:10,479][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0436, Std=0.8274, Min=-2.9659, Max=3.1478
[2025-12-24 23:56:10,795][eval][ERROR] - Error sample 0: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:10,803][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:10,804][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:10,811][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nBesides the dyes, we shall also have left naphtha, useful in making varnish, and various oils that are used in more ways than I can stop to tell you, or you would care now to hear.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:10,813][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 51455, 279, 294, 9693, 11, 582, 4880, 1083, 614, 2115, 308, 1342, 22410, 11, 5390, 304, 3259, 762, 96839, 11, 323, 5257, 31362, 429, 525, 1483, 304, 803, 5510, 1091, 358, 646, 2936, 311, 3291, 498, 11, 476, 498, 1035, 2453, 1431, 311, 6723, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:10,814][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0371, Std=0.7089, Min=-3.0038, Max=3.1045
[2025-12-24 23:56:10,915][eval][ERROR] - Error sample 1: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:10,922][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:10,923][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:10,929][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\n"Then you are quite willing to try the third great experiment?"<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:10,931][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 1, 12209, 498, 525, 5008, 9831, 311, 1430, 279, 4843, 2244, 9342, 7521, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:10,932][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0351, Std=0.8676, Min=-3.2592, Max=2.8191
[2025-12-24 23:56:11,031][eval][ERROR] - Error sample 2: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:11,038][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:11,039][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:11,045][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: "<|im_start|>user\nRead this text:\nAT THAT WORD DECEPTION SPOKEN WITH SUCH SELF CONTEMPT THE COLOR FLASHED BACK INTO HILDA'S FACE AS SUDDENLY AS IF SHE HAD BEEN STRUCK BY A WHIPLASH<|im_end|>\n<|im_start|>assistant\n"
[2025-12-24 23:56:11,047][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 828, 25269, 36891, 3385, 21377, 328, 2045, 61829, 4769, 26094, 66225, 3418, 21019, 2828, 3168, 25419, 52326, 1479, 32082, 12496, 472, 1715, 6352, 13272, 58227, 5752, 15490, 4103, 953, 8932, 5752, 11551, 53595, 472, 1808, 74653, 12152, 27543, 7710, 362, 8494, 40, 2916, 9537, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:11,048][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0466, Std=0.6772, Min=-3.1046, Max=2.8736
[2025-12-24 23:56:11,147][eval][ERROR] - Error sample 3: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:11,154][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:11,155][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:11,161][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\n"Tidvatten," said the guide.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:11,162][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 95029, 307, 85, 14456, 1335, 1053, 279, 8474, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:11,163][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0173, Std=0.7220, Min=-3.0686, Max=4.4782
[2025-12-24 23:56:11,262][eval][ERROR] - Error sample 4: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:11,269][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:11,269][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:11,276][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: "<|im_start|>user\nRead this text:\nNature's struggles were soon exhausted, and he breathed his little life away in peace.<|im_end|>\n<|im_start|>assistant\n"
[2025-12-24 23:56:11,277][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 78419, 594, 27870, 1033, 5135, 37919, 11, 323, 566, 91166, 806, 2632, 2272, 3123, 304, 8919, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:11,278][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0235, Std=0.7411, Min=-3.0375, Max=2.7731
[2025-12-24 23:56:11,377][eval][ERROR] - Error sample 5: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:11,384][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:11,385][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:11,391][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\n"Look, Tony, that\'s his poison," I said.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:11,392][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 1, 10380, 11, 18528, 11, 429, 594, 806, 20476, 1335, 358, 1053, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:11,393][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0194, Std=0.7786, Min=-3.1088, Max=3.4382
[2025-12-24 23:56:11,492][eval][ERROR] - Error sample 6: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:11,499][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:11,499][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:11,506][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: "<|im_start|>user\nRead this text:\nThat's nothing.<|im_end|>\n<|im_start|>assistant\n"
[2025-12-24 23:56:11,507][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 4792, 594, 4302, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:11,508][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0062, Std=0.9249, Min=-3.2509, Max=3.0651
[2025-12-24 23:56:11,606][eval][ERROR] - Error sample 7: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:11,613][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:11,613][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:11,620][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\n"All right, old man.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:11,621][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 67049, 1290, 11, 2310, 883, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:11,621][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0602, Std=0.8791, Min=-3.5097, Max=2.9372
[2025-12-24 23:56:11,724][eval][ERROR] - Error sample 8: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:11,731][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:11,732][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:11,739][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nAfter we sat down to our waffles and sausage, Jake told us how pleased the Shimerdas had been with their presents; even Ambrosch was friendly and went to the creek with him to cut the Christmas tree.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:11,740][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 6025, 582, 7578, 1495, 311, 1039, 289, 80535, 323, 58886, 11, 32072, 3229, 601, 1246, 18442, 279, 1417, 3134, 34889, 1030, 1012, 448, 862, 18404, 26, 1496, 19833, 3630, 331, 572, 11657, 323, 3937, 311, 279, 64405, 448, 1435, 311, 3931, 279, 10074, 4916, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:11,741][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0398, Std=0.7257, Min=-2.9559, Max=4.4219
[2025-12-24 23:56:11,840][eval][ERROR] - Error sample 9: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:11,847][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:11,848][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:11,854][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nI did not go and purchase the engagement ring the first thing on the Monday morning, I own it.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:11,855][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 40, 1521, 537, 728, 323, 7627, 279, 19805, 10058, 279, 1156, 3166, 389, 279, 7014, 6556, 11, 358, 1828, 432, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:11,856][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0361, Std=0.7559, Min=-2.7940, Max=3.6133
[2025-12-24 23:56:11,954][eval][ERROR] - Error sample 10: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:11,961][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:11,962][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:11,968][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nLady Helena repeated the evening prayer aloud, her companions, bare headed, repeated it after her.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:11,969][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 75602, 71946, 11504, 279, 11458, 22936, 70411, 11, 1059, 40857, 11, 12461, 19383, 11, 11504, 432, 1283, 1059, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:11,970][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0274, Std=0.7375, Min=-3.0764, Max=2.9453
[2025-12-24 23:56:12,072][eval][ERROR] - Error sample 11: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:12,079][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:12,080][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:12,086][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nThey retreat in wild confusion.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:12,087][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 6865, 30014, 304, 8380, 21340, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:12,088][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0095, Std=0.7972, Min=-3.4812, Max=3.7842
[2025-12-24 23:56:12,195][eval][ERROR] - Error sample 12: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:12,202][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:12,203][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:12,211][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nIt was plain that his castanet girl-his mother and sister took a pleasure in crediting her daily with some fresh and unpleasing instrument-could have had neither taste, money, nor honesty to such a point as this.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:12,213][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 2132, 572, 14396, 429, 806, 6311, 276, 295, 3743, 2832, 285, 6554, 323, 12923, 3867, 264, 16656, 304, 4187, 5853, 1059, 7298, 448, 1045, 7722, 323, 650, 694, 4422, 14141, 1786, 616, 614, 1030, 13866, 12656, 11, 3220, 11, 6329, 47848, 311, 1741, 264, 1459, 438, 419, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:12,214][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0454, Std=0.7712, Min=-3.5378, Max=3.3694
[2025-12-24 23:56:12,315][eval][ERROR] - Error sample 13: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:12,323][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:12,324][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:12,331][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nMany little wrinkles gathered between his eyes as he contemplated this, and his brow moistened.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:12,332][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 8441, 2632, 73107, 20190, 1948, 806, 6414, 438, 566, 92339, 419, 11, 323, 806, 59275, 20507, 6758, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:12,333][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0409, Std=0.7781, Min=-3.9765, Max=3.6063
[2025-12-24 23:56:12,433][eval][ERROR] - Error sample 14: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:12,440][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:12,440][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:12,447][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nOVER THE CHILD WHICH YET BREATHED THE FATHER BENT WATCHING ANXIOUSLY FOR SOME GROUND OF HOPE WHERE HOPE THERE WAS NONE<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:12,448][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 49105, 3168, 72520, 78164, 809, 1348, 80562, 4827, 1479, 3168, 434, 46139, 425, 1825, 47407, 1718, 2100, 55, 42652, 8932, 4613, 65555, 479, 22919, 3008, 30250, 1740, 5288, 30250, 1740, 61107, 37776, 42869, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:12,449][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0117, Std=0.6278, Min=-2.4781, Max=2.5752
[2025-12-24 23:56:12,552][eval][ERROR] - Error sample 15: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:12,559][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:12,559][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:12,567][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nHer residence at the sea side, helped by the lapse of time, had restored to her personal attractions almost all they had lost under the deteriorating influences of care and grief; and her change of name must have protected her from a discovery of the Divorce which would have shocked a man so sincerely religious as Bennydeck.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:12,568][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 20705, 21682, 518, 279, 9396, 3108, 11, 8910, 553, 279, 89317, 315, 882, 11, 1030, 27003, 311, 1059, 4345, 38491, 4558, 678, 807, 1030, 5558, 1212, 279, 38336, 1095, 33353, 315, 2453, 323, 37284, 26, 323, 1059, 2297, 315, 829, 1969, 614, 2617, 1059, 504, 264, 18335, 315, 279, 8765, 16316, 892, 1035, 614, 26620, 264, 883, 773, 58283, 10381, 438, 94185, 33425, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:12,570][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0136, Std=0.7194, Min=-3.1965, Max=3.5618
[2025-12-24 23:56:12,670][eval][ERROR] - Error sample 16: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:12,677][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:12,677][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:12,687][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\n"They still come?" he asked peter.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:12,688][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 31308, 2058, 2525, 7521, 566, 4588, 93987, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:12,689][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0320, Std=0.8237, Min=-3.3044, Max=2.9131
[2025-12-24 23:56:12,788][eval][ERROR] - Error sample 17: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:12,795][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:12,796][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:12,802][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nMeanwhile, he would have time to think.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:12,803][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 25004, 11, 566, 1035, 614, 882, 311, 1744, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:12,804][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0261, Std=0.8345, Min=-3.2324, Max=3.1396
[2025-12-24 23:56:12,907][eval][ERROR] - Error sample 18: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:12,914][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:12,914][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:12,921][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nHow can he, whose sphere lies above the stars, stoop every moment to earth?<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:12,922][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 4340, 646, 566, 11, 6693, 25366, 15448, 3403, 279, 9759, 11, 42032, 453, 1449, 4445, 311, 9393, 30, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:12,923][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0488, Std=0.7736, Min=-3.5134, Max=4.2035
[2025-12-24 23:56:13,021][eval][ERROR] - Error sample 19: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:13,028][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:13,029][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:13,035][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nANIMAL OR MAN ANSWERED THE MAJOR I WILL SOON FIND OUT<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:13,036][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 1093, 37577, 2726, 25735, 96704, 640, 1479, 3168, 9718, 77080, 358, 27915, 5627, 711, 63758, 9808, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:13,037][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0657, Std=0.6456, Min=-2.8929, Max=2.5135
[2025-12-24 23:56:13,139][eval][ERROR] - Error sample 20: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:13,146][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:13,147][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:13,153][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nBut death here, means not death only, it means torture, insult, perhaps, and here are two ladies-"<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:13,154][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 3983, 4545, 1588, 11, 3363, 537, 4545, 1172, 11, 432, 3363, 29567, 11, 26132, 11, 8365, 11, 323, 1588, 525, 1378, 22786, 27651, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:13,155][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0009, Std=0.6884, Min=-2.8700, Max=3.2873
[2025-12-24 23:56:13,257][eval][ERROR] - Error sample 21: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:13,264][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:13,265][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:13,271][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\n"He is old as well as poor," she said.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:13,272][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 26326, 374, 2310, 438, 1632, 438, 7852, 1335, 1340, 1053, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:13,273][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0242, Std=0.8471, Min=-3.0951, Max=3.6086
[2025-12-24 23:56:13,372][eval][ERROR] - Error sample 22: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:13,379][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:13,380][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:13,386][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nHEAVEN HELP THAT BODY WHICH A LITTLE MIND HOUSED IN A HEAD LACKING EARS TONGUE AND EYES AND SENSELESS BUT FOR SMELL CAN TYRANNISE<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:13,387][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 1799, 8093, 953, 55471, 25269, 68583, 78164, 362, 444, 60109, 386, 5245, 30250, 15796, 1964, 362, 33080, 444, 4032, 1718, 468, 17048, 350, 7539, 2230, 3567, 468, 14004, 3567, 328, 6367, 37773, 10915, 4613, 13716, 19114, 19508, 48862, 49, 12003, 9133, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:13,388][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0544, Std=0.7007, Min=-2.5145, Max=2.9120
[2025-12-24 23:56:13,490][eval][ERROR] - Error sample 23: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:13,497][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:13,498][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:13,504][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nYOU ARE NOT LIKE MY PEOPLE THE PINKIES AND THERE IS NO PLACE FOR YOU IN OUR COUNTRY<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:13,505][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 56389, 15824, 4183, 20529, 18224, 66725, 3168, 393, 11637, 5369, 3567, 61107, 3424, 5664, 81882, 4613, 14985, 1964, 44913, 356, 48768, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:13,506][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0057, Std=0.7672, Min=-2.9237, Max=2.7109
[2025-12-24 23:56:13,607][eval][ERROR] - Error sample 24: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:13,614][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:13,615][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:13,622][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nBUT EARNEST AS THE FATHER WAS IN WATCHING THE YET LIVING HE HAD EYES AND EARS FOR ALL THAT CONCERNED THE DEAD AND SPRANG GENTLY UP AND TOOK HIS DEAD SON ON HIS HARD COUCH IN HIS ARMS WITH TENDER STRENGTH AND CARRIED HIM UPSTAIRS AS IF AFRAID OF WAKENING HIM<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:13,623][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 33, 1381, 90296, 3944, 784, 5752, 3168, 434, 46139, 37776, 1964, 47407, 1718, 3168, 809, 1348, 444, 72146, 11685, 472, 1808, 468, 14004, 3567, 468, 17048, 4613, 13097, 25269, 3418, 34, 13660, 1479, 3168, 78069, 3567, 51267, 5218, 479, 1825, 8932, 11982, 3567, 5146, 3925, 65126, 78069, 76412, 6197, 65126, 69902, 7284, 29493, 1964, 65126, 6261, 4826, 4769, 350, 43604, 3928, 787, 8977, 3567, 356, 3915, 9326, 88894, 11982, 784, 57161, 50, 5752, 11551, 19885, 5609, 915, 3008, 467, 11907, 953, 1718, 88894, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:13,624][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0088, Std=0.6503, Min=-3.3476, Max=3.4135
[2025-12-24 23:56:13,723][eval][ERROR] - Error sample 25: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:13,730][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:13,731][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:13,737][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nIT IS NOT CULTIVATED IN ENGLAND BEING PRINCIPALLY CONFINED TO THE EAST<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:13,738][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 952, 3424, 4183, 356, 3532, 3090, 9005, 1964, 5190, 3825, 3976, 7206, 1718, 8575, 38439, 3298, 28455, 3418, 16750, 1479, 5146, 3168, 77625, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:13,739][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0153, Std=0.6984, Min=-3.0993, Max=2.7271
[2025-12-24 23:56:13,843][eval][ERROR] - Error sample 26: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:13,850][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:13,851][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:13,858][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nDAMN YOUR IMPERTINENCE SIR BURST OUT BURGESS<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:13,859][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 35, 1402, 45, 20922, 39440, 3399, 687, 10150, 328, 2801, 425, 1511, 784, 9808, 425, 1511, 38, 9996, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:13,860][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0236, Std=0.7653, Min=-2.8952, Max=3.0446
[2025-12-24 23:56:13,958][eval][ERROR] - Error sample 27: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:13,965][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:13,966][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:13,972][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nBUT SHE KNEW NOBODY AND WANDERED ALONE IN THE GARDEN OPPRESSED WITH SOMETHING SHE DID NOT UNDERSTAND<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:13,973][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 33, 1381, 53595, 730, 20594, 5664, 41817, 3567, 467, 3976, 640, 1479, 8753, 5225, 1964, 3168, 479, 7376, 953, 94181, 49061, 4769, 73290, 7625, 1718, 53595, 59119, 4183, 56218, 784, 3976, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:13,974][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0228, Std=0.6929, Min=-2.4876, Max=2.8120
[2025-12-24 23:56:14,073][eval][ERROR] - Error sample 28: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:14,082][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:14,083][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:14,090][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nShe looked at me, her eyes fairly blazing with things she could not say.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:14,092][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 7941, 6966, 518, 752, 11, 1059, 6414, 14138, 85250, 448, 2513, 1340, 1410, 537, 1977, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:14,093][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0116, Std=0.7347, Min=-3.0567, Max=3.0304
[2025-12-24 23:56:14,193][eval][ERROR] - Error sample 29: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:14,200][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:14,200][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:14,207][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nSo we both try to do a little better, and a lot of things we used to argue and fight about, like my jazz records, we just kid each other about now.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:14,208][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 4416, 582, 2176, 1430, 311, 653, 264, 2632, 2664, 11, 323, 264, 2696, 315, 2513, 582, 1483, 311, 17585, 323, 4367, 911, 11, 1075, 847, 33897, 7424, 11, 582, 1101, 10369, 1817, 1008, 911, 1431, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:14,209][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0256, Std=0.7440, Min=-3.1132, Max=3.3194
[2025-12-24 23:56:14,310][eval][ERROR] - Error sample 30: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:14,317][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:14,318][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:14,324][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: "<|im_start|>user\nRead this text:\nBut I must take some notice of the letter.'<|im_end|>\n<|im_start|>assistant\n"
[2025-12-24 23:56:14,325][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 3983, 358, 1969, 1896, 1045, 5293, 315, 279, 6524, 3159, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:14,326][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0443, Std=0.8119, Min=-3.4895, Max=3.1928
[2025-12-24 23:56:14,425][eval][ERROR] - Error sample 31: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:14,432][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:14,432][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:14,439][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nThen, when your manager added two more weeks, I was already committed." He dropped upon the stool in front of her and sat with his hands hanging between his knees.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:14,440][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 12209, 11, 979, 697, 6645, 3694, 1378, 803, 5555, 11, 358, 572, 2669, 11163, 1189, 1260, 12226, 5193, 279, 63072, 304, 4065, 315, 1059, 323, 7578, 448, 806, 6078, 20704, 1948, 806, 30524, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:14,441][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0415, Std=0.7371, Min=-3.1270, Max=3.4828
[2025-12-24 23:56:14,542][eval][ERROR] - Error sample 32: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:14,549][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:14,551][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:14,558][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\n"There is my news," he said.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:14,558][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 24894, 374, 847, 3669, 1335, 566, 1053, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:14,559][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0132, Std=0.7408, Min=-3.1515, Max=2.3759
[2025-12-24 23:56:14,658][eval][ERROR] - Error sample 33: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:14,665][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:14,665][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:14,672][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nTHE HOF BRAU HOWEVER IS LESS DISTINCTIVELY GERMAN AS THE GREATER NUMBER OF ITS PATRONS ARE AMERICANS<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:14,673][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 17229, 472, 12483, 73763, 52, 29316, 3424, 74948, 59086, 3090, 49209, 89221, 22310, 5752, 3168, 61896, 19157, 36836, 3008, 46075, 44370, 49, 29526, 15824, 6769, 35916, 11692, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:14,674][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0458, Std=0.7179, Min=-3.6345, Max=3.1501
[2025-12-24 23:56:14,773][eval][ERROR] - Error sample 34: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:14,780][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:14,780][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:14,786][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\n"You\'re getting altogether too upset about these programs.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:14,787][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 21608, 2299, 3709, 30055, 2238, 22459, 911, 1493, 7468, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:14,788][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0001, Std=0.7753, Min=-3.0462, Max=2.9204
[2025-12-24 23:56:14,887][eval][ERROR] - Error sample 35: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:14,894][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:14,895][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:14,901][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nHE SEEMED TO BE CURSING PEOPLE WHO HAD WRONGED HIM<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:14,902][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 1799, 5052, 2716, 1479, 5146, 7206, 18548, 50, 1718, 66725, 39212, 472, 1808, 98660, 1479, 88894, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:14,903][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0116, Std=0.7366, Min=-2.9447, Max=3.3564
[2025-12-24 23:56:15,001][eval][ERROR] - Error sample 36: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:15,008][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:15,009][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:15,015][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: "<|im_start|>user\nRead this text:\nAt about four o'clock a visitor appeared: mr Shimerda, wearing his rabbit skin cap and collar, and new mittens his wife had knitted.<|im_end|>\n<|im_start|>assistant\n"
[2025-12-24 23:56:15,016][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 1655, 911, 3040, 297, 62410, 264, 20181, 9723, 25, 17317, 1417, 3134, 3235, 11, 12233, 806, 38724, 6787, 2062, 323, 36104, 11, 323, 501, 47332, 724, 806, 7403, 1030, 1148, 3762, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:15,017][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0283, Std=0.7235, Min=-3.1402, Max=3.6844
[2025-12-24 23:56:15,116][eval][ERROR] - Error sample 37: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:15,123][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:15,123][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:15,130][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nKIRKLEATHAM YEAST<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:15,131][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 42, 2801, 42, 867, 4827, 1402, 76948, 6349, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:15,132][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0025, Std=0.7117, Min=-3.0762, Max=2.6471
[2025-12-24 23:56:15,230][eval][ERROR] - Error sample 38: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:15,237][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:15,238][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:15,244][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\n"I go too."<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:15,245][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 7044, 728, 2238, 1189, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:15,246][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0592, Std=0.8733, Min=-3.4208, Max=3.6220
[2025-12-24 23:56:15,343][eval][ERROR] - Error sample 39: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:15,351][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:15,351][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:15,358][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nOTHER SWEET HERBS ARE CULTIVATED FOR PURPOSES OF MEDICINE AND PERFUMERY THEY ARE MOST GRATEFUL BOTH TO THE ORGANS OF TASTE AND SMELLING AND TO THE AROMA DERIVED FROM THEM IS DUE IN A GREAT MEASURE THE SWEET AND EXHILARATING FRAGRANCE OF OUR FLOWERY MEADS<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:15,359][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 30267, 328, 12457, 1348, 15645, 7347, 15824, 356, 3532, 3090, 9005, 4613, 7515, 50, 3008, 51599, 1317, 3981, 3567, 78377, 2794, 13663, 62493, 15824, 79099, 14773, 2336, 49636, 85489, 5146, 3168, 2726, 38, 11692, 3008, 350, 86520, 3567, 13716, 19114, 1718, 3567, 5146, 3168, 362, 3361, 32, 46777, 46549, 4295, 73703, 3424, 422, 2230, 1964, 362, 60993, 16292, 72169, 3168, 328, 12457, 1348, 3567, 4063, 39, 1715, 934, 33557, 16654, 81156, 8440, 3008, 44913, 434, 9441, 13663, 16292, 49541, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:15,360][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0323, Std=0.6554, Min=-3.1995, Max=3.9843
[2025-12-24 23:56:15,459][eval][ERROR] - Error sample 40: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:15,466][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:15,466][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:15,473][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nThrough the sore change in mine aspect, the secret of my heart was now understood of many.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:15,474][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 23857, 279, 35266, 2297, 304, 10485, 12893, 11, 279, 6234, 315, 847, 4746, 572, 1431, 15985, 315, 1657, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:15,475][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0411, Std=0.7844, Min=-2.9879, Max=3.6119
[2025-12-24 23:56:15,573][eval][ERROR] - Error sample 41: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:15,580][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:15,580][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:15,587][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: "<|im_start|>user\nRead this text:\nAT LENGTH THE STURDY LITTLE PONY SPREADING OUT HIS LEGS IN A STIFF AND LUDICROUS ATTITUDE GOT FROM UNDER THE PROFESSOR'S LEGS AND LEFT HIM STANDING WITH BOTH FEET ON A SEPARATE STONE LIKE THE COLOSSUS OF RHODES<|im_end|>\n<|im_start|>assistant\n"
[2025-12-24 23:56:15,588][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 828, 71838, 3168, 3928, 1511, 69916, 444, 60109, 393, 52275, 9256, 98521, 9808, 65126, 35426, 50, 1964, 362, 3928, 29035, 3567, 444, 4656, 1317, 1285, 2034, 41285, 57999, 79909, 4295, 56218, 3168, 25518, 9996, 868, 13272, 35426, 50, 3567, 21920, 88894, 3928, 81579, 4769, 85489, 27931, 1348, 6197, 362, 5052, 16567, 2336, 3928, 5225, 20529, 3168, 7284, 58860, 2034, 3008, 56156, 2880, 50, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:15,589][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0032, Std=0.6650, Min=-2.9359, Max=3.1912
[2025-12-24 23:56:15,687][eval][ERROR] - Error sample 42: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:15,694][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:15,695][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:15,701][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nContinue in this manner till the border is completed, arranging the sippets a pale and a dark one alternately.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:15,702][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 23526, 304, 419, 11566, 11956, 279, 3886, 374, 8145, 11, 76118, 279, 274, 42870, 264, 27539, 323, 264, 6319, 825, 6919, 2652, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:15,703][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0240, Std=0.7827, Min=-3.0391, Max=3.3839
[2025-12-24 23:56:15,801][eval][ERROR] - Error sample 43: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:15,808][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:15,809][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:15,815][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nOh, you ministers of Christ-wolves in sheep\'s clothing-you shall be judged for this!"<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:15,816][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 11908, 11, 498, 33950, 315, 3686, 2630, 18186, 304, 31912, 594, 17438, 45417, 4880, 387, 44387, 369, 419, 8958, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:15,817][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0055, Std=0.7076, Min=-3.0338, Max=2.7854
[2025-12-24 23:56:15,916][eval][ERROR] - Error sample 44: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:15,923][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:15,923][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:15,930][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nA YOUNG FELLOW WHO HAD BEEN HOVERING IN THE BACKGROUND AT ONCE STEPPED FORWARD<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:15,930][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 32, 809, 5044, 38, 434, 34671, 39212, 472, 1808, 74653, 30250, 3763, 1718, 1964, 3168, 96614, 7369, 6197, 2104, 26499, 4406, 1479, 90079, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:15,931][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0370, Std=0.6598, Min=-2.5931, Max=2.5657
[2025-12-24 23:56:16,030][eval][ERROR] - Error sample 45: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:16,037][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:16,037][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:16,044][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nWe came from Knoxville to Chattanooga, and seemed destined to make a permanent stay here.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:16,045][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 1654, 3697, 504, 94902, 311, 96843, 11, 323, 9324, 50587, 311, 1281, 264, 15330, 4717, 1588, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:16,045][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=-0.0082, Std=0.7934, Min=-2.9926, Max=3.0809
[2025-12-24 23:56:16,144][eval][ERROR] - Error sample 46: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:16,151][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:16,151][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:16,158][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nIT WAS ONE OF THOSE EFFECTS WHICH A PAINTER LOVES TO REPRESENT AND MINGLED WELL WITH THE STRUGGLING LIGHT WHICH FOUND ITS WAY BETWEEN THE BOUGHS OF THE SHADY ARCH THAT VAULTED THE BROAD GREEN ALLEY<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:16,159][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 952, 37776, 24038, 3008, 4434, 75634, 62053, 50, 78164, 362, 12878, 15942, 5017, 76764, 5146, 97122, 3567, 386, 1718, 13639, 90152, 4769, 3168, 12152, 2941, 3825, 1718, 52203, 78164, 52559, 46075, 24400, 56639, 3168, 7811, 2941, 11961, 3008, 3168, 6434, 1808, 56, 55213, 25269, 20901, 3532, 1479, 3168, 425, 47931, 53559, 8753, 53849, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:16,160][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0170, Std=0.7515, Min=-3.5375, Max=3.4661
[2025-12-24 23:56:16,258][eval][ERROR] - Error sample 47: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:16,265][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:16,266][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:16,272][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nGRETHEL SHE CRIED IN A PASSION GET SOME WATER QUICKLY BE HANSEL FAT OR LEAN THIS MORNING I WILL KILL AND COOK HIM<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:16,273][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 57063, 17229, 43, 53595, 356, 4305, 1479, 1964, 362, 50835, 1271, 7890, 65555, 75247, 93012, 8932, 7206, 472, 1093, 29365, 67953, 2726, 11148, 1093, 10039, 71313, 29871, 358, 27915, 730, 9228, 3567, 7284, 3925, 88894, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:16,274][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0044, Std=0.7239, Min=-3.3447, Max=2.9156
[2025-12-24 23:56:16,373][eval][ERROR] - Error sample 48: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:56:16,380][eval][INFO] - ðŸ” [Debug-Weights] Active Adapters: <bound method PeftAdapterMixin.active_adapters of Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (k_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (v_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (o_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Qwen2MLP(
          (gate_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (up_proj): lora.Linear(
            (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=3584, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=18944, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (down_proj): lora.Linear(
            (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
            (lora_dropout): ModuleDict(
              (tts): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (tts): Linear(in_features=18944, out_features=64, bias=False)
            )
            (lora_B): ModuleDict(
              (tts): Linear(in_features=64, out_features=3584, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)>
[2025-12-24 23:56:16,380][eval][INFO] - ðŸ” [Debug-Weights] Input Proj Stats: Mean=-0.000025, Std=0.046387
[2025-12-24 23:56:16,387][eval][INFO] - ðŸ“ [Debug-Prompt] Raw Text: '<|im_start|>user\nRead this text:\nThey met a good many acquaintances; Mainhall, indeed, knew almost every one, and he babbled on incontinently, screwing his small head about over his high collar.<|im_end|>\n<|im_start|>assistant\n'
[2025-12-24 23:56:16,388][eval][INFO] - ðŸ”¢ [Debug-Token] IDs: [151644, 872, 198, 4418, 419, 1467, 510, 6865, 2270, 264, 1661, 1657, 53527, 3020, 26, 4697, 42241, 11, 12824, 11, 6876, 4558, 1449, 825, 11, 323, 566, 293, 12523, 832, 389, 304, 21319, 4402, 11, 21966, 287, 806, 2613, 1968, 911, 916, 806, 1550, 36104, 13, 151645, 198, 151644, 77091, 198]
[2025-12-24 23:56:16,389][eval][INFO] - ðŸ“‰ [Debug-GT] Stats: Mean=0.0336, Std=0.7409, Min=-2.9293, Max=2.8708
[2025-12-24 23:56:16,487][eval][ERROR] - Error sample 49: AudioInputProjector.forward() got an unexpected keyword argument 'offset'
[2025-12-24 23:57:59,355][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-24 23:58:03,976][eval][INFO] - ðŸ” Loaded input_proj | Mean=-0.000025 | Std=0.046312
[2025-12-24 23:58:04,045][eval][INFO] - ðŸ” Loaded output_head | Mean=0.000729 | Std=0.037206
[2025-12-24 23:58:04,046][eval][INFO] - ðŸ©¹ Applying Monkey Patch to Input Projector...
[2025-12-24 23:58:07,242][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-24 23:58:07,249][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-24 23:58:07,268][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-24 23:58:07,268][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-24 23:58:07,269][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-24 23:58:07,269][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-24 23:58:07,290][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-24 23:58:07,290][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-24 23:58:07,291][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-24 23:58:07,291][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-24 23:58:07,300][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-24 23:58:07,301][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-24 23:58:07,658][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-24 23:58:07,659][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-24 23:58:07,659][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:58:07,660][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-24 23:58:07,660][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-24 23:58:07,705][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-24 23:58:07,795][eval][INFO] - ðŸ§ª Running Oracle Test...
[2025-12-24 23:58:26,322][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,322][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,324][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,324][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,324][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,325][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,325][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,326][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,331][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,331][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,331][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,331][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,331][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,331][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,334][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,335][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,335][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,335][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,335][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,335][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,335][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,336][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,336][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,336][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,336][asyncio][WARNING] - socket.send() raised exception.
[2025-12-24 23:58:26,336][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:01:33,410][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-25 00:01:34,733][eval][INFO] - ðŸ©¹ Applying Monkey Patch to Input Projector...
[2025-12-25 00:01:38,070][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-25 00:01:38,076][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-25 00:01:38,098][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-25 00:01:38,098][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-25 00:01:38,099][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-25 00:01:38,099][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-25 00:01:38,161][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-25 00:01:38,161][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-25 00:01:38,162][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-25 00:01:38,162][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-25 00:01:38,396][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-25 00:01:38,397][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-25 00:01:38,783][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-25 00:01:38,784][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-25 00:01:38,784][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:01:38,784][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-25 00:01:38,785][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:01:38,839][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-25 00:01:38,957][eval][INFO] - ðŸ§ª Running Oracle Test...
[2025-12-25 00:01:39,203][eval][ERROR] - Error sample 0: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,209][eval][ERROR] - Error sample 1: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,215][eval][ERROR] - Error sample 2: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,220][eval][ERROR] - Error sample 3: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,225][eval][ERROR] - Error sample 4: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,231][eval][ERROR] - Error sample 5: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,236][eval][ERROR] - Error sample 6: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,241][eval][ERROR] - Error sample 7: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,247][eval][ERROR] - Error sample 8: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,251][eval][ERROR] - Error sample 9: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,257][eval][ERROR] - Error sample 10: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,262][eval][ERROR] - Error sample 11: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,266][eval][ERROR] - Error sample 12: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,271][eval][ERROR] - Error sample 13: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,276][eval][ERROR] - Error sample 14: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,281][eval][ERROR] - Error sample 15: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,287][eval][ERROR] - Error sample 16: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,291][eval][ERROR] - Error sample 17: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,297][eval][ERROR] - Error sample 18: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,302][eval][ERROR] - Error sample 19: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,307][eval][ERROR] - Error sample 20: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,311][eval][ERROR] - Error sample 21: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,316][eval][ERROR] - Error sample 22: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,321][eval][ERROR] - Error sample 23: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,325][eval][ERROR] - Error sample 24: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,330][eval][ERROR] - Error sample 25: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,335][eval][ERROR] - Error sample 26: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,340][eval][ERROR] - Error sample 27: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,347][eval][ERROR] - Error sample 28: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,352][eval][ERROR] - Error sample 29: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,358][eval][ERROR] - Error sample 30: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,362][eval][ERROR] - Error sample 31: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,367][eval][ERROR] - Error sample 32: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,372][eval][ERROR] - Error sample 33: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,377][eval][ERROR] - Error sample 34: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,381][eval][ERROR] - Error sample 35: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,387][eval][ERROR] - Error sample 36: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,391][eval][ERROR] - Error sample 37: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,396][eval][ERROR] - Error sample 38: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,401][eval][ERROR] - Error sample 39: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,406][eval][ERROR] - Error sample 40: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,412][eval][ERROR] - Error sample 41: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,420][eval][ERROR] - Error sample 42: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,425][eval][ERROR] - Error sample 43: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,431][eval][ERROR] - Error sample 44: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,435][eval][ERROR] - Error sample 45: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,440][eval][ERROR] - Error sample 46: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,445][eval][ERROR] - Error sample 47: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,450][eval][ERROR] - Error sample 48: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:01:39,454][eval][ERROR] - Error sample 49: 'Qwen2ForCausalLM' object has no attribute 'peft_config'
[2025-12-25 00:03:45,250][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-25 00:03:46,576][eval][INFO] - ðŸ©¹ Applying Monkey Patch to Input Projector...
[2025-12-25 00:03:49,823][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-25 00:03:49,830][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-25 00:03:49,852][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-25 00:03:49,852][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-25 00:03:49,853][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-25 00:03:49,853][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-25 00:03:49,915][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-25 00:03:49,915][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-25 00:03:49,916][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-25 00:03:49,916][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-25 00:03:50,149][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-25 00:03:50,150][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-25 00:03:50,510][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-25 00:03:50,511][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-25 00:03:50,511][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:03:50,512][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-25 00:03:50,512][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:03:50,566][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-25 00:03:50,680][eval][INFO] - ðŸ§ª Running Oracle Test...
[2025-12-25 00:03:56,310][eval][INFO] - Gen Latent Final Stats: Mean=-0.00, Std=1.02
[2025-12-25 00:04:14,182][eval][INFO] - Gen Latent Final Stats: Mean=0.00, Std=1.00
[2025-12-25 00:04:18,015][eval][INFO] - Gen Latent Final Stats: Mean=0.01, Std=1.01
[2025-12-25 00:04:21,102][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,102][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,105][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,106][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,111][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,111][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,111][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,111][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,111][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,111][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,113][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,115][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,116][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,116][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,116][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,116][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:04:21,116][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:06:57,578][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-25 00:06:58,904][eval][INFO] - ðŸ©¹ Applying Monkey Patch to Input Projector...
[2025-12-25 00:07:02,144][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-25 00:07:02,151][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-25 00:07:02,172][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-25 00:07:02,172][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-25 00:07:02,173][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-25 00:07:02,173][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-25 00:07:02,236][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-25 00:07:02,236][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-25 00:07:02,237][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-25 00:07:02,237][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-25 00:07:02,483][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-25 00:07:02,484][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-25 00:07:02,858][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-25 00:07:02,859][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-25 00:07:02,859][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:07:02,860][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-25 00:07:02,860][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:07:02,914][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-25 00:07:03,027][eval][INFO] - ðŸ§ª Running Oracle Test...
[2025-12-25 00:07:08,684][eval][INFO] - Gen Latent Final Stats: Mean=-0.00, Std=1.02
[2025-12-25 00:07:26,708][eval][INFO] - Gen Latent Final Stats: Mean=0.00, Std=1.00
[2025-12-25 00:07:30,580][eval][INFO] - Gen Latent Final Stats: Mean=0.01, Std=1.01
[2025-12-25 00:07:43,405][eval][INFO] - Gen Latent Final Stats: Mean=-0.02, Std=1.00
[2025-12-25 00:07:45,204][eval][INFO] - Gen Latent Final Stats: Mean=0.02, Std=0.98
[2025-12-25 00:07:51,911][eval][INFO] - Gen Latent Final Stats: Mean=0.02, Std=0.98
[2025-12-25 00:07:55,369][eval][INFO] - Gen Latent Final Stats: Mean=0.00, Std=1.00
[2025-12-25 00:07:56,061][eval][INFO] - Gen Latent Final Stats: Mean=-0.00, Std=1.02
[2025-12-25 00:07:57,387][eval][INFO] - Gen Latent Final Stats: Mean=-0.04, Std=1.00
[2025-12-25 00:08:10,357][eval][INFO] - Gen Latent Final Stats: Mean=0.00, Std=1.01
[2025-12-25 00:08:12,417][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,417][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,420][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,420][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,421][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,425][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,426][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,426][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,426][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,426][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,426][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,426][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,428][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,429][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,429][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,429][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,429][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,429][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,429][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,429][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,430][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,430][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,430][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,430][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:08:12,430][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:09:22,568][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-25 00:09:23,872][eval][INFO] - ðŸ©¹ Applying Monkey Patch to Input Projector...
[2025-12-25 00:09:27,016][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-25 00:09:27,023][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-25 00:09:27,044][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-25 00:09:27,044][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-25 00:09:27,045][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-25 00:09:27,045][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-25 00:09:27,106][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-25 00:09:27,107][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-25 00:09:27,108][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-25 00:09:27,108][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-25 00:09:27,331][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-25 00:09:27,332][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-25 00:09:27,745][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-25 00:09:27,746][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-25 00:09:27,747][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:09:27,747][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-25 00:09:27,747][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:09:27,801][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-25 00:09:27,916][eval][INFO] - ðŸ§ª Running Oracle Test...
[2025-12-25 00:09:36,860][eval][INFO] - Gen Latent Final Stats: Mean=-0.00, Std=1.02
[2025-12-25 00:10:06,457][eval][INFO] - Gen Latent Final Stats: Mean=0.00, Std=1.00
[2025-12-25 00:10:12,763][eval][INFO] - Gen Latent Final Stats: Mean=0.01, Std=1.01
[2025-12-25 00:10:33,830][eval][INFO] - Gen Latent Final Stats: Mean=-0.02, Std=1.00
[2025-12-25 00:10:36,774][eval][INFO] - Gen Latent Final Stats: Mean=0.02, Std=0.98
[2025-12-25 00:10:47,814][eval][INFO] - Gen Latent Final Stats: Mean=0.02, Std=0.98
[2025-12-25 00:10:53,485][eval][INFO] - Gen Latent Final Stats: Mean=0.00, Std=1.00
[2025-12-25 00:10:54,144][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,145][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,147][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,147][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,148][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,152][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,152][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,152][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,153][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,153][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,153][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,153][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,153][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,155][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,157][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,157][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,157][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,157][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,157][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,157][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,157][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,157][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,157][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,157][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,157][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,157][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:10:54,157][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:11:47,109][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-25 00:11:48,415][eval][INFO] - ðŸ©¹ Applying Monkey Patch to Input Projector...
[2025-12-25 00:11:51,550][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-25 00:11:51,556][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-25 00:11:51,817][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-25 00:11:51,818][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-25 00:11:51,818][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-25 00:11:51,818][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-25 00:11:51,874][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
[2025-12-25 00:11:51,874][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-25 00:11:51,875][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-25 00:11:51,875][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-25 00:11:51,884][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-25 00:11:51,885][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-25 00:11:52,260][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-25 00:11:52,261][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-25 00:11:52,261][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:11:52,261][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-25 00:11:52,262][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:11:52,315][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-25 00:11:52,427][eval][INFO] - ðŸ§ª Running Oracle Test...
[2025-12-25 00:12:01,375][eval][INFO] - Gen Latent Final Stats: Mean=-0.00, Std=1.02
[2025-12-25 00:12:31,127][eval][INFO] - Gen Latent Final Stats: Mean=0.00, Std=1.00
[2025-12-25 00:12:37,006][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,006][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,010][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,015][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,015][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,015][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,015][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,015][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,016][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,016][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,018][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,019][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,019][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,019][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,019][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,019][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,019][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,019][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,019][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,020][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,020][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,020][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,020][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,020][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:37,023][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:12:55,687][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-25 00:12:57,025][eval][INFO] - ðŸ©¹ Applying Monkey Patch to Input Projector...
[2025-12-25 00:13:00,286][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-25 00:13:00,292][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-25 00:13:00,314][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-25 00:13:00,314][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-25 00:13:00,314][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-25 00:13:00,315][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-25 00:13:00,377][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-25 00:13:00,377][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-25 00:13:00,378][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-25 00:13:00,378][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-25 00:13:00,625][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-25 00:13:00,626][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-25 00:13:00,990][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-25 00:13:00,991][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-25 00:13:00,991][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:13:00,991][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-25 00:13:00,991][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:13:01,045][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-25 00:13:01,160][eval][INFO] - ðŸ§ª Running Oracle Test...
[2025-12-25 00:13:10,131][eval][INFO] - Gen Latent Final Stats: Mean=-0.00, Std=1.02
[2025-12-25 00:13:39,871][eval][INFO] - Gen Latent Final Stats: Mean=0.00, Std=1.00
[2025-12-25 00:13:46,207][eval][INFO] - Gen Latent Final Stats: Mean=0.01, Std=1.01
[2025-12-25 00:14:07,255][eval][INFO] - Gen Latent Final Stats: Mean=-0.02, Std=1.00
[2025-12-25 00:14:10,193][eval][INFO] - Gen Latent Final Stats: Mean=0.02, Std=0.98
[2025-12-25 00:14:21,196][eval][INFO] - Gen Latent Final Stats: Mean=0.02, Std=0.98
[2025-12-25 00:14:26,847][eval][INFO] - Gen Latent Final Stats: Mean=0.00, Std=1.00
[2025-12-25 00:14:27,953][eval][INFO] - Gen Latent Final Stats: Mean=-0.00, Std=1.02
[2025-12-25 00:14:30,105][eval][INFO] - Gen Latent Final Stats: Mean=-0.04, Std=1.00
[2025-12-25 00:14:31,245][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,246][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,248][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,248][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,249][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,253][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,254][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,254][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,254][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,254][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,254][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,254][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,255][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,258][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,258][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,258][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,258][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,258][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,258][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,258][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,258][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,258][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,258][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,259][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,259][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:14:31,262][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:19:32,864][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-25 00:19:34,187][eval][INFO] - ðŸ©¹ Applying Monkey Patch...
[2025-12-25 00:19:37,441][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-25 00:19:37,448][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-25 00:19:37,470][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-25 00:19:37,470][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-25 00:19:37,470][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-25 00:19:37,471][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-25 00:19:37,532][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-25 00:19:37,533][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-25 00:19:37,534][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-25 00:19:37,534][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-25 00:19:37,778][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-25 00:19:37,779][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-25 00:19:38,145][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-25 00:19:38,146][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-25 00:19:38,146][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:19:38,146][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-25 00:19:38,147][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:19:38,201][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-25 00:19:47,352][eval][INFO] - Gen Latent Truth: Mean=-0.00, Std=1.02
[2025-12-25 00:20:17,369][eval][INFO] - Gen Latent Truth: Mean=0.00, Std=1.00
[2025-12-25 00:20:23,743][eval][INFO] - Gen Latent Truth: Mean=0.01, Std=1.01
[2025-12-25 00:20:38,174][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,174][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,176][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,176][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,177][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,178][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,182][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,182][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,183][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,183][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,183][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,183][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,184][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,187][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,187][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,187][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,187][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,187][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,187][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:20:38,187][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:23:26,784][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-25 00:23:28,102][eval][INFO] - ðŸ©¹ Applying Monkey Patch...
[2025-12-25 00:23:31,314][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-25 00:23:31,321][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-25 00:23:31,342][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-25 00:23:31,342][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-25 00:23:31,342][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-25 00:23:31,343][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-25 00:23:31,405][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-25 00:23:31,405][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-25 00:23:31,406][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-25 00:23:31,406][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-25 00:23:31,646][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-25 00:23:31,646][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-25 00:23:32,033][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-25 00:23:32,034][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-25 00:23:32,035][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:23:32,035][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-25 00:23:32,035][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:23:32,089][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-25 00:23:32,200][eval][INFO] - ðŸ§ª Running Oracle Test...
[2025-12-25 00:23:41,204][eval][INFO] - Gen Latent Stats: Mean=-0.00, Std=1.02
[2025-12-25 00:24:11,065][eval][INFO] - Gen Latent Stats: Mean=0.00, Std=1.00
[2025-12-25 00:24:17,418][eval][INFO] - Gen Latent Stats: Mean=0.01, Std=1.01
[2025-12-25 00:24:27,943][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,943][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,946][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,946][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,947][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,947][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,952][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,952][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,952][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,952][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,952][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,952][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,954][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,956][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,956][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,956][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,956][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,957][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,957][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,957][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:24:27,957][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:27:57,188][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-25 00:27:58,518][eval][INFO] - ðŸ©¹ Applying Monkey Patch...
[2025-12-25 00:28:01,788][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-25 00:28:01,795][eval][INFO] - ðŸ”§ Initializing Vocoder...
[2025-12-25 00:28:01,817][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _speechbrain_save
[2025-12-25 00:28:01,817][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _speechbrain_load
[2025-12-25 00:28:01,818][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save
[2025-12-25 00:28:01,818][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load
[2025-12-25 00:28:01,881][speechbrain.utils.quirks][INFO] - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
[2025-12-25 00:28:01,881][speechbrain.utils.quirks][INFO] - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
[2025-12-25 00:28:01,882][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for _save
[2025-12-25 00:28:01,882][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for _recover
[2025-12-25 00:28:02,128][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/hyperparams.yaml'
[2025-12-25 00:28:02,128][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-libritts-16kHz' if not cached
[2025-12-25 00:28:02,507][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tmp_hifigan.
[2025-12-25 00:28:02,508][speechbrain.utils.fetching][INFO] - Fetch generator.ckpt: Using symlink found at '/data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt'
[2025-12-25 00:28:02,509][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths["generator"] = /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:28:02,509][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: generator
[2025-12-25 00:28:02,509][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): generator -> /data0/determined/users/andywu/Audio-CALM-v2/tmp_hifigan/generator.ckpt
[2025-12-25 00:28:02,564][eval][INFO] - âœ… SpeechBrain HiFi-GAN loaded.
[2025-12-25 00:28:02,674][eval][INFO] - ðŸ§ª Running Oracle Test...
[2025-12-25 00:29:11,872][eval][INFO] - Gen Latent Stats: Mean=-0.00, Std=1.02
[2025-12-25 00:29:23,276][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,277][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,284][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,284][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,285][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,285][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,285][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,285][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,285][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,287][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,289][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,289][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,289][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,289][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,289][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,289][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,289][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,289][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,289][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,289][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,289][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,289][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:29:23,290][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:30:11,697][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-25 00:30:13,013][eval][INFO] - ðŸ©¹ Applying Monkey Patch...
[2025-12-25 00:30:16,285][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-25 00:31:36,994][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:31:36,994][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:31:36,997][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:31:36,998][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:31:37,002][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:31:37,002][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:31:37,002][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:31:37,002][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:31:37,003][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:31:37,004][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:31:37,005][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:31:37,005][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:31:37,006][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:31:37,006][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:31:37,006][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:33:37,974][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-25 00:33:39,293][eval][INFO] - ðŸ©¹ Applying Monkey Patch...
[2025-12-25 00:33:42,569][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-25 00:34:03,456][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,457][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,459][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,459][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,459][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,460][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,461][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,466][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,466][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,466][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,466][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,466][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,466][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,466][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,469][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,469][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,469][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,469][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,469][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:34:03,469][asyncio][WARNING] - socket.send() raised exception.
[2025-12-25 00:36:28,847][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-25 00:36:34,834][eval][INFO] - ðŸ©¹ Applying Monkey Patch...
[2025-12-25 00:36:37,981][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-25 00:39:43,416][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-25 00:39:48,370][eval][INFO] - ðŸ©¹ Applying Monkey Patch...
[2025-12-25 00:39:52,826][eval][INFO] - Subsampling 50 from 8439 total samples.
[2025-12-25 00:44:23,601][eval][INFO] - ðŸ¤– Loading Model: /data0/determined/users/andywu/Audio-CALM-v2/qwen2_7B_Instruct
[2025-12-25 00:44:28,520][eval][INFO] - ðŸ©¹ Applying Monkey Patch...
[2025-12-25 00:44:31,978][eval][INFO] - Subsampling 50 from 8439 total samples.
