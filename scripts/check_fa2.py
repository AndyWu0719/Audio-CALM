import torch
import sys

def check_flash_attn():
    print(f"python: {sys.version.split()[0]}")
    print(f"torch: {torch.__version__}")
    
    # 1. æ£€æŸ¥ CUDA
    if not torch.cuda.is_available():
        print("âŒ CUDA is NOT available. Flash Attention requires GPU.")
        return False
    
    device = torch.device("cuda")
    print(f"CUDA version (torch): {torch.version.cuda}")
    
    # 2. æ£€æŸ¥ç®—åŠ› (Compute Capability)
    # Flash Attention 2 éœ€è¦ Ampere (8.0) æˆ–æ›´é«˜æ¶æ„
    capability = torch.cuda.get_device_capability(device)
    major, minor = capability
    cc_str = f"{major}.{minor}"
    print(f"GPU: {torch.cuda.get_device_name(device)} (Compute Capability: {cc_str})")
    
    if major < 8:
        print(f"âš ï¸  Warning: Flash Attention 2 requires Compute Capability >= 8.0 (Ampere).")
        print(f"    Your GPU ({cc_str}) might only support Flash Attention 1.x or standard attention.")
    
    # 3. å°è¯•å¯¼å…¥ Flash Attention
    try:
        import flash_attn
        print(f"âœ… Flash Attention package found. Version: {flash_attn.__version__}")
        
        # å°è¯•å¯¼å…¥å…·ä½“å‡½æ•°ä»¥ç¡®ä¿ç¼–è¯‘æ— è¯¯
        from flash_attn import flash_attn_func
        print("âœ… flash_attn_func loaded successfully.")
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒ v2
        if int(flash_attn.__version__.split('.')[0]) >= 2:
            print("ğŸš€ Ready for Flash Attention 2!")
            return True
        else:
            print("âš ï¸  Installed version is < 2.0. Recommended to upgrade.")
            return False
            
    except ImportError:
        print("âŒ Flash Attention package NOT found.")
        return False
    except Exception as e:
        print(f"âŒ Flash Attention found but failed to load. Error:\n{e}")
        return False

if __name__ == "__main__":
    print("-" * 30)
    success = check_flash_attn()
    print("-" * 30)
    if success:
        print("ç»“è®º: ä½ çš„ç¯å¢ƒæ”¯æŒå¹¶å·²é…ç½® Flash Attention 2ã€‚")
    else:
        print("ç»“è®º: éœ€è¦å®‰è£…æˆ–ä¿®å¤ Flash Attentionã€‚")