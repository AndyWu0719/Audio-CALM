"""
Flow-based Audio-CALM Training Script.
Optimized for Speed, DDP Stability, and Mixture of Adapters (MoA).
VERSION: FINAL_STABLE (Includes SOA training & Explicit Saving)
"""

import os
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ŒæŠ‘åˆ¶ Transformers çš„è¿‡æ—¶è­¦å‘Šï¼Œä¿æŒæ—¥å¿—æ¸…çˆ½
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "true"
import sys
import math
import random
import warnings
from dataclasses import dataclass
from typing import List, Dict, Any
from glob import glob

import torch
import torch.distributed as dist
from torch.utils.data import Dataset, DataLoader
from transformers import Trainer, TrainingArguments, set_seed, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from peft.utils import set_peft_model_state_dict

import hydra
from omegaconf import DictConfig, OmegaConf, open_dict
from rich.console import Console
from rich.traceback import install

# --- Monkey Patch for PyTorch 2.6+ & DeepSpeed ---
# ã€ä¿®å¤ã€‘ï¼šPyTorch æ–°ç‰ˆæœ¬ä¸­ torch.load é»˜è®¤å¯ç”¨äº† weights_only=Trueï¼Œ
# è¿™ä¼šå¯¼è‡´åŠ è½½æ—§ç‰ˆ checkpoint æˆ–ç”± DeepSpeed ä¿å­˜çš„å¤æ‚å¯¹è±¡æ—¶æŠ¥é”™ã€‚
# è¿™é‡Œå¼ºåˆ¶å°†å…¶æ”¹å› weights_only=False ä»¥å…¼å®¹æ—§è¡Œä¸ºã€‚
_orig_torch_load = torch.load
def safe_torch_load(*args, **kwargs):
    if 'weights_only' not in kwargs: kwargs['weights_only'] = False
    return _orig_torch_load(*args, **kwargs)
torch.load = safe_torch_load
# -------------------------------------------------

# å°†å½“å‰å·¥ä½œç›®å½•åŠ å…¥è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ models æ¨¡å—
sys.path.append(os.getcwd())
# ã€å¯¹åº”å…³ç³»ã€‘ï¼šå¯¼å…¥ modeling_calm.py ä¸­çš„æ¨¡å‹å®šä¹‰
from models.modeling_calm import QwenCALM, QwenCALMConfig

install(show_locals=False)
console = Console()
warnings.filterwarnings("ignore")
torch.backends.cuda.matmul.allow_tf32 = True

def _get_rank_safe() -> int:
    """å®‰å…¨è·å–å½“å‰è¿›ç¨‹çš„ Rankï¼Œç”¨äºå¤šå¡è®­ç»ƒæ—¶çš„æ¡ä»¶æ‰“å°"""
    try: return dist.get_rank()
    except: return 0

# ---------------------------------------------------------------------
# Dataset Definition
# ---------------------------------------------------------------------
class CalmDataset(Dataset):
    """
    åŠŸèƒ½ï¼šCALM æ¨¡å‹çš„æ··åˆæ•°æ®é›†åŠ è½½å™¨ã€‚
    
    ã€æ–‡ä»¶é—´å…³ç³»ã€‘ï¼š
    - è¾“å…¥ï¼šè¯»å–ç”± `preprocess/build_manifest.py` ç”Ÿæˆçš„ .jsonl æ¸…å•æˆ–ç›®å½•ä¸‹çš„ .trans.txt ç´¢å¼•ã€‚
    - ä¾èµ–ï¼šè¯»å–ç”± `preprocess/process_dataset.py` ç”Ÿæˆçš„ .pt (Latent) æ–‡ä»¶ã€‚
    """
    def __init__(self, latent_dir, subsets, tokenizer, max_text_len=512, 
                 max_audio_len=1024, use_latents=False, task_mode="mix", task_prob_tts=0.5, 
                 max_samples=None):
        self.tokenizer = tokenizer
        self.max_text_len = max_text_len
        self.max_audio_len = max_audio_len
        self.task_mode = task_mode
        self.task_prob_tts = task_prob_tts
        self.latent_dir = latent_dir
        
        # ç¡®å®š <|im_end|> Token çš„ IDï¼Œç”¨äº ASR ä»»åŠ¡çš„ Label æˆªæ–­
        if hasattr(tokenizer, "eod_id"):
             self.im_end_id = tokenizer.eod_id
        else:
             enc = tokenizer.encode("<|im_end|>", add_special_tokens=False)
             self.im_end_id = enc[-1] if len(enc)>0 else tokenizer.eos_token_id

        self.data = []
        if _get_rank_safe() == 0: 
            console.log(f"[green]Scanning Latent Directory: {latent_dir}[/green]")
            console.log(f"[dim]Subsets pattern: {subsets}[/dim]")

        # 1. æ‰«æè½¬å½•æ–‡ä»¶ (.trans.txt)
        # æ”¯æŒé€šè¿‡é€—å·åˆ†éš”çš„å­é›†åˆ—è¡¨ï¼ˆå¦‚ train-clean-100,train-other-500ï¼‰
        trans_files = []
        for subset in subsets.split(","):
            subset = subset.strip()
            if subset == ".":
                pattern = os.path.join(latent_dir, "**", "*.trans.txt")
            else:
                pattern = os.path.join(latent_dir, subset, "**", "*.trans.txt")
            
            found = glob(pattern, recursive=True)
            trans_files.extend(found)

        # 2. è§£æè½¬å½•æ–‡ä»¶ï¼Œæ„å»ºå†…å­˜ä¸­çš„æ•°æ®ç´¢å¼•
        for trans_file in trans_files:
            folder = os.path.dirname(trans_file)
            try:
                with open(trans_file, "r", encoding="utf-8") as fh:
                    for line in fh:
                        # æ ¼å¼: file_id transcript_text
                        parts = line.strip().split(" ", 1)
                        if len(parts) != 2: continue
                        
                        fid, txt = parts
                        # å‡è®¾ Latent æ–‡ä»¶åä¸º {fid}.ptï¼Œä¸ preprocess é˜¶æ®µä¸€è‡´
                        pt_path = os.path.join(folder, f"{fid}.pt")
                        
                        if os.path.exists(pt_path):
                            self.data.append({"text": txt, "file_path": pt_path})
            except Exception:
                continue
                                    
        if _get_rank_safe() == 0: 
            console.log(f"[bold green]Matched Pairs: {len(self.data)}[/bold green]")
            if len(self.data) == 0:
                console.log(f"[bold red]âŒ CRITICAL: No data found in {latent_dir}.[/bold red]")
        
        # 3. æ ·æœ¬æ•°é‡é™åˆ¶ï¼ˆç”¨äºå¿«é€Ÿè°ƒè¯•ï¼‰
        if max_samples is not None and max_samples > 0:
            if len(self.data) > max_samples:
                self.data = self.data[:max_samples]
                if _get_rank_safe() == 0:
                    console.log(f"[yellow]âš ï¸ Subsampled dataset to {len(self.data)} items.[/yellow]")

    def __len__(self): return len(self.data)

    def __getitem__(self, idx):
        """
        åŠŸèƒ½ï¼šè·å–å•ä¸ªæ ·æœ¬ï¼Œå¹¶æ ¹æ®ä»»åŠ¡æ¨¡å¼æ„å»º Input IDs å’Œ Labelsã€‚
        """
        try:
            item = self.data[idx]
            
            # 1. åŠ¨æ€å†³å®šå½“å‰æ ·æœ¬çš„ä»»åŠ¡æ¨¡å¼ (Mix Mode)
            if self.task_mode == "mix":
                # æŒ‰æ¦‚ç‡éšæœºåˆ†é… TTS æˆ– ASR
                mode = "tts" if random.random() < self.task_prob_tts else "asr"
            else:
                mode = self.task_mode

            # 2. åŠ è½½éŸ³é¢‘ Latent
            # ã€å¯¹åº”å…³ç³»ã€‘ï¼šåŠ è½½ç”± process_dataset.py ä¿å­˜çš„ .pt æ–‡ä»¶
            payload = torch.load(item["file_path"], map_location="cpu")
            # å…¼å®¹å¤„ç†ï¼šæ”¯æŒç›´æ¥å­˜å‚¨ Tensor æˆ–å­˜å‚¨åœ¨ dict ä¸­
            audio = payload.get("latent", payload) if isinstance(payload, dict) else payload
            if audio is None: return {"_valid": False}
            
            # ç»´åº¦è°ƒæ•´ï¼šç¡®ä¿æ˜¯ [Time, Dim] æ ¼å¼
            # VAE è¾“å‡ºé€šå¸¸æ˜¯ [Dim=64, Time]ï¼Œè¿™é‡Œéœ€è¦è½¬ç½®
            if audio.shape[0] == 64: audio = audio.transpose(0, 1)
            
            # 3. éŸ³é¢‘é•¿åº¦è£å‰ªé€»è¾‘
            if audio.shape[0] > self.max_audio_len:
                if mode == "asr": 
                    # ASR ä»»åŠ¡ï¼šå¦‚æœéŸ³é¢‘å¤ªé•¿ï¼Œè¿™é‡Œç®€å•è·³è¿‡ï¼ˆå®é™…ç”Ÿäº§ä¸­åº”åˆ‡ç‰‡ï¼‰
                    return {"input_ids": [0], "_valid": False} 
                else:
                    # TTS ä»»åŠ¡ï¼š[é‡è¦ä¿®å¤] å¿…é¡»ä»å¤´å¼€å§‹æˆªå– (Start=0)
                    # å› ä¸ºæˆ‘ä»¬ä½¿ç”¨äº† SOA (Start of Audio) Tokenï¼Œå®ƒéšå«è¡¨ç¤ºéŸ³é¢‘çš„å¼€å§‹ã€‚
                    # å¦‚æœéšæœºæˆªå–ä¸­é—´ä¸€æ®µï¼ŒLLM ä¼šå› ä¸ºä¸Šä¸‹æ–‡ä¸åŒ¹é…è€Œæ— æ³•æ”¶æ•›ã€‚
                    start = 0 
                    audio = audio[start : start + self.max_audio_len]

            # 4. æ„å»º Prompt å’Œ Text IDs
            prompt = f"Read this text:\n{item['text']}" if mode == "tts" else "Transcribe the following audio:"
            # ä½¿ç”¨ ChatML æ ¼å¼
            user_txt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
            user_ids = self.tokenizer.encode(user_txt, add_special_tokens=False)
            
            # 5. æ„å»ºæœ€ç»ˆçš„ IDs å’Œ Labels
            if mode == "tts":
                # TTS æ¨¡å¼:
                # Input = [Prompt]
                # Label = [-100] (å› ä¸ºæ–‡æœ¬éƒ¨åˆ†ä¸éœ€è¦ LLM é¢„æµ‹ï¼ŒLLM åªé¢„æµ‹éŸ³é¢‘ Condition)
                text_ids = user_ids
                labels = [-100] * len(text_ids)
            else:
                # ASR æ¨¡å¼:
                # Input = [Prompt + Transcript]
                # Label = [-100 (Prompt) + Transcript (Target)]
                target_txt = f"{item['text']}<|im_end|>"
                target_ids = self.tokenizer.encode(target_txt, add_special_tokens=False)
                
                # æ–‡æœ¬é•¿åº¦æˆªæ–­
                max_target_len = self.max_text_len - len(user_ids)
                if len(target_ids) > max_target_len:
                    target_ids = target_ids[:max_target_len]
                    if self.im_end_id is not None:
                        target_ids[-1] = self.im_end_id

                text_ids = user_ids + target_ids
                labels = [-100] * len(user_ids) + target_ids

            # æœ€ç»ˆé•¿åº¦æˆªæ–­
            if len(text_ids) > self.max_text_len:
                text_ids = text_ids[:self.max_text_len]
                labels = labels[:self.max_text_len]

            # è¿”å›æ•°æ®å­—å…¸
            return {
                "input_ids": torch.tensor(text_ids, dtype=torch.long),
                "labels": torch.tensor(labels, dtype=torch.long),
                "audio_features": audio, # [Time, Dim]
                "task_mode": mode,
                "_valid": True
            }
        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šè¿”å›æ— æ•ˆæ ‡è®°ï¼ŒCollator ä¼šè¿‡æ»¤æ‰
            return {"input_ids": [0], "_valid": False}

# ---------------------------------------------------------------------
# Data Collator
# ---------------------------------------------------------------------
@dataclass
class CalmCollator:
    """
    åŠŸèƒ½ï¼šæ•°æ®æ•´ç†å™¨ã€‚
    ä½œç”¨ï¼šå°† Dataset è¿”å›çš„æ ·æœ¬åˆ—è¡¨å †å æˆ Batchï¼Œå¹¶è¿›è¡Œ Padding å’Œ ç‰¹å¾å¢å¼ºã€‚
    """
    pad_token_id: int
    audio_pad_val: float = 0.0
    training: bool = False

    def _apply_spec_augment(self, audio_feat: torch.Tensor):
        """
        åŠŸèƒ½ï¼šé¢‘è°±å¢å¼º (SpecAugment)ã€‚
        ä½œç”¨ï¼šåœ¨è®­ç»ƒ ASR æ—¶ï¼Œéšæœºæ©ç›–æ—¶é—´æ®µï¼Œå¼ºè¿«æ¨¡å‹åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
        """
        D, T = audio_feat.shape
        num_masks = 1 if T < 150 else 2
        for _ in range(num_masks):
            if T > 20:
                mask_len = random.randint(5, 10) 
                t0 = random.randint(0, T - mask_len)
                # ä½¿ç”¨å½“å‰ç‰¹å¾çš„æœ€å°å€¼è¿›è¡Œå¡«å……ï¼ˆæ¨¡æ‹Ÿé™éŸ³/èƒŒæ™¯åº•å™ªï¼‰
                min_val = audio_feat.min()
                audio_feat[:, t0 : t0 + mask_len].fill_(min_val)
        return audio_feat

    def __call__(self, features):
        # 1. è¿‡æ»¤æ— æ•ˆæ ·æœ¬
        valid = [f for f in features if f.get("_valid", False)]
        if not valid:
            # å¦‚æœæ•´ä¸ª Batch éƒ½æ— æ•ˆï¼Œè¿”å›ä¸€ä¸ªå‡çš„æœ€å° Batch é˜²æ­¢è®­ç»ƒå´©æºƒ
            return {
                "text_input_ids": torch.tensor([[self.pad_token_id]], dtype=torch.long),
                "attention_mask": torch.tensor([[0]], dtype=torch.long),
                "labels": torch.tensor([[-100]], dtype=torch.long),
                "audio_features": torch.zeros(1, 1, 64),
                "audio_lens": torch.tensor([1], dtype=torch.long),
                "task_modes": ["tts"]
            }

        # 2. å¤„ç†éŸ³é¢‘ç‰¹å¾
        proc_audio = []
        for f in valid:
            feat = f["audio_features"]
            feat = feat.transpose(0, 1) # è½¬ç½®ä¸º [Dim, Time] ä»¥ä¾¿è¿›è¡Œ Mask æ“ä½œ
            if self.training and f["task_mode"] == "asr":
                feat = self._apply_spec_augment(feat.clone())
            proc_audio.append(feat.transpose(0, 1)) # è½¬å› [Time, Dim]

        # 3. ç»„è£… Batch
        batch = {
            # æ–‡æœ¬ Padding (Right Padding)
            "text_input_ids": torch.nn.utils.rnn.pad_sequence(
                [f["input_ids"] for f in valid],
                batch_first=True, 
                padding_value=self.pad_token_id
            ),
            # Label Padding (-100 è¡¨ç¤ºå¿½ç•¥è®¡ç®— Loss)
            "labels": torch.nn.utils.rnn.pad_sequence(
                [f["labels"] for f in valid], 
                batch_first=True, 
                padding_value=-100
            ),
            # éŸ³é¢‘ Padding
            "audio_features": torch.nn.utils.rnn.pad_sequence(
                proc_audio, 
                batch_first=True, 
                padding_value=self.audio_pad_val
            ).transpose(1, 2), # æœ€ç»ˆè¾“å‡º [Batch, Dim, Time] é€‚é… Conv1d
            "audio_lens": torch.tensor([f.shape[0] for f in proc_audio], dtype=torch.long),
            "task_modes": [f["task_mode"] for f in valid]
        }
        
        # ç”Ÿæˆ Attention Mask
        batch["attention_mask"] = (batch["text_input_ids"] != self.pad_token_id).long()
        return batch

# ---------------------------------------------------------------------
# Trainer (Modified for Saving)
# ---------------------------------------------------------------------
class CalmTrainer(Trainer):
    """
    åŠŸèƒ½ï¼šè‡ªå®šä¹‰è®­ç»ƒå™¨ã€‚
    ä½œç”¨ï¼š
    1. å®ç°æ··åˆé€‚é…å™¨ (MoA) çš„åŠ¨æ€åˆ‡æ¢é€»è¾‘ã€‚
    2. å®ç°å‚æ•°åˆ†ç»„ä¼˜åŒ–ï¼ˆåŒºåˆ† Head å’Œ Base Modelï¼‰ã€‚
    3. è‡ªå®šä¹‰æ¨¡å‹ä¿å­˜é€»è¾‘ï¼ˆä¿å­˜é LoRA å‚æ•°ï¼‰ã€‚
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # ç”¨äºè®°å½•åˆ†ä»»åŠ¡çš„ Loss
        self.loss_meters = {"tts": 0.0, "asr": 0.0, "tts_cnt": 0, "asr_cnt": 0}

    def create_optimizer(self):
        """
        åŠŸèƒ½ï¼šåˆ›å»ºä¼˜åŒ–å™¨ã€‚
        ä½œç”¨ï¼šå°† Projector/Head/SOA Embed åˆ†ç¦»å‡ºæ¥ï¼Œå…è®¸è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡ï¼ˆå¦‚æœéœ€è¦ï¼‰ã€‚
        """
        if self.optimizer is None:
            decay_parameters = []
            no_decay_parameters = []
            projector_parameters = []
            
            # [å…³é”®] æ ‡è®°å“ªäº›å‚æ•°å±äºâ€œå¤´éƒ¨ç»„ä»¶â€
            head_keywords = ["input_proj", "output_head", "soa_embed"]
            
            model_to_opt = self.model_wrapped if hasattr(self, "model_wrapped") else self.model
            if hasattr(model_to_opt, "module"): model_to_opt = model_to_opt.module

            for name, param in model_to_opt.named_parameters():
                if not param.requires_grad: continue
                
                is_head = any(k in name for k in head_keywords) and "lora" not in name
                
                if is_head:
                    projector_parameters.append(param)
                else:
                    if "bias" in name or "LayerNorm" in name:
                        no_decay_parameters.append(param)
                    else:
                        decay_parameters.append(param)

            # å‚æ•°åˆ†ç»„
            optimizer_grouped_parameters = [
                {"params": decay_parameters, "weight_decay": self.args.weight_decay, "lr": self.args.learning_rate},
                {"params": no_decay_parameters, "weight_decay": 0.0, "lr": self.args.learning_rate},
                # Head éƒ¨åˆ†å¯ä»¥è®¾ç½®æ›´é«˜çš„ LR (è¿™é‡Œæš‚æ—¶è®¾ä¸º 1.0 * base_lr)
                {"params": projector_parameters, "weight_decay": self.args.weight_decay, "lr": 1.0 * self.args.learning_rate}, 
            ]

            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)
            self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)
        return self.optimizer

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        """
        åŠŸèƒ½ï¼šè®¡ç®— Loss æ­¥éª¤ã€‚
        ä½œç”¨ï¼š
        1. æ ¹æ® task_mode åˆ‡æ¢ LoRA Adapterã€‚
        2. è°ƒç”¨æ¨¡å‹ forward è®¡ç®— Lossã€‚
        3. å¤„ç† DDP æ¨¡å¼ä¸‹çš„ Ghost Gradientsã€‚
        """
        # 1. åˆ‡æ¢ Adapter (MoA æ ¸å¿ƒé€»è¾‘)
        peft_model = model.module.llm if hasattr(model, "module") else model.llm
        task_modes = inputs.get("task_modes", ["tts"])
        # ç®€å•ç­–ç•¥ï¼šæ ¹æ® Batch ä¸­ä»»åŠ¡æ•°é‡çš„å¤šæ•°æ´¾å†³å®šæ¿€æ´»å“ªä¸ª Adapter
        target_adapter = "tts" if task_modes.count("tts") >= task_modes.count("asr") else "asr"
        
        if hasattr(peft_model, "set_adapter") and target_adapter in peft_model.peft_config:
            peft_model.set_adapter(target_adapter)
        
        # 2. å‰å‘ä¼ æ’­
        # ã€å¯¹åº”å…³ç³»ã€‘ï¼šè°ƒç”¨ modeling_calm.py ä¸­çš„ QwenCALM.forward
        outputs = model(**inputs)
        loss = outputs["loss"]

        # 3. DDP å…¼å®¹å¤„ç†
        # åœ¨ DDP æ¨¡å¼ä¸‹ï¼Œå¦‚æœ forward ä¸­æŸäº›å‚æ•°æ²¡æœ‰å‚ä¸è®¡ç®—ï¼ˆä¾‹å¦‚ TTS Batch ä¸­ ASR çš„å‚æ•°ï¼‰ï¼Œ
        # åå‘ä¼ æ’­ä¼šæŠ¥é”™ã€‚è¿™é‡ŒåŠ ä¸€ä¸ª dummy loss * 0.0 æ¥æ¬ºéª— DDPã€‚
        if self.model.training:
            raw_model = model.module if hasattr(model, "module") else model
            dummy_loss = 0.0
            for name, param in raw_model.named_parameters():
                if param.requires_grad and param.grad is None:
                    dummy_loss += param.sum() * 0.0
            loss += dummy_loss

        # 4. è®°å½•æ—¥å¿—
        if self.model.training:
             l_tts = outputs.get("loss_tts", torch.tensor(0., device=loss.device)).detach()
             l_asr = outputs.get("loss_asr", torch.tensor(0., device=loss.device)).detach()
             self.loss_meters["tts"] += l_tts.item()
             self.loss_meters["asr"] += l_asr.item()
             if l_tts > 0: self.loss_meters["tts_cnt"] += 1
             if l_asr > 0: self.loss_meters["asr_cnt"] += 1

        return (loss, outputs) if return_outputs else loss

    def log(self, logs: Dict[str, float], *args, **kwargs):
        """é‡å†™æ—¥å¿—è®°å½•ï¼ŒåŠ å…¥ TTS/ASR åˆ†é¡¹ Loss"""
        t_c = max(self.loss_meters["tts_cnt"], 1)
        a_c = max(self.loss_meters["asr_cnt"], 1)
        logs["loss_tts"] = round(self.loss_meters["tts"] / t_c, 4)
        logs["loss_asr"] = round(self.loss_meters["asr"] / a_c, 4)
        # é‡ç½®è®¡æ•°å™¨
        self.loss_meters = {"tts": 0.0, "asr": 0.0, "tts_cnt": 0, "asr_cnt": 0}
        super().log(logs, *args, **kwargs)
        
    # [å…³é”®ä¿®å¤] è‡ªå®šä¹‰ä¿å­˜é€»è¾‘
    # ä¿®å¤äº†å‚æ•°ç­¾åä»¥å…¼å®¹æ–°ç‰ˆ HF Trainerï¼Œå¹¶å¢åŠ äº†æ‰‹åŠ¨ä¿å­˜é€»è¾‘
    def save_model(self, output_dir=None, _internal_call=False, **kwargs):
        """
        åŠŸèƒ½ï¼šä¿å­˜æ¨¡å‹ Checkpointã€‚
        ä½œç”¨ï¼š
        1. è°ƒç”¨çˆ¶ç±»ä¿å­˜ LoRA Adapterã€‚
        2. æ‰‹åŠ¨ä¿å­˜ Input Projector, Output Head, å’Œ SOA Embed ä¸º .bin æ–‡ä»¶ã€‚
        """
        if output_dir is None:
            output_dir = self.args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. ä¿å­˜ LoRA (è°ƒç”¨çˆ¶ç±»)
        super().save_model(output_dir, _internal_call=_internal_call, **kwargs)
        
        # 2. æ‰‹åŠ¨ä¿å­˜é LoRA ç»„ä»¶ (ä»…ä¸»è¿›ç¨‹æ‰§è¡Œ)
        if _get_rank_safe() == 0:
            model = self.model
            if hasattr(model, "module"): 
                model = model.module 
            
            console.print(f"[magenta]ğŸ’¾ Saving Projectors & SOA to {output_dir}...[/magenta]")
            
            try:
                # ä¿å­˜ Input Projector (ASR ç”¨)
                torch.save(model.input_proj.state_dict(), os.path.join(output_dir, "input_proj.bin"))
                
                # ä¿å­˜ Output Head (TTS ç”¨)
                torch.save(model.output_head.state_dict(), os.path.join(output_dir, "output_head.bin"))
                
                # ä¿å­˜ SOA Embed (TTS ç”¨)
                if hasattr(model, "soa_embed"):
                    data_to_save = model.soa_embed.data if isinstance(model.soa_embed, torch.nn.Parameter) else model.soa_embed
                    torch.save({"weight": data_to_save}, os.path.join(output_dir, "soa_embed.bin"))
            except Exception as e:
                console.print(f"[bold red]âŒ Error saving custom components: {e}[/bold red]")
            
    def get_eval_dataloader(self, eval_dataset=None):
        if eval_dataset is None:
            eval_dataset = self.eval_dataset
        
        eval_collator = getattr(self, "eval_collator", None)
        if eval_collator is None:
            eval_collator = CalmCollator(
                pad_token_id=self.tokenizer.pad_token_id, 
                training=False
            )
        
        return DataLoader(
            eval_dataset,
            batch_size=self.args.eval_batch_size,
            collate_fn=eval_collator,
            num_workers=self.args.dataloader_num_workers,
            pin_memory=self.args.dataloader_pin_memory,
        )

# ---------------------------------------------------------------------
# Utilities
# ---------------------------------------------------------------------
def load_soft_restart_components(model, cfg, console):
    """
    åŠŸèƒ½ï¼šè½¯é‡å¯/çƒ­å¯åŠ¨åŠ è½½ã€‚
    ä½œç”¨ï¼šä»æŒ‡å®šçš„ checkpoint è·¯å¾„åŠ è½½ Projector æˆ– Head çš„æƒé‡ï¼Œç”¨äºåˆ†é˜¶æ®µè®­ç»ƒã€‚
    """
    def _load(key, model_attr, name):
        path = cfg.model.get(key, None)
        if path and os.path.exists(path):
            console.print(f"[green]Loading {name} from: {path}[/green]")
            state_dict = torch.load(path, map_location="cpu")
            # æ¸…ç† key åç§°
            clean_sd = {k.replace(f"{name}.", "").replace(f"input_proj.", "").replace(f"output_head.", ""): v for k, v in state_dict.items()}
            try:
                getattr(model, model_attr).load_state_dict(clean_sd, strict=False)
                console.print(f"[bold green]âœ… {name} Loaded.[/bold green]")
            except Exception as e:
                console.print(f"[bold red]âŒ {name} Fail: {e}[/bold red]")
        else:
            console.print(f"[yellow]âš ï¸ {name}: Random Init (Path not found)[/yellow]")

    _load("pretrained_projector_path", "input_proj", "input_proj")
    _load("pretrained_head_path", "output_head", "output_head")

# ---------------------------------------------------------------------
# Main Execution
# ---------------------------------------------------------------------
@hydra.main(version_base=None, config_path="../config", config_name="calm_config")
def main(cfg: DictConfig):
    task_mode = cfg.data.task_mode
    console.print(f"[bold]ğŸ”„ Task Mode:[/bold] {task_mode.upper()}")

    if task_mode not in cfg.data.datasets:
        raise ValueError(f"âŒ Unknown task_mode: '{task_mode}'. Available: {list(cfg.data.datasets.keys())}")

    # è·¯å¾„è§£æ
    selected_paths = cfg.data.datasets[task_mode]
    with open_dict(cfg):
        cfg.data.latent_dir = selected_paths.latent_dir
        cfg.data.eval_latent_dir = selected_paths.eval_latent_dir
        cfg.data.raw_root = selected_paths.raw_root

    console.print(f"ğŸ“‚ [Data] Training Latents: {cfg.data.latent_dir}")
    
    set_seed(cfg.training.seed)
    
    # è½¬æ¢å‚æ•°
    training_args = TrainingArguments(**OmegaConf.to_container(cfg.training, resolve=True))
    training_args.ddp_find_unused_parameters = True 
    training_args.ignore_data_skip = True
    
    # åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(cfg.model.qwen_path, trust_remote_code=True)
    if tokenizer.pad_token_id is None: 
        tokenizer.pad_token_id = tokenizer.eod_id if hasattr(tokenizer, 'eod_id') else tokenizer.eos_token_id
    
    tokenizer.padding_side = "right" # å¯¹é½æ¨¡å‹ Right Padding é€»è¾‘

    # 1. æ¨¡å‹åˆå§‹åŒ–
    # ã€å¯¹åº”å…³ç³»ã€‘ï¼šå®ä¾‹åŒ– modeling_calm.py ä¸­çš„ QwenCALM
    config = QwenCALMConfig(
        qwen_path=cfg.model.qwen_path,
        vae_path=cfg.model.vae_path,
        head_type="flow", 
        use_precomputed_latents=cfg.model.use_precomputed_latents,
        latent_dim=cfg.model.latent_dim,
        audio_loss_weight=cfg.model.audio_loss_weight,
        downsample_rate=cfg.data.latent_downsample,
        flow_hidden_dim=cfg.model.flow_hidden_dim,
        flow_num_layers=cfg.model.flow_num_layers,
    )
    model = QwenCALM(config)

    # 2. ç»„ä»¶åŠ è½½ (Soft Restart)
    console.rule("[bold cyan]Component Loading[/bold cyan]")
    load_soft_restart_components(model, cfg, console)
    
    # 3. LoRA / MoA åˆå§‹åŒ–
    if cfg.model.use_lora:
        console.print("[blue]Initializing LoRA Config...[/blue]")
        lora_config = LoraConfig(
            r=cfg.model.lora_rank, lora_alpha=cfg.model.lora_alpha,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            lora_dropout=cfg.model.lora_dropout, bias="none", task_type=TaskType.CAUSAL_LM,
            modules_to_save=[], # [FIX] æˆ‘ä»¬åœ¨ CalmTrainer ä¸­æ‰‹åŠ¨ä¿å­˜ï¼Œè¿™é‡Œç•™ç©ºä»¥é¿å…é‡å¤
        )
        
        def load_adapter_if_path_exists(adapter_name, path_key):
            path = cfg.model.get(path_key, None)
            if path and os.path.exists(path):
                console.print(f"[yellow]Loading {adapter_name} from {path}...[/yellow]")
                try:
                    if path.endswith(".safetensors"):
                        from safetensors.torch import load_file
                        sd = load_file(path)
                    else:
                        sd = torch.load(path, map_location="cpu")
                    set_peft_model_state_dict(model.llm, sd, adapter_name=adapter_name)
                    console.print(f"[bold green]âœ… {adapter_name} loaded![/bold green]")
                except Exception as e:
                    console.print(f"[red]âŒ Failed to load {adapter_name}: {e}[/red]")
            else:
                console.print(f"[dim]â„¹ï¸  {adapter_name} initialized from scratch[/dim]")
        
        # æ ¹æ®ä»»åŠ¡æ¨¡å¼é…ç½® Adapter
        if cfg.data.task_mode == "tts":
            console.print("[green] -> Mode: TTS Only[/green]")
            model.llm = get_peft_model(model.llm, lora_config, adapter_name="tts")
            load_adapter_if_path_exists("tts", "pretrained_lora_path_tts")
            
        elif cfg.data.task_mode == "asr":
            console.print("[green] -> Mode: ASR Only[/green]")
            model.llm = get_peft_model(model.llm, lora_config, adapter_name="asr")
            load_adapter_if_path_exists("asr", "pretrained_lora_path_asr")
            
        else:
            console.print("[green] -> Mode: Mix (MoA)[/green]")
            # æ··åˆæ¨¡å¼ï¼šåŒæ—¶æ³¨å…¥ä¸¤ä¸ª Adapter
            model.llm = get_peft_model(model.llm, lora_config, adapter_name="tts")
            model.llm.add_adapter("asr", lora_config)
            load_adapter_if_path_exists("tts", "pretrained_lora_path_tts")
            load_adapter_if_path_exists("asr", "pretrained_lora_path_asr")

    # 4. å†»ç»“ç­–ç•¥
    # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å†»ç»“ Input Projector (ä¿æŠ¤ ASR èƒ½åŠ›)
    should_freeze_proj = cfg.model.get("freeze_projector", False)
    
    # Projector
    model.input_proj.requires_grad_(not should_freeze_proj)
    if should_freeze_proj:
        model.input_proj.eval()
        console.print("[bold yellow]ğŸ”’ Input Projector Frozen (Protecting ASR capabilities)[/bold yellow]")
    
    # Head å§‹ç»ˆè®­ç»ƒ
    model.output_head.requires_grad_(True)
    
    # [FIX] æ˜¾å¼è§£å†» SOA Embed (TTS ä»»åŠ¡å¿…é¡»)
    if hasattr(model, "soa_embed"):
        model.soa_embed.requires_grad_(True)
        console.print("[bold green]ğŸ”“ SOA Embed Unfrozen (Ready for TTS training)[/bold green]")

    trainable_params = [n for n, p in model.named_parameters() if p.requires_grad]
    console.print(f"ğŸ”¥ Trainable Modules: {[n for n in trainable_params if 'bias' not in n][:10]} ...")

    console.rule()

    # 5. æ„å»º Trainer
    # åˆå§‹åŒ–è®­ç»ƒé›†
    train_ds = CalmDataset(
        latent_dir=cfg.data.latent_dir, 
        subsets=cfg.data.train_subsets, 
        tokenizer=tokenizer, 
        max_text_len=cfg.data.max_text_len, 
        max_audio_len=cfg.data.max_audio_len, 
        use_latents=cfg.model.use_precomputed_latents, 
        task_mode=cfg.data.task_mode, 
        task_prob_tts=cfg.data.task_prob_tts,
        max_samples=None 
    )
    
    # åˆå§‹åŒ–éªŒè¯é›†
    eval_max_samples = cfg.training.get("eval_max_samples", 200)
    eval_ds = CalmDataset(
        latent_dir=cfg.data.eval_latent_dir or cfg.data.latent_dir, 
        subsets=cfg.data.eval_subsets, 
        tokenizer=tokenizer, 
        max_text_len=cfg.data.max_text_len, 
        max_audio_len=cfg.data.max_audio_len, 
        use_latents=cfg.model.use_precomputed_latents, 
        task_mode=cfg.data.task_mode, 
        task_prob_tts=cfg.data.task_prob_tts,
        max_samples=eval_max_samples
    )
    
    # åˆå§‹åŒ– Collator
    train_collator = CalmCollator(tokenizer.pad_token_id, training=True)
    eval_collator = CalmCollator(tokenizer.pad_token_id, training=False)

    trainer = CalmTrainer(
        model=model, args=training_args,
        train_dataset=train_ds, eval_dataset=eval_ds,
        data_collator=train_collator
    )
    
    trainer.eval_collator = eval_collator
    trainer.tokenizer = tokenizer

    # 6. å¼€å§‹è®­ç»ƒ
    if training_args.resume_from_checkpoint:
        trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
    else:
        trainer.train()
    
    # 7. æœ€ç»ˆä¿å­˜
    trainer.save_model(training_args.output_dir)

if __name__ == "__main__":
    main()